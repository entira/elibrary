# PDF Library Processor s Memvid

AutomatizovanÃ½ nÃ¡stroj na spracovanie kniÅ¾nice PDF sÃºborov s pouÅ¾itÃ­m lokÃ¡lnej Ollama AI na extrakciu metadÃ¡t a vytvorenie video indexu pomocou memvid.

## ğŸ¯ ÃšÄel

Skript spracovÃ¡va kolekciu PDF knÃ­h, extrahuje z nich metadÃ¡ta (autori, vydavatelia, rok, DOI) pomocou lokÃ¡lnej Ollama AI a vytvÃ¡ra vyhÄ¾adÃ¡vateÄ¾nÃ½ video index, kde kaÅ¾dÃ½ chunk textu je zakÃ³dovanÃ½ do QR kÃ³du vo video framoch.

## ğŸ“‹ PoÅ¾iadavky

### SystÃ©movÃ© poÅ¾iadavky
- Python >= 3.10
- Ollama server beÅ¾iaci na `localhost:11434`
- Modely v Ollama: `mistral:latest` a `nomic-embed-text`

### Python zÃ¡vislosti
```bash
pip install -r requirements.txt
```

```txt
memvid
PyPDF2
requests
tqdm
```

## ğŸ—ï¸ ArchitektÃºra

### Komponenty

#### 1. **OllamaEmbedder**
- **ÃšÄel**: Generovanie embeddings pomocou `nomic-embed-text` modelu
- **Endpoint**: `POST http://localhost:11434/api/generate`
- **Parametre**: `{"model": "nomic-embed-text", "embedding": true}`

#### 2. **PDFLibraryProcessor** 
- **HlavnÃ¡ trieda** zodpovednÃ¡ za orchestrÃ¡ciu celÃ©ho procesu
- **KonfigurÃ¡cia**:
  - VstupnÃ½ prieÄinok: `./pdf_books`
  - VÃ½stupnÃ½ prieÄinok: `./memvid_out`
  - Chunk veÄ¾kosÅ¥: 512 znakov
  - Overlap: 50 znakov

### Workflow

```mermaid
graph TD
    A[PDF sÃºbory] --> B[Extrakcia textu]
    B --> C[Chunking ~100 slov]
    C --> D[PrvÃ½ch 10 chunks]
    D --> E[Ollama mistral:latest]
    E --> F[JSON metadÃ¡ta]
    F --> G[Memvid encoder]
    G --> H[QR frames generovanie]
    H --> I[Video + Index]
```

## ğŸ”§ Funkcionalita

### 1. Extrakcia metadÃ¡t pomocou AI

```python
def extract_metadata_with_ollama(self, sample_text: str) -> Dict[str, str]:
```

**Proces**:
1. Vezme prvÃ½ch 10 chunks z PDF
2. SpojÃ­ ich do sample_text 
3. PoÅ¡le prompt do `mistral:latest`:
   ```
   Extract JSON with keys: title, authors, publishers, year, doi from this text:
   [sample_text]
   Return only valid JSON.
   ```
4. Parsuje JSON odpoveÄ
5. Validuje a normalizuje dÃ¡ta

**VÃ½stup**:
```json
{
  "title": "RAG-Driven Generative AI",
  "authors": "Denis Rothman",
  "publishers": "Packt Publishing", 
  "year": "2024",
  "doi": "10.1234/example"
}
```

### 2. PDF Processing Pipeline

```python
def process_pdf(self, pdf_path: Path) -> bool:
```

**Kroky**:
1. **Extrakcia textu** pomocou PyPDF2
2. **Chunking** na ~100 slov per chunk
3. **Metadata extraction** z prvÃ½ch 10 chunks
4. **Pridanie do memvid** encoder

### 3. Video Index Generation

```python
encoder.add_pdf(str(pdf_path), chunk_size=512, overlap=50)
encoder.build_video("library.mp4", "library_index.json")
```

**VytvorÃ­**:
- ğŸ¥ **library.mp4**: Video kde kaÅ¾dÃ½ frame obsahuje QR kÃ³d s chunks
- ğŸ“‹ **library_index.json**: Metadata a mappings
- ğŸ” **library_index.faiss**: Vector search index

## ğŸ“ Å truktÃºra sÃºborov

```
memvid/
â”œâ”€â”€ pdf_library_processor.py    # HlavnÃ½ skript na spracovanie PDF
â”œâ”€â”€ pdf_library_chat.py         # Chat interface pre video pamÃ¤Å¥
â”œâ”€â”€ requirements.txt            # Python zÃ¡vislosti  
â”œâ”€â”€ pdf_books/                  # VstupnÃ© PDF sÃºbory
â”‚   â”œâ”€â”€ book1.pdf
â”‚   â””â”€â”€ book2.pdf
â”œâ”€â”€ memvid_out/                # VÃ½stupnÃ© sÃºbory
â”‚   â”œâ”€â”€ library.mp4            # Video index
â”‚   â”œâ”€â”€ library_index.json     # Metadata
â”‚   â””â”€â”€ library_index.faiss    # Vector index
â””â”€â”€ venv/                      # Python virtualenv
```

## ğŸš€ PouÅ¾itie

### 1. PrÃ­prava prostredia

```bash
# Vytvorenie virtual environment
python3 -m venv venv
source venv/bin/activate

# InÅ¡talÃ¡cia zÃ¡vislostÃ­
pip install -r requirements.txt
```

### 2. Spustenie Ollama

```bash
# Stiahnutie modelov
ollama pull mistral:latest
ollama pull nomic-embed-text

# Overenie Å¾e Ollama beÅ¾Ã­
curl http://localhost:11434/api/tags
```

### 3. Pridanie PDF sÃºborov

```bash
# Vytvorenie prieÄinka a pridanie PDF
mkdir -p pdf_books
cp *.pdf pdf_books/
```

### 4. Spustenie spracovania

```bash
python3 pdf_library_processor.py
```

### 5. Chat s kniÅ¾nicou

Po vytvorenÃ­ video indexu mÃ´Å¾ete spustiÅ¥ chat interface:

```bash
python3 pdf_library_chat.py
```

## ğŸ“Š VÃ½stup

### Konzola log
```
Found 7 PDF files to process
Processing: RAG-Driven Generative AI...
  - Pages: 517, Chunks: 1079
  - Extracting metadata...
  - Title: RAG-Driven Generative AI
  - Authors: Denis Rothman
  - Year: 2024
Processing: LangChain and LlamaIndex...
  - Pages: 86, Chunks: 194
  - Extracting metadata...
  - Title: LangChain and LlamaIndex Projects Lab Book
  - Authors: Mark Watson
  - Year: 2024

Building video index...
Generating QR frames: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8975/8975
âœ… SUCCESS!
ğŸ“š Processed 7 PDF books
ğŸ¥ Video saved to: memvid_out/library.mp4
ğŸ“‹ Index saved to: memvid_out/library_index.json
```

### VytvorenÃ© sÃºbory

#### `library.mp4`
- Video kde kaÅ¾dÃ½ frame = 1 chunk ako QR kÃ³d
- Framerate: 1 FPS (default memvid)
- FormÃ¡t: MP4 s H.264 codec

#### `library_index.json`
```json
{
  "chunks": [
    {
      "id": 0,
      "text": "Chapter 1: Introduction to RAG...",
      "frame": 0,
      "metadata": {
        "file_name": "rag_book.pdf",
        "title": "RAG-Driven Generative AI",
        "authors": "Denis Rothman",
        "year": "2024",
        "page": 15
      }
    }
  ],
  "total_frames": 8975,
  "total_chunks": 8975
}
```

#### `library_index.faiss`
- BinÃ¡rny FAISS vector index
- 384-dimenzionÃ¡lne embeddings (nomic-embed-text)
- UmoÅ¾Åˆuje semantickÃ© vyhÄ¾adÃ¡vanie

## ğŸ” InternÃ© detaily

### Chunk Storage Lifecycle

1. **V pamÃ¤ti**: `encoder.chunks[]` - list vÅ¡etkÃ½ch chunks
2. **DoÄasne**: `/tmp/tmp*/frames/frame_*.png` - QR obrÃ¡zky
3. **FinÃ¡lne**: `library.mp4` + index sÃºbory

### Error Handling

```python
# PDF parsing errors
except Exception as e:
    print(f"Error reading PDF {pdf_path}: {e}")
    return [], 0

# Ollama API errors  
except Exception as e:
    print(f"Error extracting metadata with Ollama: {e}")
    return self._empty_metadata()
```

### Performance

- **Chunk generovanie**: ~10 chunks/sec
- **QR frame generovanie**: ~10-15 frames/sec  
- **Metadata extrakcia**: ~5-10 sec per PDF
- **CelkovÃ½ Äas**: ~2-3 min pre 7 PDF (varies by size)

## ğŸ›ï¸ KonfigurÃ¡cia

### Memvid parametre
```python
# V PDFLibraryProcessor.__init__()
self.encoder = MemvidEncoder()

# V process_pdf()
self.encoder.add_pdf(str(pdf_path), chunk_size=512, overlap=50)
```

### Ollama endpointy
```python
# Embeddings
POST http://localhost:11434/api/generate
{
  "model": "nomic-embed-text",
  "prompt": text,
  "embedding": true
}

# Metadata extraction
POST http://localhost:11434/api/generate  
{
  "model": "mistral:latest",
  "prompt": "Extract JSON...",
  "options": {"temperature": 0.1}
}
```

## âš ï¸ ZnÃ¡me limitÃ¡cie

1. **PDF parsing**: NiektorÃ© PDF mÃ´Å¾u maÅ¥ problÃ©my s text extraction
2. **Ollama dostupnosÅ¥**: VyÅ¾aduje beÅ¾iaci Ollama server
3. **Memory usage**: VeÄ¾kÃ© PDF mÃ´Å¾u spotrebovaÅ¥ veÄ¾a RAM
4. **Processing time**: Video generovanie je Äasovo nÃ¡roÄnÃ©
5. **Metadata quality**: ZÃ¡visÃ­ od kvality text extraction a AI modelu

## ğŸ”§ Troubleshooting

### Ollama connection failed
```bash
# SkontrolovaÅ¥ Äi Ollama beÅ¾Ã­
ollama list
curl http://localhost:11434/api/tags
```

### PDF extraction errors
```python
# SkontrolovaÅ¥ PyPDF2 log v konzole
# NiektorÃ© PDF mÃ´Å¾u byÅ¥ chrÃ¡nenÃ©/poÅ¡kodenÃ©
```

### Memory errors
```bash
# SpracovÃ¡vaÅ¥ PDF po menÅ¡Ã­ch dÃ¡vkach
# Alebo zvÃ½Å¡iÅ¥ system memory/swap
```

## ğŸ“ˆ RozÅ¡Ã­renia

### MoÅ¾nÃ© vylepÅ¡enia
1. **Batch processing**: Spracovanie po dÃ¡vkach pre veÄ¾kÃ© kolekcie
2. **Multi-threading**: ParalelnÃ© spracovanie PDF
3. **Database storage**: Ukladanie do DB namiesto JSON
4. **Web interface**: GUI pre browsing a vyhÄ¾adÃ¡vanie
5. **Alternative models**: Podpora pre inÃ© LLM/embedding modely

### Custom metadata fields
```python
# PridaÅ¥ novÃ© pole do metadata extraction prompt
prompt = f"""Extract JSON with keys: title, authors, publishers, year, doi, isbn, language from this text:
{sample_text}
Return only valid JSON."""
```

# PDF Library Chat Interface

InteraktÃ­vny chat systÃ©m pre komunikÃ¡ciu s PDF kniÅ¾nicou pomocou video pamÃ¤te.

## ğŸ¯ Funkcionalita

### Chat Commands
```
help          - ZobrazÃ­ nÃ¡povedu
info          - InformÃ¡cie o kniÅ¾nici  
search <query>- VyhÄ¾adÃ¡vanie v obsahu
stats         - Å tatistiky session
clear         - VyÄistÃ­ obrazovku
exit/quit     - UkonÄÃ­ chat
```

### PrÃ­klady pouÅ¾Ã­vania

```bash
ğŸ¤” You: What is RAG in AI?
ğŸ¤– Assistant: Based on the library content, RAG (Retrieval Augmented Generation) is...

ğŸ¤” You: search machine learning
ğŸ” Search results for: 'machine learning' (0.15s)
ğŸ“„ Relevant passages:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[Relevant text chunks from PDFs...]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ¤” You: info
ğŸ“– Library Overview:
   ğŸ“š Total books: 7
   ğŸ“ Total chunks: 8975

ğŸ“‘ Books in library:
   1. RAG-Driven Generative AI
      ğŸ“– Author(s): Denis Rothman
      ğŸ“… Year: 2024
      ğŸ“ Chunks: 1079
```

## ğŸ”§ TechnickÃ© detaily

### Komponenty
- **PDFLibraryChat**: HlavnÃ¡ trieda pre chat interface
- **OllamaLLM**: LokÃ¡lne LLM pre generovanie odpovedÃ­  
- **MemvidChat**: Video pamÃ¤Å¥ search a retrieval

### Workflow
1. **NaÄÃ­tanie video indexu** a validÃ¡cia sÃºborov
2. **Semantic search** v PDF chunks pomocou embeddings
3. **Context retrieval** z relevantnÃ½ch chunks
4. **LLM response** pomocou Ollama mistral:latest
5. **FormÃ¡tovanÃ½ vÃ½stup** s metadÃ¡tami

### KonfigurÃ¡cia
```python
# V PDFLibraryChat.__init__()
self.chat = MemvidChat(video_file, index_file)
self.llm = OllamaLLM(model="mistral:latest")
```

---

**Autor**: Claude Code  
**Verzia**: 1.0  
**PoslednÃ¡ aktualizÃ¡cia**: JÃºn 2025