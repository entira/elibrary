# eLibrary - PDF Knowledge Base with RAG

Advanced PDF processing system that converts document libraries into searchable video-based indexes using Memvid technology and Retrieval Augmented Generation (RAG).

## ğŸ—ï¸ System Architecture

Two-version processing pipeline with enhanced metadata extraction and cross-page context preservation:

- **V1 Processor**: Basic PDF chunking (512 chars, ~480 avg length)
- **V2 Processor**: Enhanced chunking (400 chars, ~362 avg length) with detailed page metadata

## ğŸ¯ Features

- **Enhanced PDF Processing**: Two-tier processing system (V1 basic, V2 enhanced)
- **AI Metadata Extraction**: Ollama-powered extraction of titles, authors, publishers, years
- **Video-based Indexing**: QR-encoded text chunks in video frames for efficient storage
- **Cross-page Context**: Enhanced V2 processor preserves context between pages
- **RAG Integration**: Ready for Retrieval Augmented Generation workflows
- **Interactive Chat**: Query your PDF library with natural language

## ğŸ“‹ PoÅ¾iadavky

### SystÃ©movÃ© poÅ¾iadavky
- Python >= 3.10
- Ollama server beÅ¾iaci na `localhost:11434`
- Modely v Ollama: `mistral:latest` a `nomic-embed-text`

### Python zÃ¡vislosti
```bash
pip install -r requirements.txt
```

```txt
memvid
PyPDF2
requests
tqdm
```

## ğŸ—ï¸ ArchitektÃºra

### Core Components

#### 1. **OllamaEmbedder**
- **Purpose**: Generate embeddings using `nomic-embed-text` model
- **Endpoint**: `POST http://localhost:11434/api/generate`
- **Parameters**: `{"model": "nomic-embed-text", "embedding": true}`

#### 2. **PDFLibraryProcessor (V1)**
- **Basic processing** with standard chunking
- **Configuration**:
  - Input folder: `./pdf_books`
  - Output folder: `./memvid_out`
  - Chunk size: 512 characters, Overlap: 50 characters
  - Output: 8,975 segments, ~482 chars average

#### 3. **PDFLibraryProcessorV2 (Enhanced)**
- **Advanced processing** with detailed page metadata
- **Configuration**:
  - Input folder: `./pdf_books`
  - Output folder: `./memvid_out_v2`
  - Chunk size: 400 characters, Overlap: 50 characters
  - Output: 14,486 segments, ~362 chars average
  - Features: Cross-page chunks, detailed page references

### Workflow

```mermaid
graph TD
    A[PDF sÃºbory] --> B[Extrakcia textu]
    B --> C[Chunking ~100 slov]
    C --> D[PrvÃ½ch 10 chunks]
    D --> E[Ollama mistral:latest]
    E --> F[JSON metadÃ¡ta]
    F --> G[Memvid encoder]
    G --> H[QR frames generovanie]
    H --> I[Video + Index]
```

## ğŸ”§ Funkcionalita

### 1. Extrakcia metadÃ¡t pomocou AI

```python
def extract_metadata_with_ollama(self, sample_text: str) -> Dict[str, str]:
```

**Proces**:
1. Vezme prvÃ½ch 10 chunks z PDF
2. SpojÃ­ ich do sample_text 
3. PoÅ¡le prompt do `mistral:latest`:
   ```
   Extract JSON with keys: title, authors, publishers, year, doi from this text:
   [sample_text]
   Return only valid JSON.
   ```
4. Parsuje JSON odpoveÄ
5. Validuje a normalizuje dÃ¡ta

**VÃ½stup**:
```json
{
  "title": "RAG-Driven Generative AI",
  "authors": "Denis Rothman",
  "publishers": "Packt Publishing", 
  "year": "2024",
  "doi": "10.1234/example"
}
```

### 2. PDF Processing Pipeline

```python
def process_pdf(self, pdf_path: Path) -> bool:
```

**Kroky**:
1. **Extrakcia textu** pomocou PyPDF2
2. **Chunking** na ~100 slov per chunk
3. **Metadata extraction** z prvÃ½ch 10 chunks
4. **Pridanie do memvid** encoder

### 3. Video Index Generation

```python
encoder.add_pdf(str(pdf_path), chunk_size=512, overlap=50)
encoder.build_video("library.mp4", "library_index.json")
```

**VytvorÃ­**:
- ğŸ¥ **library.mp4**: Video kde kaÅ¾dÃ½ frame obsahuje QR kÃ³d s chunks
- ğŸ“‹ **library_index.json**: Metadata a mappings
- ğŸ” **library_index.faiss**: Vector search index

## ğŸ“ Project Structure

```
eLibrary/
â”œâ”€â”€ pdf_library_processor.py      # V1: Basic PDF processor
â”œâ”€â”€ pdf_library_processor_v2.py   # V2: Enhanced processor with page metadata
â”œâ”€â”€ pdf_library_chat.py           # Interactive chat interface
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # This documentation
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ .github/                      # ğŸ¤– GitHub Actions automation
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â””â”€â”€ auto-fix-issues.yml   # Automated issue fixing workflow
â”‚   â”œâ”€â”€ actions/
â”‚   â”‚   â””â”€â”€ claude-ai-fix/        # Custom AI fix action
â”‚   â”œâ”€â”€ ISSUE_TEMPLATE/           # Structured issue templates
â”‚   â”‚   â”œâ”€â”€ auto-fix-bug.md       # Bug report template
â”‚   â”‚   â””â”€â”€ auto-fix-enhancement.md # Enhancement template
â”‚   â”œâ”€â”€ README.md                 # GitHub Actions documentation
â”‚   â”œâ”€â”€ SETUP_INSTRUCTIONS.md     # Setup guide
â”‚   â””â”€â”€ test_auto_fix.py          # Test script
â”œâ”€â”€ pdf_books/                    # Input PDF files (excluded from git)
â”‚   â”œâ”€â”€ book1.pdf
â”‚   â””â”€â”€ book2.pdf
â”œâ”€â”€ memvid_out/                   # V1 Output (excluded from git)
â”‚   â”œâ”€â”€ library.mp4              # V1 Video index
â”‚   â”œâ”€â”€ library_index.json       # V1 Metadata (8,975 segments)
â”‚   â””â”€â”€ library_index.faiss      # V1 Vector index
â”œâ”€â”€ memvid_out_v2/               # V2 Enhanced Output (excluded from git)
â”‚   â”œâ”€â”€ library_v2.mp4           # V2 Video index
â”‚   â”œâ”€â”€ library_v2_index.json    # V2 Enhanced metadata (14,486 segments)
â”‚   â””â”€â”€ library_v2_index.faiss   # V2 Vector index
â””â”€â”€ venv/                        # Python virtual environment (excluded)
```

## ğŸš€ PouÅ¾itie

### 1. PrÃ­prava prostredia

```bash
# Vytvorenie virtual environment
python3 -m venv venv
source venv/bin/activate

# InÅ¡talÃ¡cia zÃ¡vislostÃ­
pip install -r requirements.txt
```

### 2. Spustenie Ollama

```bash
# Stiahnutie modelov
ollama pull mistral:latest
ollama pull nomic-embed-text

# Overenie Å¾e Ollama beÅ¾Ã­
curl http://localhost:11434/api/tags
```

### 3. Pridanie PDF sÃºborov

```bash
# Vytvorenie prieÄinka a pridanie PDF
mkdir -p pdf_books
cp *.pdf pdf_books/
```

### 4. Run Processing

**Option A: Basic V1 Processor**
```bash
python3 pdf_library_processor.py
```

**Option B: Enhanced V2 Processor (Recommended)**
```bash
python3 pdf_library_processor_v2.py
```

**V2 Benefits:**
- 61% more text segments (14,486 vs 8,975)
- Shorter, more precise chunks (362 vs 482 chars)
- Cross-page context preservation (2,184 cross-page chunks)
- Detailed page metadata for each chunk
- Enhanced statistics and book information

### 5. Chat s kniÅ¾nicou

Po vytvorenÃ­ video indexu mÃ´Å¾ete spustiÅ¥ chat interface:

```bash
python3 pdf_library_chat.py
```

## ğŸ“Š Processing Results

### V2 Enhanced Output (Recommended)
```
Found 7 PDF files to process
Output directory: memvid_out_v2
Chunk size: 400 chars, Overlap: 50 chars

Processing: RAG-Driven Generative AI...
  - Pages: 517, Enhanced chunks: 2316
  - Extracting metadata...
  - Title: RAG-Driven Generative AI
  - Authors: Denis Rothman
  - Year: 2024

Generating QR frames: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14486/14486 [24:09<00:00]
Enhancing index with detailed metadata...

âœ… SUCCESS!
ğŸ“š Processed 7 PDF books
ğŸ¥ Enhanced video: memvid_out_v2/library_v2.mp4
ğŸ“‹ Enhanced index: memvid_out_v2/library_v2_index.json
ğŸ“„ Each chunk includes detailed page references!
```

### Performance Comparison

| Metric | V1 Basic | V2 Enhanced | Improvement |
|--------|----------|-------------|-------------|
| Total segments | 8,975 | 14,486 | +61% |
| Avg segment length | 482 chars | 362 chars | -25% (more precise) |
| Cross-page chunks | 0 | 2,184 | New feature |
| Page metadata | Basic | Detailed | Enhanced |
| Unique pages indexed | N/A | 2,245 | New feature |

### VytvorenÃ© sÃºbory

#### `library.mp4`
- Video kde kaÅ¾dÃ½ frame = 1 chunk ako QR kÃ³d
- Framerate: 1 FPS (default memvid)
- FormÃ¡t: MP4 s H.264 codec

#### `library_v2_index.json` (Enhanced)
```json
{
  "metadata": [
    {
      "id": 0,
      "text": "RAG-Driven Generative AI\nBuild custom retrieval...",
      "frame": 0,
      "length": 134
    }
  ],
  "enhanced_stats": {
    "total_files": 7,
    "total_chunks": 14486,
    "total_unique_pages": 2245,
    "cross_page_chunks": 2184,
    "files": {
      "RAG-Driven Generative AI...pdf": {
        "chunks": 2316,
        "unique_pages": 517,
        "title": "RAG-Driven Generative AI",
        "authors": "Denis Rothman",
        "year": "2024"
      }
    }
  }
}
```

#### `library_index.faiss`
- BinÃ¡rny FAISS vector index
- 384-dimenzionÃ¡lne embeddings (nomic-embed-text)
- UmoÅ¾Åˆuje semantickÃ© vyhÄ¾adÃ¡vanie

## ğŸ” InternÃ© detaily

### Chunk Storage Lifecycle

1. **V pamÃ¤ti**: `encoder.chunks[]` - list vÅ¡etkÃ½ch chunks
2. **DoÄasne**: `/tmp/tmp*/frames/frame_*.png` - QR obrÃ¡zky
3. **FinÃ¡lne**: `library.mp4` + index sÃºbory

### Error Handling

```python
# PDF parsing errors
except Exception as e:
    print(f"Error reading PDF {pdf_path}: {e}")
    return [], 0

# Ollama API errors  
except Exception as e:
    print(f"Error extracting metadata with Ollama: {e}")
    return self._empty_metadata()
```

### Performance

- **Chunk generovanie**: ~10 chunks/sec
- **QR frame generovanie**: ~10-15 frames/sec  
- **Metadata extrakcia**: ~5-10 sec per PDF
- **CelkovÃ½ Äas**: ~2-3 min pre 7 PDF (varies by size)

## ğŸ›ï¸ KonfigurÃ¡cia

### Memvid parametre
```python
# V PDFLibraryProcessor.__init__()
self.encoder = MemvidEncoder()

# V process_pdf()
self.encoder.add_pdf(str(pdf_path), chunk_size=512, overlap=50)
```

### Ollama endpointy
```python
# Embeddings
POST http://localhost:11434/api/generate
{
  "model": "nomic-embed-text",
  "prompt": text,
  "embedding": true
}

# Metadata extraction
POST http://localhost:11434/api/generate  
{
  "model": "mistral:latest",
  "prompt": "Extract JSON...",
  "options": {"temperature": 0.1}
}
```

## âš ï¸ ZnÃ¡me limitÃ¡cie

1. **PDF parsing**: NiektorÃ© PDF mÃ´Å¾u maÅ¥ problÃ©my s text extraction
2. **Ollama dostupnosÅ¥**: VyÅ¾aduje beÅ¾iaci Ollama server
3. **Memory usage**: VeÄ¾kÃ© PDF mÃ´Å¾u spotrebovaÅ¥ veÄ¾a RAM
4. **Processing time**: Video generovanie je Äasovo nÃ¡roÄnÃ©
5. **Metadata quality**: ZÃ¡visÃ­ od kvality text extraction a AI modelu

## ğŸ”§ Troubleshooting

### Ollama connection failed
```bash
# SkontrolovaÅ¥ Äi Ollama beÅ¾Ã­
ollama list
curl http://localhost:11434/api/tags
```

### PDF extraction errors
```python
# SkontrolovaÅ¥ PyPDF2 log v konzole
# NiektorÃ© PDF mÃ´Å¾u byÅ¥ chrÃ¡nenÃ©/poÅ¡kodenÃ©
```

### Memory errors
```bash
# SpracovÃ¡vaÅ¥ PDF po menÅ¡Ã­ch dÃ¡vkach
# Alebo zvÃ½Å¡iÅ¥ system memory/swap
```

## ğŸ¤– GitHub Actions AutomatizÃ¡cia

âœ… **Kompletne implementovanÃ½ automatizovanÃ½ systÃ©m na rieÅ¡enie issues!**

### ğŸ¯ Ako pouÅ¾Ã­vaÅ¥:

#### 1. **Vytvorte issue s template:**
```
GitHub â†’ Issues â†’ New issue â†’ Choose template:
- ğŸ¤– Auto-Fix Bug Report
- ğŸš€ Auto-Fix Enhancement
```

#### 2. **Aktivujte automatizÃ¡ciu:**
```bash
# Pridajte label na issue
gh issue edit ISSUE_NUMBER --add-label "auto-fix"

# Alebo cez web interface v GitHub
```

#### 3. **Sledujte progress:**
```bash
# GitHub Actions workflow sa automaticky spustÃ­
https://github.com/entira/elibrary/actions

# AI vytvorÃ­ PR s rieÅ¡enÃ­m
gh pr list --label "auto-fix"
```

### ğŸ› ï¸ ImplementovanÃ© komponenty:
- âœ… **GitHub Actions workflow** (`.github/workflows/auto-fix-issues.yml`)
- âœ… **Issue templates** s Å¡truktÃºrovanÃ½m formulÃ¡rom
- âœ… **Custom AI action** pre automated fixes
- âœ… **Automated testing** a validation
- âœ… **Smart labeling system**

### ğŸ“‹ Ako to funguje:
1. **Issue detection** - workflow reaguje na label `auto-fix`
2. **AI analysis** - analyzuje problÃ©m a kÃ³d context
3. **Fix generation** - vytvorÃ­ rieÅ¡enie based on issue description
4. **Automated testing** - validuje syntax a imports
5. **PR creation** - vytvorÃ­ pull request s fix
6. **Manual review** - vyÅ¾aduje human approval pred merge

### ğŸ“š DokumentÃ¡cia:
- **Setup guide**: [`.github/SETUP_INSTRUCTIONS.md`](.github/SETUP_INSTRUCTIONS.md)
- **System overview**: [`.github/README.md`](.github/README.md)
- **Test script**: [`.github/test_auto_fix.py`](.github/test_auto_fix.py)

## ğŸ“ˆ RozÅ¡Ã­renia

### MoÅ¾nÃ© vylepÅ¡enia
1. **Batch processing**: Spracovanie po dÃ¡vkach pre veÄ¾kÃ© kolekcie
2. **Multi-threading**: ParalelnÃ© spracovanie PDF
3. **Database storage**: Ukladanie do DB namiesto JSON
4. **Web interface**: GUI pre browsing a vyhÄ¾adÃ¡vanie
5. **Alternative models**: Podpora pre inÃ© LLM/embedding modely

### Custom metadata fields
```python
# PridaÅ¥ novÃ© pole do metadata extraction prompt
prompt = f"""Extract JSON with keys: title, authors, publishers, year, doi, isbn, language from this text:
{sample_text}
Return only valid JSON."""
```

# PDF Library Chat Interface

InteraktÃ­vny chat systÃ©m pre komunikÃ¡ciu s PDF kniÅ¾nicou pomocou video pamÃ¤te.

## ğŸ¯ Funkcionalita

### Chat Commands
```
help          - ZobrazÃ­ nÃ¡povedu
info          - InformÃ¡cie o kniÅ¾nici  
search <query>- VyhÄ¾adÃ¡vanie v obsahu
stats         - Å tatistiky session
clear         - VyÄistÃ­ obrazovku
exit/quit     - UkonÄÃ­ chat
```

### PrÃ­klady pouÅ¾Ã­vania

```bash
ğŸ¤” You: What is RAG in AI?
ğŸ¤– Assistant: Based on the library content, RAG (Retrieval Augmented Generation) is...

ğŸ¤” You: search machine learning
ğŸ” Search results for: 'machine learning' (0.15s)
ğŸ“„ Relevant passages:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[Relevant text chunks from PDFs...]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ¤” You: info
ğŸ“– Library Overview:
   ğŸ“š Total books: 7
   ğŸ“ Total chunks: 8975

ğŸ“‘ Books in library:
   1. RAG-Driven Generative AI
      ğŸ“– Author(s): Denis Rothman
      ğŸ“… Year: 2024
      ğŸ“ Chunks: 1079
```

## ğŸ”§ TechnickÃ© detaily

### Komponenty
- **PDFLibraryChat**: HlavnÃ¡ trieda pre chat interface
- **OllamaLLM**: LokÃ¡lne LLM pre generovanie odpovedÃ­  
- **MemvidChat**: Video pamÃ¤Å¥ search a retrieval

### Workflow
1. **NaÄÃ­tanie video indexu** a validÃ¡cia sÃºborov
2. **Semantic search** v PDF chunks pomocou embeddings
3. **Context retrieval** z relevantnÃ½ch chunks
4. **LLM response** pomocou Ollama mistral:latest
5. **FormÃ¡tovanÃ½ vÃ½stup** s metadÃ¡tami

### KonfigurÃ¡cia
```python
# V PDFLibraryChat.__init__()
self.chat = MemvidChat(video_file, index_file)
self.llm = OllamaLLM(model="mistral:latest")
```

## ğŸ“ Version History

### V2.0 (Current) - Enhanced Processing
- **14,486 segments** with detailed page metadata
- **Cross-page chunks** for better context preservation
- **Enhanced statistics** and book information
- **Improved chunking** (400 chars vs 512 chars)
- **Better RAG performance** with more precise segments

### V1.0 - Basic Processing
- **8,975 segments** with basic metadata
- **512-character chunks** with standard overlap
- **Simple PDF processing** without page references

---

**Repository**: eLibrary PDF Knowledge Base  
**Version**: 2.0 Enhanced  
**Last Updated**: December 2024  
**License**: MIT