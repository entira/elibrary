Cleaned PYPDF2 extraction from: RAG-Driven Generative AI_ Build Custom Retrieval Augmented -- Denis Rothman -- 2024 -- Packt Publishing Ltd -- 9781836200901 -- 4200fd9730727b48a7ada8aed0de7a8b -- Anna’s Archive.pdf
============================================================

RAG-Driven Gener ative AI
Build custom retrieval augmented generation
pipelines with LlamaIndex, Deep Lake, and Pinecone Denis Rothman RAG-Driven Gener ative AI
Copyright © 2024 P ackt Publishing All rights reserv ed. No part of this book may be reproduced, stored inaretriev al system, or transmied in any form or by any means,
without the prior wrien permission of the publisher, except in the
case of brief quotations embedded in critical articles or reviews.
Every eﬀort has been made in the preparation of this book to ensure
the accuracy of the information presented. How ever, the information
contained in this book is sold withoutwarranty, either express or
implied. Neither the author, nor P ackt Publishing or its dealers and
distributors, will be held liable for any damages caused or alleged to
have been caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information
about all of the companies and products mentioned in this book by
the appropriate use of capitals. How ever, P ackt Publishing cannot
guarantee the accuracy of this information.
Senior Publishing Product Manager: Bhav esh Amin Acquisition Editor – P eer Reviews: Swaroop Singh Project Editor: Janice Gonsalv es Content Dev elopment Editor: Tanya D’cruz Copy Editor: Saﬁs Editor Technical Editor: Karan Sonaw ane Proofreader: Saﬁs Editor Indexer: Rekha Nair Presentation Designer: Ajay P atule Developer Relations Marketing Executiv e: Anamika Singh First published: September 2024
Production reference: 1250924
Published by P ackt Publishing Ltd.
Livery P lace
35 Liv ery Street Birmingham B3 2PB, UK.
ISBN: 978-1-83620-091-8
www.packt.com Contributors About the author Denis Rothman graduated from Sorbonne Univ ersity and P aris-
Diderot Univ ersity, and asastudent, he wrote and registeredapatent for one of the earliest word2v ector embeddings and word
piece tokenization solutions. He startedacompany focused on
deploying AI andwent on to author one of the ﬁrst AI cognitiv e NLP
chatbots, applied asalanguage teaching tool for Moët et Chandon
(part of L VMH) and more. Denis rapidly became an expert in
explainable AI, incorporating interpretable, acceptance-based
explanation data and interfaces into solutions implemented for
major corporate projects in the aerospace, apparel, and supply chain
sectors. His core belief is that you only really know something once
you havetaught somebody how to do it.
About the reviewers Alberto Romero has alw ays hadapassion for technology and open
source, from programming at the age of 12 to hacking the Linux
kernel by 14 back in the 90s. In 2017, he co-founded an AI startup
and serv ed as its CTO for sixyears, building an aw ard-winning InsurT ech platform from scratch. He currently continues to design
and build generativ e AI platforms in ﬁnancial services, leading
multiple initiativ es in this space. He has dev eloped and
productionized numerous AI products that automate and improvedecision-making processes, already serving thousands of users. He
serves as an advisor to an adv anced data security and gov ernance
startup that lev erages predictiv e ML and Generativ e AI to address
modern enterprise data security challenges.
I would like to express my deepest gratitude to my wife, Alicia, and
daughters, Adriana and Catalina, for their unw avering support
throughout the process of reviewing this book. Their patience,
encouragement, and lovehavebeen inv aluable, and I am truly
fortunate to havethem by my side.
Shubham Garg isasenior applied scientist at Amazon, specializing
in dev eloping Large Language Models (LLMs ) and Vision-
Language Models (VLMs ). He has led innov ative projects at Amazon and IBM, including dev eloping Alexa’s translation features,
dynamic prompt construction, and optimizing AI tools. Shubham
has contributed to adv ancements in NLP, multilingual models, and AI-driv en solutions. He has published at major NLP conferences,
review ed for conferences and journals, and holdsapatent. His deep
expertise in AI technologies makes his perspectiveasareview er both
valuable and insightful.
Tamilselv an Subramanian isaseasoned AI leader and two-time
founder, specializing in generativ e AI for text and images. He has
built and scaled AI-driv en products, including an AI conserv ation
platform to saveendangered species, a medical image diagnostic
platform, an AI-driv en EV leasing platform, and an Enterprise AI
platform from scratch. Tamil has authored multiple AI articles
published in medical journals and holds two patents in AI and
image processing. He has serv ed asatechnical architect and
consultant for ﬁnance and energy companies across Europe, the US,
and Australia, and has also worked for IBM and W ipro. Currently,
he focuses on cuing-edge applications of computer vision, text, and
generativ e AI.
My special thanks go to my wife Suganthi, my son Sanjeev, and my
mom and dad for their unw avering support, allowing me the
personal time to work on this book.
Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag Preface Designing and managing controlled, reliable, multimodal generativ e AI pipelines is complex. RAG-Driv en Generativ e AI providesaroadmap for building eﬀectiv e LLM, computer vision, and
generativ e AI systems that will balance performance and costs.
From foundational concepts to complex implementations, this book
oﬀersadetailed exploration of how RAG can control and enhance AI
systems by tracing each output to its source document. RAG’s
traceable process allows human feedback for continual
improv ements, minimizing inaccuracies, hallucinations, and bias.
This AI book shows you how to build a RAG framework from
scratch, providing practical know ledge onvector stores, chunking,
indexing, and ranking. You’ll discov er techniques in optimizing
performance and costs, improving model accuracy by integrating
human feedback, balancing costs with when to ﬁne-tune, and
improving accuracy and retriev al speed by utilizing embedded-
indexed know ledge graphs.
Experienceablend of theory and practice using frameworks like LlamaIndex, Pinecone, and Deep Lake and generativ e AI platforms
such as OpenAI and Hugging Face.
By the end of this book, you will haveacquired the skills to
implement intelligent solutions, keeping you competitivein ﬁelds
from production to customer service across any project.
Who this book is for This book is ideal for data scientists, AI engineers, machine learning
engineers, and MLOps engineers, aswell as solution architects,
softw are dev elopers, and product and project managers working on LLM and computer vision projects whowant to learn and apply RAG for real-world applications. Researchers and natural language
processing practitioners working with large language models and
text generation will also ﬁnd the book useful.
What this book covers Chapter 1 , Why Retriev al Augmented Generation? , introduces RAG’s
foundational concepts, outlines its adaptability across diﬀerent data
types, and navigates the complexities of integrating the RAG
framework into existing AI platforms. By the end of this chapter, you
will havegainedasolid understanding of RAG and practical
experience in building div erse RAG conﬁgurations for naïv e,
advanced, and modular RAG using Python, preparing you for more
advanced applications in subsequent chapters.
Chapter 2 , RAG Embedding V ector Stores with Deep Lake and OpenAI ,
dives into the complexities of RAG-driv en generativ e AI by focusing
on embeddingvectors and their storage solutions. We explore the
transition from raw data to organizedvector stores using Activ eloop Deep Lake and OpenAI models, detailing the process of creating and
managing embeddings that capture deep semantic meanings. You
will learn to buildascalable, multi-team RAG pipeline from scratch
in Python by dissecting the RAG ecosystem into independent
components. By the end, you’ ll be equipped to handle large datasets
with sophisticated retriev al capabilities, enhancing generativ e AI
outputs with embedded documentvectors.
Chapter 3 , Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI , div es into index-based RAG, focusing on enhancing AI’s
precision, speed, and transparency through indexing. We’ll see how LlamaIndex, Deep Lake, and OpenAI can be integrated to put
togetheratraceable and eﬃcient RAG pipeline. Through practical
examples, includingadomain-speciﬁc drone technology project, you
will learn to manage and optimize index-based retriev al systems. By
the end, you will be proﬁcient in usingvarious indexing types and
understand how to enhance the data integrity and quality of your AI
outputs.
Chapter 4 , Multimodal Modular RAG for Drone T echnology , raises the
bar of all generativ e AI applications by introducingamultimodal
modular RAG framework tailored for drone technology. We’ll
developagenerativ e AI system that not only processes textual
information but also integrates adv anced image recognition
capabilities. You’ll learn to build and optimize a Python-based
multimodal modular RAG system, using tools like LlamaIndex,
Deep Lake, and OpenAI, to produce rich, context-aw are responses to
queries.
Chapter 5 , Boosting RAG P erformance with Expert Human Feedback ,
introduces adaptiv e RAG, an innov ative enhancement to standard RAG that incorporates human feedback into the generativ e AI
process. By integrating expert feedback directly, wewill createahybrid adaptiv e RAG system using Python, exploring the
integration of human feedback loops to reﬁne data continuously and
improvethe relev ance and accuracy of AI responses.
Chapter 6 , Scaling RAG Bank Customer Data with Pinecone , guides you
through buildingarecommendation system to minimize bank
customer churn, starting with data acquisition and exploratory
analysis using a Kaggle dataset. You’ll moveonto embedding and
upserting large data volumes with Pinecone and OpenAI’s
technologies, culminating in dev eloping AI-driv en
recommendations with GPT-4o. By the end, you’ ll know how to
implement adv ancedvector storage techniques and AI-driv en
analytics to enhance customer retention strategies.
Chapter 7 , Building Scalable Know ledge-Graph-Based RAG with W ikipedia API and LlamaIndex , details the dev elopment of three pipelines: data
collection from W ikipedia, populating a Deep Lakevector store, and
implementingaknow ledge graph index-based RAG. You’ll learn to
automate data retriev al and preparation, create and queryaknow ledge graph to visualize complex data relationships, and
enhance AI-generated responses with structured data insights. You’ll
be equipped by the end to build and manageaknow ledge graph-
based RAG system, providing precise, context-aw are output.
Chapter 8 , Dynamic RAG with Chroma and Hugging Face Llama ,
explores dynamic RAG using Chroma and Hugging Face’s Llama
technology. It introduces the concept of creating temporary data
collections daily, optimized for speciﬁc meetings or tasks, which
avoids long-term data storage issues. You will learn to build a Python program that manages and queries these transient datasets
eﬃciently, ensuring that the most relev ant and up-to-date
information supports ev ery meeting or decision point. By the end,
you will be able to implement dynamic RAG systems that enhance
responsiv eness and precision in data-driv en environments.
Chapter 9 , Empow ering AI Models: Fine-T uning RAG Data and Human Feedback , focuses on ﬁne-tuning techniques to streamline RAG data,
emphasizing how to transform extensiv e, non-parametric raw data
intoamore manageable, parametric format with trainedweights
suitable for continued AI interactions. You’ll explore the process of
preparing and ﬁne-tuningadataset, using OpenAI’s tools to conv ert
data into prompt and completion pairs for machine learning.
Additionally, this chapter will guide you through using OpenAI’s GPT-4o-mini model for ﬁne-tuning, assessing its eﬃciency and cost-
eﬀectiv eness.
Chapter 10 , RAG for V ideo Stock Production with Pinecone and OpenAI ,
explores the integration of RAG in video stock production,
combining human creativity with AI-driv en automation. It details
constructing an AI system that produces, comments on, and labels
video content, using OpenAI’s text-to-video and vision models
alongside Pinecone’svector storage capabilities. Starting with video
generation and technical commentary, the journey extends to
managing embedded video data within a Pineconevector store.
To get the most out of this book You should havebasic Natural Processing Language (NLP )
know ledge and some experience with Python. Additionally, most of
the programs in this book are provided as Jupyter notebooks. To run
them, all you need isafree Google Gmail account, allowing you to
execute the notebooks on Google Colaboratory’s free virtual
machine (VM). You will also need to generate API tokens for OpenAI, Activ eloop, and Pinecone.
The following modules will need to be installed when running the
notebooks:
Modules Version
deeplake 3.9.18 (with Pillow)
openai 1.40.3 (requires regular upgrades)
transformers 4.41.2
numpy >=1.24.1 (Upgraded to satisfy chex)
deepspeed 0.10.1
bitsandbytes 0.41.1
accelerate 0.31.0
tqdm 4.66.1
neural_compressor2.2.1
onnx 1.14.1
pandas 2.0.3
scipy 1.11.2
beautifulsoup44.12.3
requests 2.31.0
Download the example code files The code bundle for the book is hosted on GitHub at
https://github.com/Denis2054/RAG-Driven-Generative-AI .
We also haveother code bundles from our rich catalog of books and
videos av ailable at https://github.com/PacktPublishing/ .
Check them out!
Download the color images We also provide a PDF ﬁle that has color images of the
screenshots/diagrams used in this book. You can download it here:
https://packt.link/gbp/9781836200918 .
Conventions used There areanumber of text conv entions used throughout this book.
CodeInText: Indicates code words in text, database table names,
folder names, ﬁlenames, ﬁle extensions, pathnames, dummy URLs,
user input, and T wier handles. For example: “ self refers to the
current instance of the class to access itsvariables, methods, and
functions”.
A block of code is set as follows:
# Cosine Similarity
score = calculate_cosine_similarity(query, best_matching_recor
print(f"Best Cosine Similarity Score: {score:.3f}")
Any command-line input or output is wrien as follows:
Best Cosine Similarity Score: 0.126
Bold : Indicatesanew term, an important word, or words that you
see on the screen. For example, text in menus or dialog boxes
appears like this. Here is an example: “ Modular RAG implementing
ﬂexible retriev al methods”.
Warnings or important notes appear like this.
Tips and tricks appear like this.
Get in touch Feedback from our readers is alw ayswelcome.
General feedback : Email feedback@packtpub.com, and mention the
book’s title in the subject of your message. If you havequestions
about any aspect of this book, please email us at
questions@packtpub.com.
Errata : Althoughwe havetaken ev ery care to ensure the accuracy of
our content, mistakes do happen. If you havefoundamistake in this
bookwe would be grateful if you would report this to us. Please visit
http://www.packtpub.com/submit-errata , select your book,
click on the Errata Submission Form link, and enter the details.
Piracy : If you come across any illegal copies of our works in any
form on the Internet, wewould be grateful if you would provide us
with the location address orwebsite name. Please contact us at
copyright@packtpub.com withalink to the material.
If you are interested in becoming an author : If there isatopic that
you haveexpertise in and you are interested in either writing or
contributing toabook, please visit
http://authors.packtpub.com .
Share your thoughts Once you’veread RAG-Driv en Generativ e AI, we’d loveto hear your
thoughts! Please click here to go straight to the Amazon
review page for this book and share your feedback.
Your review is important to us and the tech community and will
help us make surewe’re deliv ering excellent quality content.
Downloadafree PDF copy of this
book Thanks for purchasing this book!
Do you like to read on the go but are unable to carry your print
books ev erywhere?
Is your eBook purchase not compatible with the device of your
choice?
Don’t worry, now with ev ery P ackt book you get a DRM-free PDF
version of that book at no cost.
Read anywhere, any place, on any device. Search, copy, and paste
code from your favorite technical books directly into your
application.
The perks don’t stop there, you can get exclusiveaccess to discounts,
newsleers, and great free content in your inbox daily.
Follow these simple steps to get the beneﬁts:
1. Scan the QR code or visit the link below:
https://packt.link/free-ebook/9781836200918
2. Submit your proof of purchase.
3. That’s it! We’ll send your free PDF and other beneﬁts to your
email directly.
1
Why Retrieval Augmented Gener ation?
Even the most adv anced generativ e AI models can only generate
responses based on the data they havebeen trained on. They cannot
provide accurate answ ers to questions about information outside
their training data. Generativ e AI models simply don’t know that
they don’t know! This leads to inaccurate or inappropriate outputs,
sometimes called hallucinations, bias, or, simply said, nonsense.
Retriev al Augmented Generation (RAG ) isaframework that
addresses this limitation by combining retriev al-based approaches
with generativemodels. It retriev es relev ant data from external
sources in real time and uses this data to generate more accurate and
contextually relev ant responses. Generativ e AI models integrated
with RAG retriev ers are revolutionizing the ﬁeld with their
unprecedented eﬃciency and pow er. One of the key strengths of RAG is its adaptability. It can be seamlessly applied to any type of
data, be it text, images, or audio. Thisversatility makes RAG
ecosystemsareliable and eﬃcient tool for enhancing generativ e AI
capabilities.
A project manager, how ever, already encountersawide range of
generativ e AI platforms, frameworks, and models such as Hugging Face, Google V ertex AI, OpenAI, LangChain, and more. An
additional lay er of emerging RAG frameworks and platforms will
only add complexity with Pinecone, Chroma, Activ eloop,
LlamaIndex, and so on. All these Generativ e AI and RAG frameworks
often ov erlap, creating an incredible number of possible
conﬁgurations. Finding the right conﬁguration of models and RAG
resources foraspeciﬁc project, therefore, can be challenging foraproject manager. There is no silv er bullet. The challenge is
tremendous, but the rew ards, when achiev ed, are immense!
We will begin this chapter by deﬁning the RAG framework atahigh
level. Then, wewill deﬁne the three main RAG conﬁgurations: naïv e RAG, adv anced RAG, and modular RAG. We will also compare RAG
and ﬁne-tuning and determine when to use these approaches. RAG
can only exist within an ecosystem, andwe will design and describe
one in this chapter. Data needs to come from somewhere and be
processed. Retriev al requires an organized environment to retrievedata, and generativ e AI models haveinput constraints.
Finally, wewill diveinto the practical aspect of this chapter. We will
build a Python program from scratch to run entry-lev el naïv e RAG
with keyword search and matching. We will also code an adv anced RAG system withvector search and index-based retriev al. Finally, wewill buildamodular RAG that takes both naïveand adv anced RAG
into account. By the end of this chapter, you will acquireatheoretical
understanding of the RAG framework and practical experience in
building a RAG-driv en generativ e AI program. This hands-on
approach will deepen your understanding and equip you for the
following chapters.
Inanutshell, this chapter cov ers the following topics:
Deﬁning the RAG framework The RAG ecosystem Naïvekeyword search and match RAG in Python Adv anced RAG withvector-search and index-based RAG in Python Buildingamodular RAG program Let’s begin by deﬁning RAG.
What is RAG?
Whenagenerativ e AI model doesn’t know how to answ er accurately,
some say it is hallucinating or producing bias. Simply said, it just
produces nonsense. How ever, it all boils down to the impossibility of
providing an adequate response when the model’s training didn’t
include the information requested beyond the classical model
conﬁguration issues. This confusion often leads to random sequences
of the most probable outputs, not the most accurate ones.
RAG begins where generativ e AI ends by providing the information
an LLM model lacks to answ er accurately. RAG w as designed (Lewis
et al., 2020) for LLMs. The RAG framework will perform optimized
information retriev al tasks, and the generation ecosystem will add
this information to the input (user query or automated prompt) to
produce improv ed output. The RAG framework can be summed up
atahigh lev el in the following ﬁgure:
Figure 1.1: The two main components of RAG-driven generative AI
Think of yourself asastudent inalibrary. You havean essay to write
on RAG. Like ChatGPT , for example, or any other AI copilot, you
have learned how to read and write. As with any Large Language Model (LLM ), you are suﬃciently trained to read adv anced
information, summarize it, and write content. How ever, like any
superhuman AI you will ﬁnd from Hugging Face, V ertex AI, or OpenAI, there are many things you don’t know.
In the retriev al phase, you search the library for books on the topic
you need (the left side of Figure 1.1 ). Then, you go back to your seat,
performaretriev al task by yourself oraco-student, and extract the
information you need from those books. In the generation phase (the
right side of Figure 1.1 ), you begin to write your essay. You are a RAG-driv en generativehuman agent, much like a RAG-driv en
generativ e AI framework.
As you continue to write your essay on RAG, you stumble across
some tough topics. You don’t havethe time to go through all the
information av ailable physically! You, asagenerativehuman agent,
are stuck, just asagenerativ e AI model would be. You may try to
write something, just asagenerativ e AI model does when its output
makes lile sense. But you, like the generativ e AI agent, will not
realize whether the content is accurate or not until somebody corrects
your essay and you getagrade that will rank your essay.
At this point, you havereached your limit and decide to turn to a RAG generativ e AI copilot to ensure you get the correct answ ers.
How ever, you are puzzled by the number of LLM models and RAG
conﬁgurations av ailable. You need ﬁrst to understand the resources
available and how RAG is organized. Let’s go through the main RAG
conﬁgurations.
Naïve, advanced, and modular RAG
configurations A RAG framework necessarily contains two main components: a
retriev er andagenerator. The generator can be any LLM or
foundation multimodal AI platform or model, such as GPT-4o,
Gemini, Llama, or one of the hundreds ofvariations of the initial
architectures. The retriev er can be any of the emerging frameworks,
methods, and tools such as Activ eloop, Pinecone, LlamaIndex,
LangChain, Chroma, and many more.
The issue now is to decide which of the three types of RAG
frameworks (Gao et al., 2024) will ﬁt the needs ofaproject. We will
illustrate these three approaches in code in the Naïv e, adv anced, and
modular RAG in code section of this chapter:
Naïv e RAG : This type of RAG framework doesn’t involvecomplex data embedding and indexing. It can be eﬃcient to
access reasonable amounts of data through keywords, for
example, to augmentauser ’s input and obtainasatisfactory
response.
Adv anced RAG : This type of RAG involv es more complex
scenarios, such as withvector search and indexed-base retriev al
applied. Adv anced RAG can be implemented withawide range
of methods. It can process multiple data types, aswell as
multimodal data, which can be structured or unstructured.
Modular RAG : Modular RAG broadens the horizon to include
any scenario that involv es naïv e RAG, adv anced RAG, machine
learning, and any algorithm needed to completeacomplex
project.
How ever, before going further, weneed to decide ifwe should
implement RAG or ﬁne-tuneamodel.
RAG versus fine-tuning RAG is not alw ays an alternativeto ﬁne-tuning, and ﬁne-tuning
cannot alw ays replace RAG. If we accumulate too much data in RAG
datasets, the system may become too cumbersome to manage. On the
other hand, wecannot ﬁne-tuneamodel with dynamic, ev er-
changing data such as dailyweather forecasts, stock marketvalues,
corporate news, and all forms of daily ev ents.
The decision of whether to implement RAG or ﬁne-tuneamodel
relies on the proportion of parametricversus non-parametric
information. The fundamental diﬀerence betw eenamodel trained
from scratch or ﬁne-tuned and RAG can be summed up in terms of
parametric and non-parametric know ledge:
Parametric : In a RAG-driv en generativ e AI ecosystem, the
parametric part refers to the generativ e AI model’s parameters
(weights) learned through training data. This means the model’s
know ledge is stored in these learnedweights and biases. The
original training data is transformed intoamathematical form,
whichwe callaparametric representation. Essentially, the model
“remembers” what it learned from the data, but the data itself is
not stored explicitly.
Non-P arametric : In contrast, the non-parametric part of a RAG
ecosystem involv es storing explicit data that can be accessed
directly. This means that the data remains av ailable and can be
queried whenev er needed. Unlike parametric models, where
know ledge is embedded indirectly in theweights, non-
parametric data in RAG allows us to see and use the actual data
for each output.
The diﬀerence betw een RAG and ﬁne-tuning relies on the amount of
static (parametric) and dynamic (non-parametric) ev er-evolving data
the generativ e AI model must process. A system that relies too
heavily on RAG might become ov erloaded and cumbersome to
manage. A system that relies too much on ﬁne-tuningagenerativemodel will display its inability to adapt to daily information updates.
There isadecision-making threshold illustrated in Figure 1.2 that
shows that a RAG-driv en generativ e AI project manager will haveto
evaluate the potential of the ecosystem’s trained parametric
generativ e AI model before implementinganon-parametric (explicit
data) RAG framework. The potential of the RAG component requires
careful ev aluation aswell.
Figure 1.2: The decision-making threshold between enhancing RAG or ﬁne-tuning an LLM
In the end, the balance betw een enhancing the retriev er and the
generator in a RAG-driv en generativ e AI ecosystem depends onaproject’s speciﬁc requirements and goals. RAG and ﬁne-tuning are
not mutually exclusiv e.
RAG can be used to improvea model’s ov erall eﬃciency, together
with ﬁne-tuning, which serv es asamethod to enhance the
performance of both the retriev al and generation components within
the RAG framework. We will ﬁne-tuneaproportion of the retriev al
data in Chapter 9 , Empow ering AI Models: Fine-T uning RAG Data and Human Feedback .
We will now see how a RAG-driv en generativ e AI involv es an
ecosystem with many components.
The RAG ecosystem RAG-driv en generativ e AI isaframework that can be implemented in
many conﬁgurations. RAG’s framework runs withinabroad
ecosystem, as shown in Figure 1.3 . How ever, no maer how many
retriev al and generation frameworks you encounter, it all boils down
to the following four domains and questions that go with them:
Data : Where is the data coming from? Is it reliable? Is it
suﬃcient? Are there copyright, priv acy, and security issues?
Storage : How is the data going to be stored before or after
processing it? What amount of data will be stored?
Retriev al: How will the correct data be retriev ed to augment the
user ’s input before it is suﬃcient for the generativemodel? What
type of RAG framework will be successful foraproject?
Generation : Which generativ e AI model will ﬁt into the type of RAG framework chosen?
The data, storage, and generation domains depend heavily on the
type of RAG framework you choose. Before making that choice, weneed to ev aluate the proportion of parametric and non-parametric
know ledge in the ecosystemwe are implementing. Figure 1.3
represents the RAG framework, which includes the main components
regardless of the types of RAG implemented:
Figure 1.3: The Generative RAG-ecosystem The Retriev er (D) handles data collection, processing, storage,
and retriev al The Generator (G) handles input augmentation, prompt
engineering, and generation The Evaluator (E) handles mathematical metrics, human
evaluation, and feedback The Trainer (T) handles the initial pre-trained model and ﬁne-
tuning the model Each of these four components relies on their respectiveecosystems,
which form the ov erall RAG-driv en generativ e AI pipeline. We will
refer to the domains D, G, E, and T in the following sections. Let’s
begin with the retriev er.
The retriever (D)
The retriev er component of a RAG ecosystem collects, processes,
stores, and retriev es data. The starting point of a RAG ecosystem is
thus an ingestion data process, of which the ﬁrst step is to collect
data.
Collect (D1)
In today’s world, AI data is as div erse as our media play lists. It can be
anything fromachunk of text inablog post toameme or ev en the
latest hit song streamed through headphones. And it doesn’t stop
there—the ﬁles themselv es come in all shapes and sizes. Think of PDFs ﬁlled with all kinds of details, w eb pages, plain text ﬁles that
get straight to the point, neatly organized JSON ﬁles, catchy MP3
tunes, videos in MP4 format, or images in PNG and JPG.
Furthermore, a large proportion of this data is unstructured and
found in unpredictable and complexways. Fortunately, many
platforms, such as Pinecone, OpenAI, Chroma, and Activ eloop,
provide ready-to-use tools to process and store this jungle of data.
Process (D2)
In the data collection phase (D1) of multimodal data processing,
various types of data, such as text, images, and videos, can be
extracted fromwebsites usingweb scraping techniques or any other
source of information. These data objects are then transformed to
create uniform feature representations. For example, data can be
chunked (broken into smaller parts), embedded (transformed into
vectors), and indexed to enhance searchability and retriev al
eﬃciency.
We will introduce these techniques, starting with the Building Hybrid Adaptiv e RAG in Python section of this chapter. In the following
chapters, wewill continue building more complex data processing
functions.
Storage (D3)
At this stage of the pipeline, wehavecollected and begun processingalarge amount of div erse data from the internet—videos, pictures,
texts, you name it. Now, what canwe do with all that data to make it
useful?
That’s wherevector stores like Deep Lake, Pinecone, and Chroma
come into play. Think of these as super smart libraries that don’t just
store your data but conv ert it into mathematical entities asvectors,
enabling pow erful computations. They can also applyav ariety of
indexing methods and other techniques for rapid access.
Instead of keeping the data in static spreadsheets and ﬁles, weturn it
intoadynamic, searchable system that can pow er anything from
chatbots to search engines.
Retrieval query (D4)
The retriev al process is triggered by the user input or automated
input (G1).
To retrievedata quickly, weload it into vector stores and datasets
after transforming it intoasuitable format. Then, usingacombination of keyword searches, smart embeddings, and indexing,
we can retrievethe data eﬃciently. Cosine similarity, for example,
ﬁnds items that are closely related, ensuring that the search results
are not just fast but also highly relev ant.
Once the data is retriev ed, wethen augment the input.
The generator (G)
The lines are blurred in the RAG ecosystem betw een input and
retriev al, as shown in Figure 1.3 , representing the RAG framework
and ecosystem. The user input (G1), automated or human, interacts
with the retriev al query (D4) to augment the input before sending it
to the generativemodel.
The generativ e ﬂow begins with an input.
Input (G1)
The input can beabatch of automated tasks (processing emails, for
example) or human prompts through a User Interface (UI). This
ﬂexibility allows you to seamlessly integrate AI intovarious
professional environments, enhancing productivity across industries.
Augmented input with HF (G2)
Human feedback (HF) can be added to the input, as described in the Human feedback (E2) under Ev aluator (E) section. Human feedback will
make a RAG ecosystem considerably adaptable and provide full
control ov er data retriev al and generativ e AI inputs. In the Building
hybrid adaptiv e RAG in Python section of this chapter, wewill build
augmented input with human feedback.
Prompt engineering (G3)
Both the retriev er (D) and the generator (G) rely heavily on prompt
engineering to prepare the standard and augmented message that the
generativ e AI model will haveto process. Prompt engineering brings
the retriev er’s output and the user input together.
Generation and output (G4)
The choice ofagenerativ e AI model depends on the goals ofaproject.
Llama, Gemini, GPT , and other models can ﬁtvarious requirements.
How ever, the prompt must meet each model’s speciﬁcations.
Frameworks such as LangChain, whichwe will implement in this
book, help streamline the integration ofvarious AI models into
applications by providing adaptable interfaces and tools.
The evaluator (E)
We often rely on mathematical metrics to assess the performance ofagenerativ e AI model. How ever, these metrics only giveus part of the
picture. It’s important to remember that the ultimate test of an AI’s
eﬀectiv eness comes down to human ev aluation.
Metrics (E1)
A model cannot be ev aluated without mathematical metrics, such as
cosine similarity, as with any AI system. These metrics ensure that
the retriev ed data is relev ant and accurate. By quantifying the
relationships and relev ance of data points, they provideasolid
foundation for assessing the model’s performance and reliability.
Human feedback (E2)
No generativ e AI system, whether RAG-driv en or not, and whether
the mathematical metrics seem suﬃcient or not, can elude human
evaluation. It is ultimately human ev aluation that decides ifasystem
designed for human users will be accepted or rejected, praised or
criticized.
Adaptiv e RAG introduces the human, real-life, pragmatic feedback
factor that will improvea RAG-driv en generativ e AI ecosystem. We
will implement adaptiv e RAG in Chapter 5 , Boosting RAG P erformance
with Expert Human Feedback .
The trainer (T)
A standard generativ e AI model is pre-trained withav ast amount of
general-purpose data. Then, wecan ﬁne-tune (T2) the model with
domain-speciﬁc data.
We will take this further by integrating static RAG data into the ﬁne-
tuning process in Chapter 9 , Empow ering AI Models: Fine-T uning RAG
Data and Human Feedback . We will also integrate human feedback,
which providesvaluable information that can be integrated into the
ﬁne-tuning process inav ariant of Reinforcement Learning from Human Feedback (RLHF ).
We are now ready to code entry-lev el naïv e, adv anced, and modular RAG in Python.
Naïve, advanced, and modular RAG
in code This section introduces naïv e, adv anced, and modular RAG through
basic educational examples. The program builds keyword matching,
vector search, and index-based retriev al methods. Using OpenAI’s GPT models, it generates responses based on input queries and
retriev ed documents.
The goal of the notebook is foraconv ersational agent to answ er
questions on RAG in general. We will build the retriev er from the
boom up, from scratch, in Python and run the generator with OpenAI GPT-4o in eight sections of code divided into two parts:
Part 1: Foundations and Basic Implementation
1. Environment setup for OpenAI API integration
2. Generator function using GPT-4o
3. Data setup withalist of documents ( db_records)
4. Query for user input Part 2: Adv anced T echniques and Ev aluation
1. Retriev al metrics to measure retriev al responses
2. Naïv e RAG withakeyword search and matching function
3. Adv anced RAG withvector search and index-based search
4. Modular RAG implementing ﬂexible retriev al methods To get started, open RAG_Overview.ipynb in the GitHub repository. We
will begin by establishing the foundations of the notebook and
exploring the basic implementation.
Part 1: Foundations and basic
implementation In this section, wewill set up the environment, createafunction for
the generator, deﬁneafunction to printaformaed response, and
deﬁne the user query.
The ﬁrst step is to install the environment.
The section titles of the following implementation of
the notebook follow the structure in the code. Thus,
you can follow the code in the notebook or read this
self-contained section.
1. Environment The main package to install is OpenAI to access GPT-4o through an API:
!pip install openai==1.40.3
Make sure to freeze the OpenAI v ersion you install. In RAG
framework ecosystems, wewill haveto install sev eral packages to
run adv anced RAG conﬁgurations. Oncewe havestabilized an
installation, wewill freeze theversion of the packages installed to
minimize potential conﬂicts betw een the libraries and moduleswe
implement.
Once you haveinstalled openai, you will haveto create an account on OpenAI (if you don’t haveone) and obtain an API key. Make sure to
check the costs and payment plans before running the API.
Once you havea key, store it inasafe place and retrieveit as follows
from Google Driv e, for example, as shown in the following code:
#API Key
#Store you key inafile and read it(you can type it directlyifrom google.colab import drive
drive.mount('/content/drive')
You can use Google Driveor any other method you choose to store
your key. You can read the key from a ﬁle, or you can also choose to
enter the key directly in the code:
f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline().strip()
f.close()

#The OpenAI Key
import os
import openai
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
With that, wehaveset up the main resources for our project. We will
now writeageneration function for the OpenAI model.
2. The generator The code imports openai to generate content and time to measure the
time the requests take:
import openai
from openai import OpenAI
import time
client = OpenAI()
gptmodel="gpt-4o"
start_time = time.time() # Start timing before the request We now createafunction that createsaprompt with an instruction
and the user input:
def call_llm_with_full_text(itext):
# Join all lines to formasingle string
text_input = '\n'.join(itext)
prompt = f"Please elaborate on the following content:\n{tex The function will try to call gpt-4o, adding additional information for
the model:
try:
response = client.chat.completions.create(
model=gptmodel,
messages=[
{"role": "system", "content": "You are an expert Na
{"role": "assistant", "content": "1.You can explain
{"role": "user", "content": prompt}
],
temperature=0.1 # Add the temperature parameter here
)
return response.choices[0].message.content.strip()
except Exception as e:
return str(e)
Note that the instruction messages remain general in this scenario so
that the model remains ﬂexible. The temperature is low (more precise)
and set to 0.1. If you wish for the system to be more creativ e, you can
set temperature toahighervalue, such as 0.7. How ever, in this case,
it is recommended to ask for precise responses.
We can add textwrap to format the response asanice paragraph
whenwe call the generativ e AI model:
import textwrap
def print_formatted_response(response):
# Define the width for wrapping the text
wrapper = textwrap.TextWrapper(width=80) # Set to 80 colum
wrapped_text = wrapper.fill(text=response)
# Print the formatted response withaheader and footer
print("Response:")
print("---------------")
print(wrapped_text)
print("---------------\n")
The generator is now ready to be called whenwe need
it. Due to the probabilistic nature of generativ e AI
models, it might produce diﬀerent outputs each time
we call it.
The program now implements the data retriev al functionality.
3. The Data Data collection includes text, images, audio, and video. In this
notebook, wewill focus on data retriev al through naïv e, adv anced,
and modular conﬁgurations, not data collection. We will collect and
embed data later in Chapter 2 , RAG Embedding V ector Stores with Deep Lake and OpenAI . As such, wewill assume that the datawe need has
been processed and thus collected, cleaned, and split into sentences.
We will also assume that the process included loading the sentences
into a Python list named db_records.
This approach illustrates three aspects of the RAG ecosystemwe
described in The RAG ecosystem section and the components of the
system described in Figure 1.3 :
The retriev er (D) has three data processing components, collect
(D1) , process (D2) , and storage (D3) , which are preparatory
phases of the retriev er.
The retriev er query (D4) is thus independent of the ﬁrst three
phases (collect, process, and storage) of the retriev er.
The data processing phase will often be done independently and
prior to activ ating the retriev er query, aswe will implement
starting in Chapter 2 .
This program assumes that data processing has been completed and
the dataset is ready:
db_records = [
"Retrieval Augmented Generation (RAG) representsasophisti
…/…
We can displayaformaedversion of the dataset:
import textwrap
paragraph = ' '.join(db_records)
wrapped_text = textwrap.fill(paragraph, width=80)
print(wrapped_text)
The output joins the sentences in db_records for display, as printed in
this excerpt, but db_records remains unchanged:
Retrieval Augmented Generation (RAG) representsasophisticated The program is now ready to processaquery.
4.The query The retriev er (D4 in Figure 1.3 ) query process depends on how the
datawas processed, but the query itself is simply user input or
automated input from another AI agent. We all dream of users who
introduce the best input into softw are systems, but unfortunately, in
real life, unexpected inputs lead to unpredictable behaviors. We
must, therefore, build systems that take imprecise inputs into
account.
In this section, wewill imagineasituation in which hundreds of
users in an organization haveheard the word “RAG” associated with
“LLM” and “v ector stores.” Many of them would like to understand
what these terms mean to keep up withasoftw are team that’s
deployingaconv ersational agent in their department. Afteracouple
of days, the terms they heard become fuzzy in their memory, so they
ask the conv ersational agent, GPT-4o in this case, to explain what
they remember with the following query:
query = "definearag store"
In this case, wewill simply store the main query of the topic of this
program in query, which represents the junction betw een the
retriev er and the generator. It will triggeraconﬁguration of RAG
(naïv e, adv anced, and modular ). The choice of conﬁguration will
depend on the goals of each project.
The program takes the query and sends it to a GPT-4o model to be
processed and then displays the formaed output:
# Call the function and print the result
llm_response = call_llm_with_full_text(query)
print_formatted_response(llm_response)
The output is rev ealing. Even the most pow erful generativ e AI
models cannot guess whatauser, who knows nothing about AI, is
trying to ﬁnd out in good faith. In this case, GPT-4o will answ er as
shown in this excerpt of the output:
Response:
---------------
Certainly! The content you've provided appears to beasequence
that, when combined, form the phrase "definearag store." Let's
step by step:…
… This is an indefinite article used before words that begin wit The output will seem likeahallucination, but is it really? The user
wrote the query with the good intentions of ev ery beginner trying to
learnanew topic. GPT-4o, in good faith, did what it could with the
limited context it had with its probabilistic algorithm, which might
even produceadiﬀerent response each timewe run it. How ever,
GPT-4o is beingwary of the query. It wasn’tvery clear, so it ends the
response with the following output that asks the user for more
context:
…Would you like more information oradifferent type of elaborat The user is puzzled, not knowing what to do, and GPT-4o is aw aiting
further instructions. The softw are team has to do something!
Generativ e AI is based on probabilistic algorithms. As
such, the response provided mightvary from one run
to another, providing similar (but not identical)
responses.
That is when RAG comes in to savethe situation. We will leavethis
query as it is for the whole notebook and see if a RAG-driv en GPT-4o
system can do beer.
Part 2: Advanced techniques and
evaluation In Part 2 , we will introduce naïv e, adv anced, and modular RAG. The
goal is to introduce the three methods, not to process complex
documents, whichwe will implement throughout the following
chapters of this book.
Let’s ﬁrst begin by deﬁning retriev al metrics to measure the accuracy
of the documentswe retriev e.
1. Retrieval metrics This section explores retriev al metrics, ﬁrst focusing on the role of
cosine similarity in assessing the relev ance of text documents. Then
we will implement enhanced similarity metrics by incorporating
synonym expansion and text preprocessing to improvethe accuracy
of similarity calculations betw een texts.
We will explore more metrics in the Metrics calculation and display
section in Chapter 7 , Building Scalable Know ledge-Graph-Based RAG with Wikipedia API and LlamaIndex .
In this chapter, let’s begin with cosine similarity.
Cosine similarity Cosine similarity measures the cosine of the angle betw een two
vectors. In our case, the twovectors are the user query and each
document inacorpus.
The program ﬁrst imports the class and functionwe need:
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity TfidfVectorizer imports the class that conv erts text documents intoamatrix of TF-IDF features. Term Frequency-Inv erse Document Frequency (TF-IDF ) quantiﬁes the relev ance ofaword toadocument
inacollection, distinguishing common words from those signiﬁcant
to speciﬁc texts. TF-IDF will thus quantify word relev ance in
documents using frequency across the document and inv erse
frequency across the corpus. cosine_similarity imports the function
we will use to calculate the similarity betw eenvectors.
calculate_cosine_similarity(text1, text2) then calculates the cosine
similarity betw een the query ( text1) and each record of the dataset.
The function conv erts the query text ( text1) and each record ( text2)
in the dataset intoav ector withav ectorizer. Then, it calculates and
returns the cosine similarity betw een the twovectors:
def calculate_cosine_similarity(text1, text2):
vectorizer = TfidfVectorizer(
stop_words='english',
use_idf=True,
norm='l2',
ngram_range=(1, 2), # Use unigrams and bigrams
sublinear_tf=True, # Apply sublinear TF scaling
analyzer='word' # You could also experiment with '
)
tfidf = vectorizer.fit_transform([text1, text2])
similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])
return similarity[0][0]
The key parameters of this function are:
stop_words='english: Ignores common English words to focus on
meaningful content
use_idf=True: Enables inv erse document frequencyweighting
norm='l2': Applies L2 normalization to each outputvector
ngram_range=(1, 2): Considers both single words and two-word
combinations
sublinear_tf=True: Applies logarithmic term frequency scaling
analyzer='word': Analyzes text at the word lev el Cosine similarity can be limited in some cases. Cosine similarity has
limitations when dealing with ambiguous queries because it strictly
measures the similarity based on the angle betw eenvector
representations of text. Ifauser asksav ague question like “What is
rag?” in the program of this chapter and the database primarily
contains information on “RAG” as in “retriev al-augmented
generation” for AI, not “rag cloths,” the cosine similarity score might
be low. This low score occurs because the mathematical model lacks
contextual understanding to diﬀerentiate betw een the diﬀerent
meanings of “rag.” It only computes similarity based on the presence
and frequency of similar words in the text, without grasping the
user ’s intent or the broader context of the query. Thus, ev en if the
answ ers provided are technically accurate within the av ailable
dataset, the cosine similarity may not reﬂect the relev ance accurately
if the query’s context isn’twell-represented in the data.
In this case, wecan try enhanced similarity.
Enhanced similarity Enhanced similarity introduces calculations that lev erage natural
language processing tools to beer capture semantic relationships
betw een words. Using libraries like spaCy and NL TK, it preprocesses
texts to reduce noise, expands terms with synonyms from WordNet,
and computes similarity based on the semantic richness of the
expanded vocabulary. This method aims to improvethe accuracy of
similarity assessments betw een two texts by consideringabroader
context than typical direct comparison methods.
The code contains four main functions:
get_synonyms(word): Retriev es synonyms foragiv en word from WordNet
preprocess_text(text): Conv erts all text to low ercase, lemmatizes
gets the (roots of words), and ﬁlters stopwords (common words)
and punctuation from text
expand_with_synonyms(words): Enhancesalist of words by adding
their synonyms
calculate_enhanced_similarity(text1, text2): Computes cosine
similarity betw een preprocessed and synonym-expanded text
vectors The calculate_enhanced_similarity(text1, text2) function takes two
texts and ultimately returns the cosine similarity score betw een two
processed and synonym-expanded texts. This score quantiﬁes the
textual similarity based on their semantic content and enhanced word
sets.
The code begins by downloading and importing the necessary
libraries and then runs the four functions beginning with
calculate_enhanced_similarity(text1, text2):
import spacy
import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
from collections import Counter
import numpy as np
# Load spaCy model
nlp = spacy.load("en_core_web_sm")
…
Enhanced similarity takes thisabit further in terms of metrics.
How ever, integrating RAG with generativ e AI presents multiple
challenges.
No maer which metricwe implement, wewill face the following
limitations:
Inputversus Document Length : User queries are often short,
while retriev ed documents are longer and richer, complicating
direct similarity ev aluations.
Creativ e Retriev al: Systems may creativ ely select longer
documents that meet user expectations but yield poor metric
scores due to unexpected content alignment.
Need for Human Feedback : Often, human judgment is crucial to
accurately assess the relev ance and eﬀectiv eness of retriev ed
content, as automated metrics may not fully capture user
satisfaction. We will explore this critical aspect of RAG in Chapter
5, Boosting RAG P erformance with Expert Human Feedback .
We will alw ays haveto ﬁnd the right balance betw een mathematical
metrics and human feedback.
We are now ready to create an example with naïv e RAG.
2. Naïve RAG
Naïv e RAG with keyword search and matching can proveeﬃcient
withwell-deﬁned documents within an organization, such as legal
and medical documents. These documents generally haveclear titles
or labels for images, for example. In this naïv e RAG function, wewill
implement keyword search and matching. To achievethis, wewill
applyastraightforw ard retriev al method in the code:
1. Split the query into individual keywords
2. Split each record in the dataset into keywords
3. Determine the length of the common matches
4. Choose the record with the best score The generation method will:
Augment the user input with the result of the retriev al query Request the generation model, which is gpt-4o in this case Display the response Let’s write the keyword search and matching function.
Keyword search and matching The best matching function ﬁrst initializes the best scores:
def find_best_match_keyword_search(query, db_records):
best_score = 0
best_record = None The query is then split into keywords. Each record is also split into
words to ﬁnd the common words, measure the length of common
content, and ﬁnd the best match:
# Split the query into individual keywords
query_keywords = set(query.lower().split())
# Iterate through each record in db_records
for record in db_records:
# Split the record into keywords
record_keywords = set(record.lower().split())
# Calculate the number of common keywords
common_keywords = query_keywords.intersection(record_ke
current_score = len(common_keywords)
# Update the best score and record if the current score
if current_score > best_score:
best_score = current_score
best_record = record
return best_score, best_record We now call the function, format the response, and print it:
# Assuming 'query' and 'db_records' are defined in previous cel
best_keyword_score, best_matching_record = find_best_match_keyw
print(f"Best Keyword Score: {best_keyword_score}")
#print(f"Best Matching Record: {best_matching_record}")
print_formatted_response(best_matching_record)
The main query of this notebook will be query = "definearag
store" to see if each RAG method produces an acceptable output.
The keyword search ﬁnds the best record in the list of sentences in
the dataset:
Best Keyword Score: 3
Response:
---------------
A RAG vector store isadatabase or dataset that contains vector
---------------
Let’s run the metrics.
Metrics We created the similarity metrics in the 1. Retriev al metrics section of
this chapter. We will ﬁrst apply cosine similarity:
# Cosine Similarity
score = calculate_cosine_similarity(query, best_matching_record
print(f"Best Cosine Similarity Score: {score:.3f}")
The output similarity is low, as explained in the 1. Retriev al metrics
section of this chapter. The user input is short and the response is
longer and complete:
Best Cosine Similarity Score: 0.126
Enhanced similarity will produceabeer score:
# Enhanced Similarity
response = best_matching_record
print(query,": ", response)
similarity_score = calculate_enhanced_similarity(query, respons
print(f"Enhanced Similarity:, {similarity_score:.3f}")
The score produced is higher with enhanced functionality:
definearag store : A RAG vector store isadatabase or datase Enhanced Similarity:, 0.642
The output of the query will now augment the user input.
Augmented input The augmented input is the concatenation of the user input and the
best matching record of the dataset detected with the keyword
search:
augmented_input=query+ ": "+ best_matching_record The augmented input is display ed if necessary for maintenance
reasons:
print_formatted_response(augmented_input)
The output then shows that the augmented input is ready:
Response:
---------------
definearag store: A RAG vector store isadatabase or dataset
vectorized data points.
---------------
The input is now ready for the generation process.
Gener ation We are now ready to call GPT-4o and display the formaed response:
llm_response = call_llm_with_full_text(augmented_input)
print_formatted_response(llm_response)
The following excerpt of the response shows that GPT-4o
understands the input and provides an interesting, pertinent
response:
Response:
---------------
Certainly! Let's break down and elaborate on the provided conten RAG Store: A **RAG (Retrieval-Augmented Generation) vector stor
specialized type of database or dataset that is designed to stor
vectorized data points…
Naïv e RAG can be suﬃcient in many situations. How ever, if the
volume of documents becomes too large or the content becomes more
complex, then adv anced RAG conﬁgurations will provide beer
results. Let’s now explore adv anced RAG.
3. Advanced RAG
As datasets grow larger, keyword search methods might provetoo
long to run. For instance, ifwe havehundreds of documents and each
document contains hundreds of sentences, it will become challenging
to use keyword search only. Using an index will reduce the
computational load to justafraction of the total data.
In this section, wewill go beyond searching text with keywords. We
will see how RAG transforms text data into numerical
representations, enhancing search eﬃciency and processing speed.
Unlike traditional methods that directly parse text, RAG ﬁrst conv erts
documents and user queries intovectors, numerical forms that speed
up calculations. In simple terms, avector isalist of numbers
representingvarious features of text. Simplevectors might count
word occurrences (term frequency), while more complexvectors,
known as embeddings, capture deeper linguistic paerns.
In this section, wewill implementvector search and index-based
search:
Vector Search : We will conv ert each sentence in our dataset intoanumericalvector. By calculating the cosine similarity betw een
the queryvector (the user query) and these documentvectors,
we can quickly ﬁnd the most relev ant documents.
Index-Based Search : In this case, all sentences are conv erted into
vectors using TF-IDF (Term Frequency-Inv erse Document Frequency ), a statistical measure used to ev aluate how
importantaword is toadocument inacollection. Thesevectors
act as indices inamatrix, allowing quick similarity comparisons
without parsing each document fully.
Let’s start withvector search and see these concepts in action.
3.1.Vector search Vector search conv erts the user query and the documents into
numericalvalues asvectors, enabling mathematical calculations that
retrieverelev ant data faster when dealing with large volumes of data .
The program runs through each record of the dataset to ﬁnd the best
matching document by computing the cosine similarity of the query
vector and each record in the dataset:
def find_best_match(text_input, records):
best_score = 0
best_record = None
for record in records:
current_score = calculate_cosine_similarity(text_input,
if current_score > best_score:
best_score = current_score
best_record = record
return best_score, best_record The code then calls thevector search function and displays the best
record found:
best_similarity_score, best_matching_record = find_best_match(q
print_formatted_response(best_matching_record)
The output is satisfactory:
Response:
---------------
A RAG vector store isadatabase or dataset that contains vector
points.
The response is the best one found, like with naïv e RAG. This shows
that there is no silv er bullet. Each RAG technique has its merits. The
metrics will conﬁrm this observ ation.
Metrics The metrics are the same for both similarity methods as for naïv e RAG because the same documentwas retriev ed:
print(f"Best Cosine Similarity Score: {best_similarity_score:.3
The output is:
Best Cosine Similarity Score: 0.126
And with enhanced similarity, weobtain the same output as for naïv e RAG:
# Enhanced Similarity
response = best_matching_record
print(query,": ", response)
similarity_score = calculate_enhanced_similarity(query, best_ma
print(f"Enhanced Similarity:, {similarity_score:.3f}")
The output conﬁrms the trend:
definearag store : A RAG vector store isadatabase or datase Enhanced Similarity:, 0.642
So why usevector search if it produces the same outputs as naïv e RAG? Well, inasmall dataset, ev erything looks easy. But whenwe’re
dealing with datasets of millions of complex documents, keyword
search will not capture subtleties thatvectors can. Let’s now augment
the user query with this information retriev ed.
Augmented input We add the information retriev ed to the user query with no other aid
and display the result:
# Call the function and print the result
augmented_input=query+": "+best_matching_record
print_formatted_response(augmented_input)
We only addedaspace betw een the user query and the retriev ed
information; nothing else. The output is satisfactory:
Response:
---------------
definearag store: A RAG vector store isadatabase or dataset
vectorized data points.
---------------
Let’s now see how the generativ e AI model reacts to this augmented
input.
Gener ation We now call GPT-4o with the augmented input and display the
formaed output:
# Call the function and print the result
augmented_input=query+best_matching_record
llm_response = call_llm_with_full_text(augmented_input)
print_formatted_response(llm_response)
The response makes sense, as shown in the following excerpt:
Response:
---------------
Certainly! Let's break down and elaborate on the provided conten Whilevector search signiﬁcantly speeds up the process of ﬁnding
relev ant documents by sequentially going through each record, its
eﬃciency can decrease as the dataset size increases. To address this
scalability issue, indexed search oﬀersamore adv anced solution.
Let’s now see how index-based search can accelerate document
retriev al.
3.2. Index-based search Index-based search compares thevector ofauser query not with the
directvector ofadocument’s content but with an indexedvector that
represents this content.
The program ﬁrst imports the class and functionwe need:
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity TfidfVectorizer imports the class that conv erts text documents intoamatrix of TF-IDF features. TF-IDF will quantify word relev ance in
documents using frequency across the document. The function ﬁnds
the best matches using the cosine similarity function to calculate the
similarity betw een the query and theweightedvectors of the matrix:
def find_best_match(query, vectorizer, tfidf_matrix):
query_tfidf = vectorizer.transform([query])
similarities = cosine_similarity(query_tfidf, tfidf_matrix)
best_index = similarities.argmax() # Get the index of the
best_score = similarities[0, best_index]
return best_score, best_index The function’s main tasks are:
Transform Query : Conv erts the input query into TF-IDF v ector
format using the providedvectorizer Calculate Similarities : Computes the cosine similarity betw een
the queryvector and allvectors in the tﬁdf_matrix Identify Best Match : Finds the index ( best_index) of the highest
similarity score in the results Retriev e Best Score : Extracts the highest cosine similarity score
(best_score)
The output is the best similarity score found and the best index.
The following code ﬁrst calls the datasetvectorizer and then searches
for the most similar record through its index:
vectorizer, tfidf_matrix = setup_vectorizer(db_records)
best_similarity_score, best_index = find_best_match(query, vect
best_matching_record = db_records[best_index]
Finally, the results are display ed:
print_formatted_response(best_matching_record)
The system ﬁnds the best similar document to the user ’s input query:
Response:
---------------
A RAG vector store isadatabase or dataset that contains vector
points.
---------------
We can see that the fuzzy user query producedareliable output at
the retriev al lev el before running GPT-4o.
The metrics that follow in the program are the same as for naïveand
advanced RAG withvector search. This is normal because the
document found is the closest to the user ’s input query. We will be
introducing more complex documents for RAG starting in Chapter 2 ,
RAG Embedding V ector Stores with Deep Lake and OpenAI . For now, let’s
havealook at the features that inﬂuence how the words are
represented invectors.
Feature extraction Before augmenting the input with this document, run the following
cell, which calls the setup_vectorizer(records) function again but
displays the matrix so that you can see its format. This is shown in
the following excerpt for the words “accurate” and “additional” in
one of the sentences:

Figure 1.4: Format of the matrix Let’s now augment the input.
Augmented input We will simply add the query to the best matching record inaminimalway to see how GPT-4o will react and display the output:
augmented_input=query+": "+best_matching_record
print_formatted_response(augmented_input)
The output is close to or the same as withvector search, but the
retriev al method is faster:
Response:
---------------
definearag store: A RAG vector store isadatabase or dataset
vectorized data points.
---------------
We will now plug this augmented input into the generativ e AI model.
Gener ation We now call GPT-4o with the augmented input and display the
output:
# Call the function and print the result
llm_response = call_llm_with_full_text(augmented_input)
print_formatted_response(llm_response)
The output makes sense for the user who entered the initial fuzzy
query:
Response:
---------------
Certainly! Let's break down and elaborate on the given content:
This approach workedwell inaclosed environment within an
organization inaspeciﬁc domain. In an open environment, the user
might haveto elaborate before submiingarequest.
In this section, wesaw that a TF-IDF matrix pre-computes document
vectors, enabling faster, simultaneous comparisons without repeated
vector transformations. We haveseen howvector and index-based
search can improveretriev al. How ever, in one project, wemay need
to apply naïveand adv anced RAG depending on the documentswe
need to retriev e. Let’s now see how modular RAG can improveour
system.
4. Modular RAG
Shouldwe use keyword search, v ector search, or index-based search
when implementing RAG? Each approach has its merits. The choice
will depend on sev eral factors:
Keyword search suits simple retriev al Vector search is ideal for semantic-rich documents Index-based search oﬀers speed with large data.
How ever, all three methods can perfectly ﬁt together inaproject. In
one scenario, for example, a keyword search can help ﬁnd clearly
deﬁned document labels, such as the titles of PDF ﬁles and labeled
images, before they are processed. Then, indexed search will group
the documents into indexed subsets. Finally, the retriev al program
can search the indexed dataset, ﬁndasubset, and only usevector
search to go throughalimited number of documents to ﬁnd the most
relev ant one.
In this section, wewill create a RetrievalComponent class that can be
called at each step ofaproject to perform the task required. The code
sums up the three methodswe havebuilt in this chapter and thatwe
can sum for the RetrievalComponent through its main members.
The following code initializes the class with search method choice
and preparesav ectorizer if needed. self refers to the current
instance of the class to access itsvariables, methods, and functions:
def __init__(self, method='vector'):
self.method = method
if self.method == 'vector' or self.method == 'indexed':
self.vectorizer = TfidfVectorizer()
self.tfidf_matrix = None In this case, thevector search is activ ated.
The fit method builds a TF-IDF matrix from records, and is
applicable forvector or indexed search methods:
def fit(self, records):
if self.method == 'vector' or self.method == 'indexed':
self.tfidf_matrix = self.vectorizer.fit_transform(r The retrievemethod directs the query to the appropriate search
method:
def retrieve(self, query):
if self.method == 'keyword':
return self.keyword_search(query)
elif self.method == 'vector':
return self.vector_search(query)
elif self.method == 'indexed':
return self.indexed_search(query)
The keyword search method ﬁnds the best match by counting
common keywords betw een queries and documents:
def keyword_search(self, query):
best_score = 0
best_record = None
query_keywords = set(query.lower().split())
for index, doc in enumerate(self.documents):
doc_keywords = set(doc.lower().split())
common_keywords = query_keywords.intersection(doc_k
score = len(common_keywords)
if score > best_score:
best_score = score
best_record = self.documents[index]
return best_record Thevector search method computes similarities betw een query TF-
IDF and document matrix and returns the best match:
def vector_search(self, query):
query_tfidf = self.vectorizer.transform([query])
similarities = cosine_similarity(query_tfidf, self.tfid
best_index = similarities.argmax()
return db_records[best_index]
The indexed search method usesaprecomputed TF-IDF matrix for
fast retriev al of the best-matching document:
def indexed_search(self, query):
# Assuming the tfidf_matrix is precomputed and stored
query_tfidf = self.vectorizer.transform([query])
similarities = cosine_similarity(query_tfidf, self.tfid
best_index = similarities.argmax()
return db_records[best_index]
We can now activ ate modular RAG strategies.
Modular RAG strategies We can call the retriev al component for any RAG conﬁgurationwe
wish when needed:
# Usage example
retrieval = RetrievalComponent(method='vector') # Choose from
retrieval.fit(db_records)
best_matching_record = retrieval.retrieve(query)
print_formatted_response(best_matching_record)
In this case, thevector search methodwas activ ated.
The following cells select the best record, as in the 3.1. Vector search
section, augment the input, call the generativemodel, and display the
output as shown in the following excerpt:
Response:
---------------
Certainly! Let's break down and elaborate on the content provide
**Define a RAG store:** A **RAG (Retrieval-Augmented Generation We havebuiltaprogram that demonstrated how diﬀerent search
methodologies—keyword, v ector, and index-based—can be
eﬀectiv ely integrated into a RAG system. Each method has its unique
strengths and addresses speciﬁc needs withinadata retriev al context.
The choice of method depends on the dataset size, query type, and
performance requirements, whichwe will explore in the following
chapters.
It’s now time to summarize our explorations in this chapter and
moveto the next lev el!
Summary RAG for generativ e AI relies on two main components: a retriev er
andagenerator. The retriev er processes data and deﬁnesasearch
method, such as fetching labeled documents with keywords—the
generator ’s input, an LLM, beneﬁts from augmented information
when producing sequences. We went through the three main
conﬁgurations of the RAG framework: naïv e RAG, which accesses
datasets through keywords and other entry-lev el search methods;
advanced RAG, which introduces embeddings and indexes to
improvethe search methods; and modular RAG, which can combine
naïveand adv anced RAG aswell as other ML methods.
The RAG framework relies on datasets that can contain dynamic
data. A generativ e AI model relies on parametric data through its
weights. These two approaches are not mutually exclusiv e. If the RAG datasets become too cumbersome, ﬁne-tuning can proveuseful.
When ﬁne-tuned models cannot respond to ev eryday information,
RAG can come in handy. RAG frameworks also rely heavily on the
ecosystem that provides the critical functionality to make the systems
work. We went through the main components of the RAG ecosystem,
from the retriev er to the generator, for which the trainer is necessary,
and the ev aluator. Finally, webuilt an entry-lev el naïv e, adv anced,
and modular RAG program in Python, lev eraging keyword
matching, v ector search, and index-based retriev al, augmenting the
input of GPT-4o.
Our next step in Chapter 2 , RAG Embedding V ector Stores with Deep Lake
and OpenAI , is to embed data invectors. We will store thevectors in
vector stores to enhance the speed and precision of the retriev al
functions of a RAG ecosystem.
Questions Answ er the following questions with Yes or No:
1. Is RAG designed to improvethe accuracy of generativ e AI
models?
2. Doesanaïv e RAG conﬁguration rely on complex data
embedding?
3. Is ﬁne-tuning alw aysabeer option than using RAG?
4. Does RAG retrievedata from external sources in real time to
enhance responses?
5. Can RAG be applied only to text-based data?
6. Is the retriev al process in RAG triggered byauser or automated
input?
7. Are cosine similarity and TF-IDF both metrics used in adv anced RAG conﬁgurations?
8. Does the RAG ecosystem include only data collection and
generation components?
9. Can adv anced RAG conﬁgurations process multimodal data
such as images and audio?
10. Is human feedback irrelev ant in ev aluating RAG systems?
References Retriev al-Augmented Generation for Know ledge-Intensiv e NLP T asks
by P atrick Lewis, Ethan P erez, Aleksandra Piktus, et al.:
https://arxiv.org/abs/2005.11401
Retriev al-Augmented Generation for Large Language Models: A
Surv ey by Yunfan Gao, Yun Xiong, Xinyu Gao, et al.:
https://arxiv.org/abs/2312.10997
OpenAI models:
https://platform.openai.com/docs/models Further reading To understand why RAG-driv en Generativ e AI transparency is
recommended, please see
https://hai.stanford.edu/news/introducing-
foundation-model-transparency-index Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag

2
RAG Embedding V ector Stores
with Deep Lake and OpenAI
There will comeapoint in the execution of your project where
complexity is unavoidable when implementing RAG-driv en
generativ e AI. Embeddings transform bulky structured or
unstructured texts into compact, high-dimensionalvectors that
capture their semantic essence, enabling faster and more eﬃcient
information retriev al. How ever, wewill inevitably be faced withastorage issue as the creation and storage of document embeddings
become necessary when managing increasingly large datasets. You
could ask the question at this point, why not use keywords instead of
embeddings? And the answ er is simple: although embeddings
require more storage space, they capture the deeper semantic
meanings of texts, with more nuanced and context-aw are retriev al
compared to the rigid and often-matched keywords. This results in
beer, more pertinent retriev als. Hence, our option is to turn tovector
stores in which embeddings are organized and rapidly accessible.
We will begin this chapter by exploring how to go from raw data to
an Activ eloop Deep Lakevector store via loading OpenAI embedding
models. This requires installing and implementing sev eral cross-
platform packages, which leads us to the architecture of such
systems. We will organize our RAG pipeline into separate
components because breaking down the RAG pipeline into
independent parts will enable sev eral teams to work onaproject
simultaneously. We will then set the blueprint for a RAG-driv en
generativ e AI pipeline. Finally, wewill buildathree-component RAG
pipeline from scratch in Python with Activ eloop Deep Lake, OpenAI,
and custom-built functions.
This coding journey will take us into the depths of cross-platform
environment issues with packages and dependencies. We will also
face the challenges of chunking data, embeddingvectors, and loading
them onvector stores. We will augment the input of a GPT-4o model
with retriev al queries and produce solid outputs. By the end of this
chapter, you will fully understand how to lev erage the pow er of
embedded documents invector stores for generativ e AI.
To sum up, this chapter cov ers the following topics:
Introducing document embeddings andvector stores How to break a RAG pipeline into independent components Building a RAG pipeline from raw data to Activ eloop Deep Lake Facing the environmental challenge of cross-platform packages
and libraries Leveraging the pow er of LLMs to embed data with an OpenAI
embedding model Querying an Activ eloop Deep Lakevector store to augment user
inputs Generativesolid augmented outputs with OpenAI GPT-4o Let’s begin by learning how to go from raw data toav ector store.
Fromraw data to embeddings in
vector stores Embeddings conv ert any form of data (text, images, or audio) into
real numbers. Thus, a document is conv erted intoav ector. These
mathematical representations of documents allow us to calculate the
distances betw een documents and retrievesimilar data.
The raw data (books, articles, blogs, pictures, or songs) is ﬁrst
collected and cleaned to removenoise. The prepared data is then fed
intoamodel such as OpenAI text-embedding-3-small, which will
embed the data. Activ eloop Deep Lake, for example, whichwe will
implement in this chapter, will breakatext down into pre-deﬁned
chunks deﬁned byacertain number of characters. The size ofachunk
could be 1,000 characters, for instance. We can let the system optimize
these chunks, aswe will implement them in the Optimizing chunking
section of the next chapter. These chunks of text make it easier to
process large amounts of data and provide more detailed
embeddings ofadocument, as shown here:
Figure 2.1: Excerpt of an Activeloop vector store dataset record Transparency has been the holy grail in AI since the beginning of
parametric models, in which the information is buried in learned
parameters that produce black box systems. RAG isagame changer,
as shown in Figure 2.1 , because the content is fully traceable:
Left side (T ext): In RAG frameworks, ev ery piece of generated
content is traceable back to its source data, ensuring the output’s
transparency. The OpenAI generativemodel will respond, taking
the augmented input into account.
Right side (Embeddings): Data embeddings are directly visible
and linked to the text, contrasting with parametric models where
data origins are encoded within model parameters.
Oncewe haveour text and embeddings, the next step is to store them
eﬃciently for quick retriev al. This is where vector stores come into
play. A vector store isaspecialized database designed to handle
high-dimensional data like embeddings. We can create datasets on
serverless platforms such as Activ eloop, as shown in Figure 2.2 . We
can create and access them in code through an API, aswe will do in
the Building a RAG pipeline section of this chapter.
Figure 2.2: Managing datasets with vector stores Another feature of vector stores is their ability to retrievedata with
optimized methods. Vector stores are built with pow erful indexing
methods, whichwe will discuss in the next chapter. This retrieving
capacity allows a RAG model to quickly ﬁnd and retrievethe most
relev ant embeddings during the generation phase, augment user
inputs, and increase the model’s ability to produce high-quality
output.
We will now see how to organize a RAG pipeline that goes from data
collection, processing, and retriev al to augmented-input generation.
Organizing RAG inapipeline A RAG pipeline will typically collect data and prepare it by cleaning
it, for example, chunking the documents, embedding them, and
storing them inav ector store dataset. Thevector dataset is then
queried to augment the user input ofagenerativ e AI model to
produce an output. How ever, it is highly recommended not to run
this sequence of RAG in one single program when it comes to usingavector store. We should at least separate the process into three
components:
Data collection and preparation Data embedding and loading into the dataset ofav ector store Querying thevectorized dataset to augment the input ofagenerativ e AI model to producearesponse Let’s go through the main reasons for this component approach:
Specialization , which will allow each member ofateam to do
what they are best at, either collecting and cleaning data,
running embedding models, managingvector stores, or
tweaking generativ e AI models.
Scalability , making it easier to upgrade separate components as
the technology evolv es and scale the diﬀerent components with
specialized methods. Storing raw data, for example, can be
scaled onadiﬀerent serv er than the cloud platform, where the
embeddedvectors are stored inav ectorized dataset.
Parallel dev elopment , which allows each team to adv ance at
their pace withoutwaiting for others. Improv ements can be
made continually on one component without disrupting the
processes of the other components.
Maintenance is component-independent. One team can work on
one component without aﬀecting the other parts of the system.
For example, if the RAG pipeline is in production, users can
continue querying and running generativ e AI through thevector
store whileateam ﬁxes the data collection component.
Security concerns and priv acy are minimized because each team
can work separately with speciﬁc authorization, access, and roles
for each component.
Aswe can see, in real-life production environments or large-scale
projects, it is rare forasingle program or team to manage end-to-end
processes. We are now ready to draw the blueprint of the RAG
pipeline thatwe will build in Python in this chapter.
A RAG-driven generative AI pipeline Let’s diveinto whatareal-life RAG pipeline looks like. Imaginewe’reateam that has to deliv erawhole system in justafewweeks. Right
oﬀ the bat, w e’re bombarded with questions like:
Who’s going to gather and clean up all the data?
Who’s going to handle seing up OpenAI’s embedding model?
Who’s writing the code to get those embeddings up and running
and managing thevector store?
Who’s going to take care of implementing GPT-4 and managing
what it spits out?
Withinafew minutes, ev eryone starts looking prey worried. The
whole thing feels ov erwhelming—like, seriously, who would ev en
think about tackling all that alone?
So here’s whatwe do. We split into three groups, each of us taking on
diﬀerent parts of the pipeline, as shown in Figure 2.3 :
Figure 2.3: RAG pipeline components Each of the three groups has one component to implement:
Data Collection and Prep (D1 and D2) : One team takes on
collecting the data and cleaning it.
Data Embedding and Storage (D2 and D3) : Another team works
on geing the data through OpenAI’s embedding model and
stores thesevectors in an Activ eloop Deep Lake dataset.
Augmented Generation (D4, G1-G4, and E1) : The last team
handles the big job of generating content based on user input
and retriev al queries. They use GPT-4 for this, and ev en though
it sounds likealot, it’s actuallyabit easier because they aren’t
waiting on anyone else—they just need the computer to do its
calculations and ev aluate the output.
Suddenly, the project doesn’t seem so scary. Everyone has their part
to focus on, andwe can all work without being distracted by the
other teams. Thisway, wecan all movefaster and get the job done
without the hold-ups that usually slow things down.
The organization of the project, represented in Figure 2.3 , isav ariant
of the RAG ecosystem’s framework represented in Figure 1.3 of Chapter 1 , Why Retriev al Augmented Generation?
We can now begin building a RAG pipeline.
Building a RAG pipeline We will now build a RAG pipeline by implementing the pipeline
described in the previous section and illustrated in Figure 2.3 . We will
implement three components assuming that three teams ( Team #1,
Team #2, and Team #3) work in parallel to implement the pipeline:
Data collection and preparation by Team #1
Data embedding and storage by Team #2
Augmented generation by Team #3
The ﬁrst step is to set up the environment for these components.
Setting up the environment Let’s face it here and now. Installing cross-platform, cross-library
packages with their dependencies can be quite challenging! It is
important to take this complexity into account and be prepared to get
the environment running correctly. Each package has dependencies
that may haveconﬂictingversions. Even ifwe adapt theversions, an
application may not run as expected anymore. So, take your time to
install the rightversions of the packages and dependencies.
We will only describe the environment once in this section for all
three components and refer to this section when necessary.
The installation packages and libraries To build the RAG pipeline in this section, wewill need packages and
need to freeze the packageversions to prev ent dependency conﬂicts
and issues with the functions of the libraries, such as:
Possible conﬂicts betw een theversions of the dependencies.
Possible conﬂicts when one of the libraries needs to be updated
for an application to run. For example, in August 2024, installing Deep Lake required Pillow version 10.x.x and Google Colab’s
versionwas 9.x.x. Thus, itwas necessary to uninstall Pillow and
reinstall it witharecentversion before installing Deep Lake.
Google Colab will no doubt update Pillow. Many cases such as
this occur inafast-moving market.
Possible deprecations if theversions remain frozen for too long.
Possible issues if theversions are frozen for too long and bugs
are not corrected by upgrades.
Thus, ifwe freeze theversions, an application may remain stable for
some time but encounter issues. But ifwe upgrade theversions too
quickly, some of the other libraries may not work anymore. There is
no silv er bullet! It’sacontinual quality control process.
For our program, in this section, wewill freeze theversions. Let’s
now go through the installation steps to create the environment for
our pipeline.
The components involved in the
installation process Let’s begin by describing the components that are installed in the Installing the environment section of each notebook. The components
are not necessarily installed in all notebooks; this section serv es as an
inventory of the packages.
In the ﬁrst pipeline section, 1. Data collection and preparation , we will
only need to install Beautiful Soup and Requests:
!pip install beautifulsoup4==4.12.3
!pip install requests==2.31.0
This explains why this component of the pipeline should remain
separate. It’sastraightforw ard job foradev eloper who enjoys
creating interfaces to interact with theweb. It’s alsoaperfect ﬁt forajunior dev eloper whowants to get involv ed in data collection and
analysis.
The two other pipeline componentswe will build in this section, 2.
Data embedding and storage and 3. Augmented generation , will require
more aention aswell as the installation of requirements01.txt, as
explained in the previous section. For now, let’s continue with the
installation step by step.
Mountingadrive In this scenario, the program mounts Google Drivein Google Colab
to safely read the OpenAI API key to access OpenAI models and the Activ eloop API token for authentication to access Activ eloop Deep Lake datasets:
#Google Drive option to store API Keys
#Store your key inafile and read it(you can type it directly
from google.colab import drive
drive.mount('/content/drive')
You can choose to store your keys and tokens elsewhere. Just make
sure they are inasafe location.
Creatingasubprocess to download files from GitHub The goal here is to writeafunction to download the grequests.py ﬁle
from GitHub. This program containsafunction to download ﬁles
using curl, with the option to addapriv ate token if necessary:
import subprocess
url = "https://raw.githubusercontent.com/Denis2054/RAG-Driven-G
output_file = "grequests.py"
# Prepare the curl command using the private token
curl_command = [
"curl",
"-o", output_file,
url
]
# Execute the curl command
try:
subprocess.run(curl_command, check=True)
print("Download successful.")
except subprocess.CalledProcessError:
print("Failed to download the file.")
The grequests.py ﬁle containsafunction that can, if necessary, acceptapriv ate token or any other security system that requires credentials
when retrieving data with curl commands:
import subprocess
import os
# addaprivate token after the filename if necessary
def download(directory, filename):
# The base URL of the image files in the GitHub repository
base_url = 'https://raw.githubusercontent.com/Denis2054/RAG
# Complete URL for the file
file_url = f"{base_url}{directory}/{filename}"
# Use curl to download the file, including an Authorization
try:
# Prepare the curl command with the Authorization heade
#curl_command = f'curl -H "Authorization: token {privat
curl_command = f'curl -H -o {filename} {file_url}'
# Execute the curl command
subprocess.run(curl_command, check=True, shell=True)
print(f"Downloaded '{filename}' successfully.")
except subprocess.CalledProcessError:
print(f"Failed to download '{filename}'. Check the URL,
Installing requirements Now, wewill install the requirements for this section when working
with Activ eloop Deep Lake and OpenAI. We will only need:
!pip install deeplake==3.9.18
!pip install openai==1.40.3
As of August 2024, Google Colab’sversion of Pillow conﬂicts with
deeplake's package. How ever, the deeplake installation package deals
with this automatically. All you haveto do is restart the session and
run it again, which is why pip install deeplake==3.9.18 is the ﬁrst
line of each notebook it is installed in.
After installing the requirements, wemust runaline of code for Activ eloop to activ ateapublic DNS serv er:
# For Google Colab and Activeloop(Deeplake library)
#This line writes the string "nameserver 8.8.8.8" to the file.
#should use is at the IP address 8.8.8.8, which is one of Googl
with open('/etc/resolv.conf', 'w') as file:
file.write("nameserver 8.8.8.8")
Authentication process You will need to sign up to OpenAI to obtain an API key:
https://openai.com/ . Make sure to check the pricing policy before
using the key. First, let’s activ ate OpenAI’s API key:
#Retrieving and setting OpenAI API key
f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline().strip()
f.close()
#The OpenAI API key
import os
import openai
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
Then, weactiv ate Activ eloop’s API token for Deep Lake:
#Retrieving and setting Activeloop API token
f = open("drive/MyDrive/files/activeloop.txt", "r")
API_token=f.readline().strip()
f.close()
ACTIVELOOP_TOKEN=API_token
os.environ['ACTIVELOOP_TOKEN'] =ACTIVELOOP_TOKEN
You will need to sign up on Activ eloop to obtain an API token:
https://www.activeloop.ai/ . Again, make sure to check the
pricing policy before using the Activ eloop token.
Once the environment is installed, you can hide the Installing the
environment cellswe just ran to focus on the content of the pipeline
components, as shown in Figure 2.4 :
Figure 2:4: Hiding the installation cells The installation cells will then be hidden but can still be run, as
shown in Figure 2.5 :
Figure 2.5: Running hidden cells We can now focus on the pipeline components for each pipeline
component. Let’s begin with data collection and preparation.
1. Data collection and preparation Data collection and preparation is the ﬁrst pipeline component, as
described earlier in this chapter. Team #1 will only focus on their
component, as shown in Figure 2.6 :
Figure 2.6: Pipeline component #1: Data collection and preparation Let’s jump in and lendahand to Team #1. Our work is clearly
deﬁned, sowe can enjoy the time taken to implement the component.
We will retrieveand process 10 W ikipedia articles that provideacomprehensiveview ofvarious aspects of space exploration:
Space exploration : Overview of the history, technologies,
missions, and plans involv ed in the exploration of space
(https://en.wikipedia.org/wiki/Space_exploration )
Apollo program : Details about the NASA program that landed
the ﬁrst humans on the Moon and its signiﬁcant missions
(https://en.wikipedia.org/wiki/Apollo_program )
Hubble Space T elescope : Information on one of the most
signiﬁcant telescopes ev er built, which has been crucial in many
astronomical discov eries
(https://en.wikipedia.org/wiki/Hubble_Space_Telescop
e)
Mars rov er: Insight into the rov ers that havebeen sent to Mars to
study its surface and environment
(https://en.wikipedia.org/wiki/Mars_rover )
International Space Station (ISS) : Details about the ISS, its
construction, international collaboration, and its role in space
research
(https://en.wikipedia.org/wiki/International_Space_S
tation )
SpaceX : Cov ers the history, achiev ements, and goals of SpaceX,
one of the most inﬂuential private spaceﬂight companies
(https://en.wikipedia.org/wiki/SpaceX )
Juno (spacecraft) : Information about the NASA space probe that
orbits and studies Jupiter, its structure, and moons
(https://en.wikipedia.org/wiki/Juno_(spacecraft))
Voyager program : Details on the Voy ager missions, including
their contributions to our understanding of the outer solar
system and interstellar space
(https://en.wikipedia.org/wiki/Voyager_program )
Galileo (spacecraft) : Overview of the mission that studied Jupiter and its moons, providingvaluable data on the gas giant
and its system
(https://en.wikipedia.org/wiki/Galileo_(spacecraft) )
Kepler space telescope : Information about the space telescope
designed to discov er Earth-size planets orbiting other stars
(https://en.wikipedia.org/wiki/Kepler_Space_Telescop
e)
These articles coverawide range of topics in space exploration, from
historical programs to modern technological adv ances and missions.
Now, open 1-Data_collection_preparation.ipynb in the GitHub
repository. We will ﬁrst collect the data.
Collecting the data We just need import requests for the HTTP requests, from bs4 import BeautifulSoup for HTML parsing, and import re, the regular
expressions module:
import requests
from bs4 import BeautifulSoup
import re We then select the URLswe need:
# URLs of the Wikipedia articles
urls = [
"https://en.wikipedia.org/wiki/Space_exploration",
"https://en.wikipedia.org/wiki/Apollo_program",
"https://en.wikipedia.org/wiki/Hubble_Space_Telescope",
"https://en.wikipedia.org/wiki/Mars_over",
"https://en.wikipedia.org/wiki/International_Space_Station"
"https://en.wikipedia.org/wiki/SpaceX",
"https://en.wikipedia.org/wiki/Juno_(spacecraft)",
"https://en.wikipedia.org/wiki/Voyager_program",
"https://en.wikipedia.org/wiki/Galileo_(spacecraft)",
"https://en.wikipedia.org/wiki/Kepler_Space_Telescope"
]
This list is in code. How ever, it could be stored inadatabase, a ﬁle, or
any other format, such as JSON. We can now prepare the data.
Preparing the data First, wewriteacleaning function. This function remov es numerical
references such as [1] [2] fromagiv en text string, using regular
expressions, and returns the cleaned text:
def clean_text(content):
# Remove references that usually appear as [1], [2], etc.
content = re.sub(r'\[\d+\]', '', content)
return content Then, wewriteaclassical fetch and clean function, which will returnanice and clean text by extracting the contentwe need from the
documents:
def fetch_and_clean(url):
# Fetch the content of the URL
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')
# Find the main content of the article, ignoring side boxes
content = soup.find('div', {'class': 'mw-parser-output'})
# Remove the bibliography section, which generally follows
for section_title in ['References', 'Bibliography', 'Extern
section = content.find('span', id=section_title)
if section:
# Remove all content from this section to the endofor sib in section.parent.find_next_siblings():
sib.decompose()
section.parent.decompose()
# Extract and clean the text
text = content.get_text(separator=' ', strip=True)
text = clean_text(text)
return text Finally, wewrite the content in llm.txt ﬁle for the team working on
the data embedding and storage functions:
# File to write the clean text
with open('llm.txt', 'w', encoding='utf-8') as file:
for url in urls:
clean_article_text = fetch_and_clean(url)
file.write(clean_article_text + '\n')
print("Content written to llm.txt")
The output conﬁrms that the text has been wrien:
Content written to llm.txt The program can be modiﬁed to savethe data in other formats and
locations, as required foraproject’s speciﬁc needs. The ﬁle can then
be veriﬁed beforewe moveon to the next batch of data to retrieveand process:
# Open the file and read the first 20 lines
with open('llm.txt', 'r', encoding='utf-8') as file:
lines = file.readlines()
# Print the first 20 lines
for line in lines[:20]:
print(line.strip())
The output shows the ﬁrst lines of the document that will be
processed:
Exploration of space, planets, and moons "Space Exploration" red This component can be managed byateam that enjoys searching for
documents on theweb or withinacompany’s data environment. The
team will gain experience in identifying the best documents foraproject, which is the foundation of any RAG framework.
Team #2 can now work on the data to embed the documents and store
them.
2. Data embedding and storage Team #2's job is to focus on the second component of the pipeline.
They will receivebatches of prepared data to work on. They don’t
have to worry about retrieving data. Team #1 has their back with their
data collection and preparation component.
Figure 2.7: Pipeline component #2: Data embedding and storage Let’s now jump in and help Team #2 to get the job done. Open 2-
Embeddings_vector_store.ipynb in the GitHub Repository. We will
embed and store the data provided by Team #1 and retrievea batch of
documents to work on.
Retrievingabatch of prepared
documents First, wedownloadabatch of documents av ailable onaserv er and
provided by Team #1, which is the ﬁrst ofacontinual stream of
incoming documents. In this case, weassume it’s the space
exploration ﬁle:
from grequests import download
source_text = "llm.txt"
directory = "Chapter02"
filename = "llm.txt"
download(directory, filename)
Note that source_text = "llm.txt" will be used by the function that
will add the data to ourvector store. We then brieﬂy check the
document just to be sure, knowing that Team #1 has alreadyveriﬁed
the information:
# Open the file and read the first 20 lines
with open('llm.txt', 'r', encoding='utf-8') as file:
lines = file.readlines()
# Print the first 20 lines
for line in lines[:20]:
print(line.strip())
The output is satisfactory, as shown in the following excerpt:
Exploration of space, planets, and moons "Space Exploration" red We will now chunk the data. We will determineachunk size deﬁned
by the number of characters. In this case, it is CHUNK_SIZE = 1000, but
we can select chunk sizes using diﬀerent strategies. Chapter 7 ,
Building Scalable Know ledge-Graph-based RAG with W ikipedia API and LlamaIndex , will take chunk size optimization further with automated
seamless chunking.
Chunking is necessary to optimize data processing: selecting portions
of text, embedding, and loading the data. It also makes the embedded
dataset easier to query. The following code chunksadocument to
complete the preparation process:
with open(source_text, 'r') as f:
text = f.read()
CHUNK_SIZE = 1000
chunked_text = [text[i:i+CHUNK_SIZE] foriin range(0,len(text)
We are now ready to createav ector store tovectorize data or add
data to an existing one.
Verifying if the vector store exists and
creating it if not First, weneed to deﬁne the path of our Activ eloopvector store path,
whether our dataset exists or not:
vector_store_path = "hub://denis76/space_exploration_v1"
Make sure to replace
`hub://denis76/space_exploration_v1` with your
organization and dataset name.
Then, wewriteafunction to aempt to load thevector store or
automatically create one if it doesn’t exist:
from deeplake.core.vectorstore.deeplake_vectorstore import Vect
import deeplake.util
try:
# Attempt to load the vector store
vector_store = VectorStore(path=vector_store_path)
print("Vector store exists")
except FileNotFoundError:
print("Vector store does not exist. You can create it.")
# Code to create the vector store goes here
create_vector_store=True The output conﬁrms that thevector store has been created:
Your Deep Lake dataset has been successfully created!
Vector store exists We now need to create an embedding function.
The embedding function The embedding function will transform the chunks of datawe
created intovectors to enablevector-based search. In this program,
we will use "text-embedding-3-small" to embed the documents.
OpenAI has other embedding models that you can use:
https://platform.openai.com/docs/models/embeddings .
Chapter 6 , Scaling RAG Bank Customer Data with Pinecone , provides
alternativecode for embedding models in the Embedding section. In
any case, it is recommended to ev aluate embedding models before
choosing one in production. Examine the characteristics of each
embedding model, as described by OpenAI, focusing on their length
and capacities. text-embedding-3-small was chosen in this case
because it stands out asarobust choice for eﬃciency and speed:
def embedding_function(texts, model="text-embedding-3-small"):
if isinstance(texts, str):
texts = [texts]
texts = [t.replace("\n", " ") fortin texts]
return [data.embedding for data in openai.embeddings.create(
The text-embedding-3-small text embedding model from OpenAI
typically uses embeddings witharestricted number of dimensions, to
balance obtaining enough detail in the embeddings with large
computational workloads and storage space. Make sure to check the
model page and pricing information before running the code:
https://platform.openai.com/docs/guides/embeddings/embed
ding-models .
We are now all set to begin populating thevector store.
Adding data to the vector store We set the adding data ﬂag to True:
add_to_vector_store=True
if add_to_vector_store == True:
with open(source_text, 'r') as f:
text = f.read()
CHUNK_SIZE = 1000
chunked_text = [text[i:i+1000] foriin range(0, len(te
vector_store.add(text = chunked_text,
embedding_function = embedding_function,
embedding_data = chunked_text,
metadata = [{"source": source_text}]*len(chunked_
The source text, source_text = "llm.txt", has been embedded and
stored. A summary of the dataset’s structure is display ed, showing
that the datasetwas loaded:
Creating 839 embeddings in 2 batches of size 500:: 100%|████████
Dataset(path='hub://denis76/space_exploration_v1', tensors=['tex
tensor htype shape dtype compression
------- ------- ------- ------- -------
text text (839, 1) str None
metadata json (839, 1) str None
embedding embedding (839, 1536) float32 None
id text (839, 1) str None Observethat the dataset contains four tensors:
embedding: Each chunk of data is embedded inav ector
id: The ID isastring of characters and is unique
metadata: The metadata contains the source of the data—in this
case, the llm.txt ﬁle.
text: The content ofachunk of text in the dataset This dataset structure canvary from one project to another, aswe will
see in Chapter 4 , Multimodal Modular RAG for Drone T echnology . We
can also visualize how the dataset is organized at any time toverify
the structure. The following code will display the summary thatwas
just display ed:
# Print the summary of the Vector Store
print(vector_store.summary())
We can also visualizevector store information ifwe wish.
Vector store information Activ eloop’s API reference provides us with all the informationwe
need to manage our datasets:
https://docs.deeplake.ai/en/latest/ . We can visualize our
datasets oncewe sign in at
https://app.activeloop.ai/datasets/mydatasets/ .
We can also load our dataset in one line of code:
ds = deeplake.load(vector_store_path)
The output providesapath to visualize our datasets and query and
explore them online:
This dataset can be visualized in Jupyter Notebook by ds.visuali
hub://denis76/space_exploration_v1 loaded successfully.
You can also access your dataset directly on Activ eloop by signing in
and going to your datasets. You will ﬁnd online dataset exploration
tools to query your dataset and more, as shown here:
Figure 2.8: Querying and exploring a Deep Lake dataset online.
Among the many functions av ailable, wecan display the estimated
size ofadataset:
#Estimates the size in bytes of the dataset.
ds_size=ds.size_approx()
Oncewe haveobtained the size, wecan conv ert it into megabytes and
gigabytes:
# Convert bytes to megabytes and limit to 5 decimal places
ds_size_mb = ds_size / 1048576
print(f"Dataset size in megabytes: {ds_size_mb:.5f} MB")
# Convert bytes to gigabytes and limit to 5 decimal places
ds_size_gb = ds_size / 1073741824
print(f"Dataset size in gigabytes: {ds_size_gb:.5f} GB")
The output shows the size of the dataset in megabytes and gigabytes:
Dataset size in megabytes: 55.31311 MB
Dataset size in gigabytes: 0.05402 GB
Team #2's pipeline component for data embedding and storage seems
to be working. Let’s now explore augmented generation.
3. Augmented input generation Augmented generation is the third pipeline component. We will use
the datawe retriev ed to augment the user input. This component
processes the user input, queries thevector store, augments the input,
and calls gpt-4-turbo, as shown in Figure 2.9 :
Figure 2.9: Pipeline component #3: Augmented input generation Figure 2.9 shows that pipeline component #3 fully deserv es its Retriev al Augmented Generation (RAG ) name. How ever, it would
be impossible to run this component without the work put in by Team
#1 and Team #2 to provide the necessary information to generate
augmented input content.
Let’s jump in and see how Team #3 does the job. Open 3-
Augmented_Generation.ipynb in the GitHub repository. The Installing
the environment section of the notebook is described in the Seing up
the environment section of this chapter. We select thevector store
(replace thevector store path with yourvector store):
vector_store_path = "hub://denis76/space_exploration_v1"
Then, weload the dataset:
from deeplake.core.vectorstore.deeplake_vectorstore import Vect
import deeplake.util
ds = deeplake.load(vector_store_path)
We printaconﬁrmation message that thevector store exists. At this
point stage, Team #2 previously ensured that ev erythingwas working
well, sowe can just moveahead rapidly:
vector_store = VectorStore(path=vector_store_path)
The output conﬁrms that the dataset exists and is loaded:
Deep Lake Dataset in hub://denis76/space_exploration_v1 already We assume that pipeline component #2, as built in the Data embedding
and storage section, has created and populated the vector_store and
hasveriﬁed that it can be queried. Let’s now process the user input.
Input and query retrieval We will need the embedding function to embed the user input:
def embedding_function(texts, model="text-embedding-3-small"):
if isinstance(texts, str):
texts = [texts]
texts = [t.replace("\n", " ") fortin texts]
return [data.embedding for data in openai.embeddings.create(
Note thatwe are using the same embedding model as the data
embedding and storage component to ensure full compatibility
betw een the input and thevector dataset: text-embedding-ada-002.
We can now either use an interactiveprompt for an input or process
user inputs in batches. In this case, weprocessauser input that has
already been entered that could be fetched fromauser interface, for
example.
We ﬁrst ask the user for an input or deﬁne one:
def get_user_prompt():
# Request user input for the search prompt
return input("Enter your search query: ")
# Get the user's search query
#user_prompt = get_user_prompt()
user_prompt="Tell me about space exploration on the Moon and Ma We then plug the prompt into the search query and store the output
in search_results:
search_results = vector_store.search(embedding_data=user_prompt The user prompt and search results stored in search_results are
formaed to be display ed. First, let’s print the user prompt:
print(user_prompt)
We can also wrap the retriev ed text to obtainaformaed output:
# Function to wrap text toaspecified width
def wrap_text(text, width=80):
lines = []
while len(text) > width:
split_index = text.rfind(' ', 0, width)
if split_index == -1:
split_index = width
lines.append(text[:split_index])
text = text[split_index:].strip()
lines.append(text)
return '\n'.join(lines)
How ever, let’s only select one of the top results and print it:
import textwrap
# Assuming the search results are ordered with the top resultftop_score = search_results['score'][0]
top_text = search_results['text'][0].strip()
top_metadata = search_results['metadata'][0]['source']
# Print the top search result
print("Top Search Result:")
print(f"Score: {top_score}")
print(f"Source: {top_metadata}")
print("Text:")
print(wrap_text(top_text))
The following output shows thatwe havea reasonably good match:
Top Search Result:
Score: 0.6016581654548645
Source: llm.txt Text:
Exploration of space, planets, and moons "Space Exploration" red For the company, see SpaceX . For broader coverage of this topic Exploration . Buzz Aldrin takingacore sample of the Moon durin We are ready to augment the input with the additional information
we haveretriev ed.
Augmented input The program adds the top retriev ed text to the user input:
augmented_input=user_prompt+" "+top_text
print(augmented_input)
The output displays the augmented input:
Tell me about space exploration on the Moon and Mars. Exploratio
gpt-4o can now process the augmented input and generate content:
from openai import OpenAI
client = OpenAI()
import time
gpt_model = "gpt-4o"
start_time = time.time() # Start timing before the request Note thatwe are timing the process. We now write the generativ e AI
call, adding roles to the messagewe create for the model:
def call_gpt4_with_full_text(itext):
# Join all lines to formasingle string
text_input = '\n'.join(itext)
prompt = f"Please summarize or elaborate on the followingctry:
response = client.chat.completions.create(
model=gpt_model,
messages=[
{"role": "system", "content": "You areaspace
{"role": "assistant", "content": "You can read
{"role": "user", "content": prompt}
],
temperature=0.1 # Fine-tune parameters as needed
)
return response.choices[0].message.content.strip()
except Exception as e:
return str(e)
The generativemodel is called with the augmented input; the
response time is calculated and display ed along with the output:
gpt4_response = call_gpt4_with_full_text(augmented_input)
response_time = time.time() - start_time # Measure response ti
print(f"Response Time: {response_time:.2f} seconds") # Printrprint(gpt_model, "Response:", gpt4_response)
Note that the raw output is display ed with the response time:
Response Time: 8.44 seconds
gpt-4o Response: Space exploration on the Moon and Mars has been Let’s format the output with textwrap and print the result.
print_formatted_response(response) ﬁrst checks if the response
returned contains Markdown features. If so, it will format the
response; if not, it will performastandard output text wrap:
import textwrap
import re
from IPython.display import display, Markdown, HTML
import markdown
def print_formatted_response(response):
# Check for markdown by looking for patterns like headers,
markdown_patterns = [
r"^#+\s", # Headers
r"^\*+", # Bullet points
r"\*\*", # Bold
r"_", # Italics
r"\[.+\]\(.+\)", # Links
r"-\s", # Dashes used for lists
r"\`\`\`" # Code blocks
]
# If any pattern matches, assume the response is in markdow
if any(re.search(pattern, response, re.MULTILINE) for patte
# Markdown detected, convert to HTML for nicer display
html_output = markdown.markdown(response)
display(HTML(html_output)) # Use display(HTML()) to re
else:
# No markdown detected, wrap and print as plain text
wrapper = textwrap.TextWrapper(width=80)
wrapped_text = wrapper.fill(text=response)
print("Text Response:")
print("--------------------")
print(wrapped_text)
print("--------------------\n")
print_formatted_response(gpt4_response)
The output is satisfactory:
Moon Exploration Historical Missions:
1. Apollo Missions: NASA's Apollo program, particularly Apol
2. Lunar Missions: Various missions have been conducted to e Scientific Goals:
3. Geological Studies: Understanding the Moon's composition,
4. Resource Utilization: Investigating the potential for min Future Plans:
1. Artemis Program: NASA's initiative to return humans to th
2. International Collaboration: Partnerships with other spac Mars Exploration Robotic Missions:
1. Rovers: NASA's rovers like Curiosity and Perseverance hav
2. Orbiters: Various orbiters have been mapping Mars' surfac Let’s introduce an ev aluation metric to measure the quality of the
output.
Evaluating the output with cosine
similarity In this section, wewill implement cosine similarity to measure the
similarity betw een user input and the generativ e AI model’s output .
We will also measure the augmented user input with the generativ e AI model’s output. Let’s ﬁrst deﬁneacosine similarity function:
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
def calculate_cosine_similarity(text1, text2):
vectorizer = TfidfVectorizer()
tfidf = vectorizer.fit_transform([text1, text2])
similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])
return similarity[0][0]
Then, let’s calculateascore that measures the similarity betw een the
user prompt and GPT-4’s response:
similarity_score = calculate_cosine_similarity(user_prompt, gpt
print(f"Cosine Similarity Score: {similarity_score:.3f}")
The score is low, although the output seemed acceptable forahuman:
Cosine Similarity Score: 0.396
It seems that eitherwe missed something or need to use another
metric.
Let’s try to calculate the similarity betw een the augmented input and GPT-4’s response:
# Example usage with your existing functions
similarity_score = calculate_cosine_similarity(augmented_input,
print(f"Cosine Similarity Score: {similarity_score:.3f}")
The score seems beer:
Cosine Similarity Score: 0.857
Canwe use another method? Cosine similarity, when using Term Frequency-Inv erse Document Frequency (TF-IDF ), relies heavily on
exact vocabulary ov erlap and takes into account important language
features, such as semantic meanings, synonyms, or contextual usage.
As such, this method may produce low er similarity scores for texts
that are conceptually similar but diﬀer in word choice.
In contrast, using Sentence Transformers to calculate similarity
involv es embeddings that capture deeper semantic relationships
betw een words and phrases. This approach is more eﬀectivein
recognizing the contextual and conceptual similarity betw een texts.
Let’s try this approach.
First, let’s install sentence-transformers:
!pip install sentence-transformers Be careful installing this library at the end of the session, since it may
induce potential conﬂicts with the RAG pipeline’s requirements.
Depending onaproject’s needs, this code could beyet another
separate pipeline component.
As of August 2024, using a Hugging Face token is
optional. If Hugging Face requiresatoken, sign up to Hugging Face to obtain an API token, check the
conditions, and set up the key as instructed.
We will now use a MiniLM architecture to perform the task with all-
MiniLM-L6-v2. This model is av ailable through the Hugging Face Model Hubwe are using. It’s part of the sentence-transformers
library, which is an extension of the Hugging Face Transformers
library. We are using this architecture because it oﬀersacompact and
eﬃcient model, withastrong performance in generating meaningful
sentence embeddings quickly. Let’s now implement it with the
following function:
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
def calculate_cosine_similarity_with_embeddings(text1, text2):
embeddings1 = model.encode(text1)
embeddings2 = model.encode(text2)
similarity = cosine_similarity([embeddings1], [embeddings2]
return similarity[0][0]
We can now call the function to calculate the similarity betw een the
augmented user input and GPT-4’s response:
similarity_score = calculate_cosine_similarity_with_embeddings(
print(f"Cosine Similarity Score: {similarity_score:.3f}")
The output shows that the Sentence Transformer captures semantic
similarities betw een the texts more eﬀectiv ely, resulting inahigh
cosine similarity score:
Cosine Similarity Score: 0.739
The choice of metrics depends on the speciﬁc requirements of each
project phase. Chapter 3 , Building Index-Based RAG with LlamaIndex,
Deep Lake, and OpenAI , will provide adv anced metrics whenwe
implement index-based RAG. At this stage, how ever, the RAG
pipeline’s three components havebeen successfully built. Let’s
summarize our journey and moveto the next lev el!
Summary In this chapter, wetackled the complexities of using RAG-driv en
generativ e AI, focusing on the essential role of document embeddings
when handling large datasets. We saw how to go from raw texts to
embeddings and store them invector stores. Vector stores such as Activ eloop, unlike parametric generativ e AI models, provide API
tools and visual interfaces that allow us to see embedded text at any
moment.
A RAG pipeline detailed the organizational process of integrating OpenAI embeddings into Activ eloop Deep Lakevector stores. The RAG pipelinewas broken down into distinct components that can
vary from one project to another. This separation allows multiple
teams to work simultaneously without dependency, accelerating
development and facilitating specialized focus on individual aspects,
such as data collection, embedding processing, and query generation
for the augmented generation AI process.
We then builtathree-component RAG pipeline, beginning by
highlighting the necessity of speciﬁc cross-platform packages and
careful system architecture planning. The resources involv edwere Python functions built from scratch, Activ eloop Deep Lake to
organize and store the embeddings inadataset inav ector store, an OpenAI embedding model, and OpenAI’s GPT-4o generativ e AI
model. The program guided us through buildingathree-part RAG
pipeline using Python, with practical steps that involv ed seing up
the environment, handling dependencies, and addressing
implementation challenges like data chunking andvector store
integration.
This journey providedarobust understanding of embedding
documents invector stores and lev eraging them for enhanced
generativ e AI outputs, preparing us to apply these insights to real-
world AI applications inwell-organized processes and teams within
an organization. Vector stores enhance the retriev al of documents that
require precision in information retriev al. Indexing takes RAG
further and increases the speed and relev ance of retriev als. The next
chapter will take usastep further by introducing adv anced indexing
methods to retrieveand augment inputs.
Questions Answ er the following questions with Yes or No:
1. Do embeddings conv ert text into high-dimensionalvectors for
faster retriev al in RAG?
2. Are keyword searches more eﬀectivethan embeddings in
retrieving detailed semantic content?
3. Is it recommended to separate RAG pipelines into independent
components?
4. Does the RAG pipeline consist of only two main components?
5. Can Activ eloop Deep Lake handle both embedding andvector
storage?
6. Is the text-embedding-3-small model from OpenAI used to
generate embeddings in this chapter?
7. Are data embeddings visible and directly traceable in an RAG-
driven system?
8. Can a RAG pipeline run smoothly without spliing into separate
components?
9. Is chunking large texts into smaller parts necessary for
embedding and storage?
10. Are cosine similarity metrics used to ev aluate the relev ance of
retriev ed information?
References OpenAI Ada documentation for embeddings:
https://platform.openai.com/docs/guides/embeddings/e
mbedding-models OpenAI GPT documentation for content generation:
https://platform.openai.com/docs/models/gpt-4-turbo-
and-gpt-4
Activ eloop API documentation:
https://docs.deeplake.ai/en/latest/
MiniLM model reference:
https://huggingface.co/sentence-transformers/all-
MiniLM-L6-v2
Further reading OpenAI’s documentation on embeddings:
https://platform.openai.com/docs/guides/embeddings Activ eloop documentation: https://docs.activeloop.ai/
Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag

3
Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI
Indexes increase precision and speed performances, but they oﬀer
more than that. Indexes transform retriev al-augmented generativ e AI
by addingalay er of transparency. With an index, the source ofaresponse generated by a RAG model is fully traceable, oﬀering
visibility into the precise location and detailed content of the data
used. This improv ement not only mitigates issues like bias and
hallucinations but also addresses concerns around copyright and
data integrity.
In this chapter, w e’ll explore how indexed data allows for greater
control ov er generativ e AI applications. If the output is
unsatisfactory, it’s no longeramystery why, since the index allows
us to identify and examine the exact data source of the issue. This
capability makes it possible to reﬁne data inputs, tw eak system
conﬁgurations, or switch components, such asvector store softw are
and generativemodels, to achievebeer outcomes.
We will begin the chapter by laying out the architecture of an index-
based RAG pipeline that will enhance speed, precision, and
traceability. We will show how LlamaIndex, Deep Lake, and OpenAI
can be seamlessly integrated without having to create all the
necessary functions ourselv es. This providesasolid base to start
building from. Then, w e’ll introduce the main indexing typeswe’ll
use in our programs, such asvector, tree, list, and keyword indexes.
Then, wewill buildadomain-speciﬁc drone technology LLM RAG
agent thatauser can interact with. Drone technology is expanding to
all domains, such as ﬁre detection, traﬃc information, and sports
events; hence, I’vedecided to use it in our example. The goal of this
chapter is to prepare an LLM drone technology dataset thatwe will
enhance with multimodal data in the next chapter. We will also
illustrate the key indexing types in code.
By the end of this chapter, you’ ll be adept at manipulating index-
based RAG throughvector stores, datasets, and LLMs, and know
how to optimize retriev al systems and ensure full traceability. You
will discov er how our integrated toolkit—combining LlamaIndex,
Deep Lake, and OpenAI—not only simpliﬁes technical complexities
but also frees your time to dev elop and hone your analytical skills,
enabling you to divedeeper into understanding RAG-driv en
generativ e AI.
We’ll cov er the following topics in this chapter:
Buildingasemantic search engine with a LlamaIndex framework
and indexing methods Populating Deep Lakevector stores Integration of LlamaIndex, Deep Lake, and OpenAI
Score ranking and cosine similarity metrics Metadata enhancement for traceability Query setup and generation conﬁguration Introducing automated document ranking Vector, tree, list, and keyword indexing types Why use index-based RAG?
Index-based search takes adv anced RAG-driv en generativ e AI to
another lev el. It increases the speed of retriev al when faced with large
volumes of data, taking us from raw chunks of data to organized,
indexed nodes thatwe can trace from the output back to the source ofadocument and its location.
Let’s understand the diﬀerences betw eenav ector-based similarity
search and an index-based search by analyzing the architecture of an
index-based RAG.
Architecture Index-based search is faster than vector-based search in RAG because
it directly accesses relev ant data using indices, whilevector-based
search sequentially compares embeddings across all records. We
implementedav ector-based similarity search program in Chapter 2 ,
RAG Embedding V ector Stores with Deep Lake and OpenAI , as shown in Figure 3.1 :
We collected and prepared data in Pipeline #1: Data Collection and Preparation We embedded the data and stored the prepared data inav ector
store in Pipeline #2: Embeddings andvector store We then ran retriev al queries and generativ e AI with Pipeline #3
to process user input, run retriev als based onvector similarity
searches, augment the input, generatearesponse, and apply
performance metrics.
This approach is ﬂexible because it giv es you manyways to
implement each component, depending on the needs of your project.
Figure 3.1: RAG-driven generative AI pipelines, as described in Chapter 2, with additional
functionality How ever, implementing index-based searches will take us into the
future of AI, which will be faster, more precise, and traceable. We will
follow the same process as in Chapter 2 , with three pipelines, to make
sure that you are ready to work inateam in which the tasks are
specialized. Sincewe are using the same pipelines as in Chapter 2 ,
let’s add the functions from that chapter to them, as shown in Figure
3.1:
Pipeline Component #1 and D2-Index : We will collect data and
preprocess it. How ever, this time, wewill prepare the data
source one document atatime and store them in separate ﬁles.
We will then add their name and location to the metadatawe
load into thevector store. The metadata will help us tracearesponse all theway back to the exact ﬁle that the retriev al
function processed. We will havea direct link fromaresponse to
the data that itwas based on.
Pipeline Component #2 and D3-Index : We will load the data
intoav ector store by installing and using the innov ative
integrated llama-index-vector-stores-deeplake package, which
includes ev erythingwe need in an optimized starter scenario:
chunking, embedding, storage, and ev en LLM integration. We
have everythingwe need to get to work on index-based RAG inafew lines of code! Thisway, oncewe havea solid program, wecan customize and expand the pipelines aswe wish, aswe did,
for example, in Chapter 2 , whenwe explicitly chose the LLM
models and chunking sizes.
Pipeline Component #3 and D4-Index : We will load the data inadataset by installing and using the innov ative integrated llama-
index-vector-stores-deeplake package, which includes
everythingwe need to get indexed-based retriev al and
generation started, including automated ranking and scoring.
The process is seamless and extremely productiv e. We’ll lev erage LlamaIndex with Deep Lake to streamline information retriev al
and processing. An integrated retriev er will eﬃciently fetch
relev ant data from the Deep Lake repository, while an LLM
agent will then intelligently synthesize and interact with the
retriev ed information to generate meaningful insights or actions.
Indexes are designed for fast retriev al, andwe will implement
several indexing methods.
Pipeline Component #3 and E1-Index : We will addatime and
score metric to ev aluate the output.
In the previous chapter, weimplementedvector-based similarity
search and retriev al. We embedded documents to transform data into
high-dimensionalvectors. Then, weperformed retriev al by
calculating distances betw eenvectors. In this chapter, wewill go
further and createav ector store. How ever, wewill load the data intoadataset that will be reorganized using retriev al indexing types. Table
3.1 shows the diﬀerences betw eenvector-based and index-based
search and retriev al methods:
Feature Vector-based
similarity search
and retriev alIndex-basedvector, tree,
list, and keyword search
and retriev al Flexibility High Medium (precomputed
structure)
Speed Slow er with large
datasetsFast and optimized for
quick retriev al Scalability Limited by real-
time processingHighly scalable with large
datasets Complexity Simpler setup More complex and requires
an indexing step Update FrequencyEasy to update Requires re-indexing for
updates Table 3.1: Vector-based and index-based characteristics We will now buildasemantic index-based RAG program with Deep Lake, LlamaIndex, and OpenAI.
Buildingasemantic search engine
and generative agent for drone
technology In this section, wewill buildasemantic index-based search engine
and generativ e AI agent engine using Deep Lakevector stores,
LlamaIndex, and OpenAI. As mentioned earlier, drone technology is
expanding in domains such as ﬁre detection and traﬃc control. As
such, the program’s goal is to provide an index-based RAG agent for
drone technology questions and answ ers. The program will
demonstrate how drones use computer vision techniques to identify
vehicles and other objects. We will implement the architecture
illustrated in Figure 3.1 , described in the Architecture section of this
chapter.
Open 2-Deep_Lake_LlamaIndex_OpenAI_indexing.ipynb
from the GitHub repository of this chapter. The titles of
this section are the same as the section titles in the
notebook, so you can match the explanations with the
code.
We will ﬁrst begin by installing the environment. Then, wewill build
the three main pipelines of the program:
Pipeline 1 : Collecting and preparing the documents. Using
sources like GitHub and W ikipedia, collect and clean documents
for indexing.
Pipeline 2 : Creating and populating a Deep Lakevector store.
Create and populate a Deep Lakevector store with the prepared
documents.
Pipeline 3 : Index-based RAG for query processing and
generation. Applying time and score performances with LLMs
and cosine similarity metrics.
When possible, break your project down into separate pipelines so
that teams can progress independently and in parallel. The pipelines
in this chapter are an example of how this can be done, but there are
many otherways to do this, depending on your project. For now, wewill begin by installing the environment.
Installing the environment The environment is mostly the same as in the previous chapter. Let’s
focus on the packages that integrate LlamaIndex, v ector store
capabilities for Deep Lake, and also OpenAI modules. This
integration isamajor step forw ard to seamless cross-platform
implementations:
!pip install llama-index-vector-stores-deeplake==0.1.6
The program requires additional Deep Lake functionalities:
!pip install deeplake==3.9.8
The program also requires LlamaIndex functionalities:
!pip install llama-index==0.10.64
Let’s now check if the packages can be properly imported from
llama-index, includingvector stores for Deep Lake:
from llama_index.core import VectorStoreIndex, SimpleDirectoryR
from llama_index.vector_stores.deeplake import DeepLakeVectorSt With that, wehaveinstalled the environment. We will now collect
and prepare the documents.
Pipeline 1: Collecting and preparing
the documents In this section, wewill collect and prepare the drone-related
documents with the metadata necessary to trace the documents back
to their source. The goal is to tracearesponse’s content back to the
exact chunk of data retriev ed to ﬁnd its source. First, wewill createadata directory in whichwe will load the documents:
!mkdir data Now, wewill useaheterogeneous corpus for the drone technology
data thatwe will process using BeautifulSoup:
import requests
from bs4 import BeautifulSoup
import re
import os
urls = [
"https://github.com/VisDrone/VisDrone-Dataset",
"https://paperswithcode.com/dataset/visdrone",
"https://openaccess.thecvf.com/content_ECCVW_2018/papers/11
"https://github.com/VisDrone/VisDrone2018-MOT-toolkit",
"https://en.wikipedia.org/wiki/Object_detection",
"https://en.wikipedia.org/wiki/Computer_vision",…
]
The corpus containsalist of sites related to drones, computer vision,
and related technologies. How ever, the list also contains noisy links
such as https://keras.io/ and https://pytorch.org/ , which do
not contain the speciﬁc informationwe are looking for.
In real-life projects, wewill not alw ays havethe luxury
of working on perfect, pertinent, structured, andwell-
formaed data. Our RAG pipelines must be suﬃciently
robust to retrieverelev ant data inanoisy environment.
In this case, weare working with unstructured data invarious
formats andvariable quality as related to drone technology. Of
course, inaclosed environment, wecan work with the persons or
organizations that produce the documents, butwe must be ready for
any type of document inafast-moving, digital world.
The code will fetch and clean the data, as it did in Chapter 2 :
def clean_text(content):
# Remove references and unwanted characters
content = re.sub(r'\[\d+\]', '', content) # Remove refere
content = re.sub(r'[^\w\s\.]', '', content) # Remove punct
return content
def fetch_and_clean(url):
try:
response = requests.get(url)
response.raise_for_status() # Raise exception for bad
soup = BeautifulSoup(response.content, 'html.parser')
# Prioritize "mw-parser-output" but fall back to "conte
content = soup.find('div', {'class': 'mw-parser-output'
if content is None:
return None
# Remove specific sections, including nested ones
for section_title in ['References', 'Bibliography', 'Ex
section = content.find('span', id=section_title)
while section:
for sib in section.parent.find_next_siblings():
sib.decompose()
section.parent.decompose()
section = content.find('span', id=section_title
# Extract and clean text
text = content.get_text(separator=' ', strip=True)
text = clean_text(text)
return text
except requests.exceptions.RequestException as e:
print(f"Error fetching content from {url}: {e}")
return None # Return None on error Each project will require speciﬁc names and paths for the original
data. In this case, wewill introduce an additional function to saveeach piece of text with the name of its data source, by creatingakeyword based on its URL:
# Directory to store the output files
output_dir = './data/'
os.makedirs(output_dir, exist_ok=True)
# Processing each URL and writing its content toaseparate fil
for url in urls:
article_name = url.split('/')[-1].replace('.html',") # Han
filename = os.path.join(output_dir, article_name + '.txt')
clean_article_text = fetch_and_clean(url)
with open(filename, 'w', encoding='utf-8') as file:
file.write(clean_article_text)
print(f"Content(ones that were possible) written to files in th The output shows that the goal is achiev ed, although some
documents could not be decoded:
WARNING:bs4.dammit:Some characters could not be decoded, and wer Content(ones that were possible) written to files in the './data Depending on the project’s goals, you can choose to inv estigate and
ensure that all documents are retriev ed, or estimate that you haveenough data for user queries.
If we check ./data/, we will ﬁnd that each article is now inaseparate
ﬁle, as shown in the content of the directory:
Figure 3.2: List of prepared documents The program now loads the documents from ./data/:
# load documents
documents = SimpleDirectoryReader("./data/").load_data()
The LlamaIndex SimpleDirectoryReader class is designed for working
with unstructured data. It recursiv ely scans the directory and
identiﬁes and loads all supported ﬁle types, such as .txt, .pdf, and
.docx. It then extracts the content from each ﬁle and returnsalist of
document objects with its text and metadata, such as the ﬁlename
and ﬁle path. Let’s display the ﬁrst entry of this list of dictionaries of
the documents:
documents[0]
The output shows that the directory reader has provided fully
transparent information on the source of its data, including the name
of the document, such as 1804.06985.txt in this case:
'/content/data/1804.06985.txt', 'file_name': '1804.06985.txt',
The content of this document contains noise that seems unrelated to
the drone technology informationwe are looking for. But that is
exactly the point of this program, which aims to do the following:
Start with all the raw, unstructured, loosely drone-related data
we can get our hands on Simulate how real-life projects often begin Evaluate howwell an index-based RAG generativ e AI program
can perform inachallenging environment Let’s now create and populate a Deep Lakevector store in complete
transparency.
Pipeline 2: Creating and populating a Deep Lake vector store In this section, wewill create a Deep Lakevector store and populate
it with the data in our documents. We will implementastandard
tensor conﬁguration with:
text (str): The text is the content of one of the text ﬁles listed in
the dictionary of documents. It will be seamless, and chunking
will be optimized, breaking the text into meaningful chunks.
metadata(json): In this case, the metadata will contain the
ﬁlename source of each chunk of text for full transparency and
control. We will see how to access this information in code.
embedding (float32): The embedding is seamless, using an OpenAI embedding model called directly by the LlamaIndex-Deep Lake-OpenAI package.
id (str, auto-populated): A unique ID is aributed
automatically to each chunk. Thevector store will also contain an
index, which isanumber from 0 to n, but it cannot be used
semantically, since it will change each timewe modify the
dataset. How ever, the unique ID ﬁeld will remain unchanged
untilwe decide to optimize it with index-based search strategies,
aswe will see in the Pipeline 3: Index-based RAG section that
follows.
The program ﬁrst deﬁnes ourvector store and dataset paths:
from llama_index.core import StorageContext
vector_store_path = "hub://denis76/drone_v2"
dataset_path = "hub://denis76/drone_v2"
Replace thevector store and dataset paths with your account name
and the name of the dataset you wish to use:
vector_store_path = "hub://[YOUR VECTOR STORE/
We then createav ector store, populate it, and create an index ov er
the documents:
# overwrite=True will overwrite dataset, False will append it
vector_store = DeepLakeVectorStore(dataset_path=dataset_path, o
storage_context = StorageContext.from_defaults(vector_store=vec
# Create an index over the documents
index = VectorStoreIndex.from_documents(documents, storage_cont
)
Notice that overwrite is set to True to create thevector store and
overwrite any existing one. If overwrite=False, the dataset will be
appended.
The index created will be reorganized by the indexing methods,
which will rearrange and create new indexes when necessary.
How ever, the responses will alw ays provide the original source of the
data. The output conﬁrms that the dataset has been created and the
data is uploaded:
Your Deep Lake dataset has been successfully created!
Uploading data to deeplake dataset.
100%|██████████| 41/41 [00:02<00:00, 18.15it/s]
The output also shows the structure of the dataset once it is
populated:
Dataset(path='hub://denis76/drone_v2', tensors=['text', 'metadat The data is stored in tensors with their type and shape:
Figure 3.3: Dataset structure We will now load our dataset in memory:
import deeplake
ds = deeplake.load(dataset_path) # Load the dataset We can visualize the dataset online by clicking on the link provided
in the output:
/
This dataset can be visualized in Jupyter Notebook by ds.visuali
hub://denis76/drone_v2 loaded successfully.
This dataset can be visualized in Jupyter Notebook by ds.visuali
hub://denis76/drone_v2 loaded successfully.
We can also decide to add code to display the dataset. We begin by
loading the data inapandas DataFrame:
import json
import pandas as pd
import numpy as np
# Assuming 'ds' is your loaded Deep Lake dataset
# Createadictionary to hold the data
data = {}
# Iterate through the tensors in the dataset
for tensor_name in ds.tensors:
tensor_data = ds[tensor_name].numpy()
# Check if the tensor is multi-dimensional
if tensor_data.ndim > 1:
# Flatten multi-dimensional tensors
data[tensor_name] = [np.array(e).flatten().tolist() for
else:
# Convert 1D tensors directly to lists and decode text
if tensor_name == "text":
data[tensor_name] = [t.tobytes().decode('utf-8') if
else:
data[tensor_name] = tensor_data.tolist()
# Create a Pandas DataFrame from the dictionary
df = pd.DataFrame(data)
Then, wecreateafunction to displayarecord:
# Function to displayaselected record
def display_record(record_number):
record = df.iloc[record_number]
display_data = {
"ID": record["id"] if "id" in record else "N/A",
"Metadata": record["metadata"] if "metadata" in record
"Text": record["text"] if "text" in record else "N/A",
"Embedding": record["embedding"] if "embedding" in reco
}
Finally, wecan selectarecord and display each ﬁeld:
# Function call to displayarecord
rec = 0 # Replace with the desired record number
display_record(rec)
The id isaunique string code:
ID:
['a89cdb8c-3a85-42ff-9d5f-98f93f414df6']
The metadata ﬁeld contains the informationwe need to trace the
content back to the original ﬁle and ﬁle path, aswell as ev erythingwe
need to understand this record, from the source to the embedded
vector. It also contains the information of the node created from the
record’s data, which can then be used for the indexing enginewe will
run in Pipeline 3 :
file_path: Path to the ﬁle in the dataset
(/content/data/1804.06985.txt).
file_name: Name of the ﬁle ( `1804.06985.txt`).
file_type: Type of ﬁle ( `text/plain`).
file_size: Size of the ﬁle in bytes ( `3700`).
creation_date: Date the ﬁlewas created ( `2024-08-09`).
last_modified_date: Date the ﬁlewas last modiﬁed ( `2024-08-
09`).
_node_content: Detailed content of the node, including the
following main items:
id_: Unique identiﬁer for the node ( `a89cdb8c-3a85-42ff-
9d5f-98f93f414df6 `).
embedding: Embedding related to the text ( null).
metadata: Repeated metadata about the ﬁle.
excluded_embed_metadata_keys: Keys excluded from
embedding metadata (not necessary for embedding).
excluded_llm_metadata_keys: Keys excluded from LLM
metadata (not necessary for an LLM).
relationships: Information about relationships to other
nodes.
text: Actual text content of the document. It can be the text
itself, an abstract, a summary, or any other approach to
optimize search functions.
start_char_idx: Starting character index of the text.
end_char_idx: Ending character index of the text.
text_template: Template for displaying text with metadata.
metadata_template: Template for displaying metadata.
metadata_seperator: Separator used in metadata display.
class_name: Type of node (e.g., `TextNode`).
_node_type: Type of node ( `TextNode`).
document_id: Identiﬁer for the document ( `61e7201d-0359-42b4-
9a5f-32c4d67f345e`).
doc_id: Document ID, same as document_id.
ref_doc_id: Reference document ID, same as document_id.
The text ﬁeld contains the ﬁeld of this chunk of data, not the whole
original text:
['High Energy Physics Theory arXiv1804.06985 hepth Submitted on The Embedding ﬁeld contains the embeddedvector of the text content:
[-0.0009671939187683165, 0.010151553899049759, -0.01097981911152
The structure and format of RAG datasetsvary from one domain or
project to another. How ever, the following four columns of this
dataset providevaluable information on the evolution of AI:
id: The id is the indexwe will be using to organize the chunks
of text of the text column in the dataset. The chunks will be
transformed into nodes that can contain the original text,
summaries of the original text, and additional information, such
as the source of the data used for the output that is stored in the
metadata column. We created this index in Pipeline 2 of this
notebook whenwe created thevector store. How ever, wecan
generate indexes in memory on an existing database that
contains no indexes, aswe will see in Chapter 4 , Multimodal Modular RAG for Drone T echnology .
metadata: The metadatawas generated automatically in Pipeline
1 when Deep Lake’s SimpleDirectoryReader loaded the source
documents inadocuments object, and also when thevector store
was created. In Chapter 2 , RAG Embedding V ector Stores with Deep Lake and OpenAI , we only had one ﬁle source of data. In this
chapter, westored the data in one ﬁle for each data source (URL ).
text: The text processed by Deep Lake’svector store creation
functionality thatwe ran in Pipeline 2 automatically chunked
the data, without us having to conﬁgure the size of the chunks,
aswe did in the Retrievingabatch of prepared documents section in Chapter 2 . Once again, the process is seamless. We will see how
smart chunking is done in the Optimized chunking section of Pipeline 3: Index-based RAG in this chapter.
embedding: The embedding for each chunk of datawas generated
through an embedding model thatwe do not haveto conﬁgure.
We could choose an embedding model, aswe did in the Data
embedding and storage section in Chapter 2 , RAG Embedding V ector Stores with Deep Lake and OpenAI . We selected an embedding
model and wroteafunction. In this program, Deep Lake selects
the embedding model and embeds the data, without us having
to writeasingle line of code.
We can see that embedding, chunking, indexing, and other data
processing functions are now encapsulated in platforms and
frameworks, such as Activ eloop Deep Lake, LlamaIndex, OpenAI,
LangChain, Hugging Face, Chroma, and many others. Progressiv ely,
the initial excitement of generativ e AI models and RAG will fade, and
they will become industrialized, encapsulated, and commonplace
components of AI pipelines. AI is evolving, and it might be helpful to
facilitateaplatform that oﬀersadefault conﬁguration based on
eﬀectivepractices. Then, oncewe haveimplementedabasic
conﬁguration, wecan customize and expand the pipelines as
necessary for our projects.
We are now ready to run index-based RAG.
Pipeline 3: Index-based RAG
In this section, wewill implement an index-based RAG pipeline
using LlamaIndex, which uses the datawe haveprepared and
processed with Deep Lake. We will retrieverelev ant information
from the heterogeneous (noise-containing) drone-related document
collection and synthesize the response through OpenAI’s LLM
models. We will implement four index engines:
Vector Store Index Engine : Createsavector store index from the
documents, enabling eﬃcient similarity-based searches.
Tree Index : Buildsahierarchical tree index from the documents,
oﬀering an alternativeretriev al structure.
List Index : Constructsastraightforw ard list index from the
documents.
Keyword T able Index : Creates an index based on keywords
extracted from the documents.
We will implement querying with an LLM:
Query Response and Source : Queries the index with user input,
retriev es the relev ant documents, and returnsasynthesized
response along with source information.
We will measure the responses withatime-w eighted av erage metric with LLM score and cosine similarity that calculatesatime-w eighted
average, based on retriev al and similarity scores. The content and
execution times mightvary from one run to another due to the
stochastic algorithms implemented.
User input and query parameters The user input will be the reference question for the four index
engineswe will run. We will ev aluate each response based on the
index engine’s retriev als and measure the outputs, using time and
score ratios. The input will be submied to the four index and query
engineswe will build later.
The user input is:
user_input="How do drones identify vehicles?"
The four query engines that implement an LLM (in this case, an OpenAI model) will seamlessly be called with the same parameters.
The three parameters thatwe will set are:
#similarity_top_k
k=3
#temperature
temp=0.1
#num_output
mt=1024
These key parameters are:
k=3: The query engine will be required to ﬁnd the top 3 most
probable responses by seing the top-k (most probable choices)
to 3. In this case, k will serveasaranking function that will force
the LLM to select the top documents.
temp=0.1: A low temperature such as 0.1 will encourage the LLM to produce precise results. If the temperature is increased to
0.9, for example, the response will be more creativ e. How ever,
in this case, weare exploring drone technology, which requires
precision.
mt=1024: This parameter will limit the number of tokens of the
output to 1,024.
The user input and parameters will be applied to the four query
engines. Let’s now build the cosine similarity metric.
Cosine similarity metric The cosine similarity metricwas described in the Evaluating the Output with the Cosine Similarity section in Chapter 2 . If necessary, take
the time to go through that section again. Here, wewill createafunction for the responses:
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
def calculate_cosine_similarity_with_embeddings(text1, text2):
embeddings1 = model.encode(text1)
embeddings2 = model.encode(text2)
similarity = cosine_similarity([embeddings1], [embeddings2]
return similarity[0][0]
The function uses sklearn and also Hugging Face’s SentenceTransformer. The program ﬁrst creates thevector store
engine.
Vector store index query engine VectorStoreIndex isatype of index within LlamaIndex that
implementsvector embeddings to represent and retrieveinformation
from documents. These documents with similar meanings will haveembeddings that are closer together in thevector space, aswe
explored in the previous chapter. How ever, this time, the VectorStoreIndex does not automatically use the existing Deep Lake
vector store. It can createanew in-memoryvector index, re-embed
the documents, and createanew index structure. We will take this
approach further in Chapter 4 , Multimodal Modular RAG for Drone Technology , whenwe implementadataset that contains no indexes or
embeddings.
There is no silv er bullet to deciding which indexing
method is suitable for your project! The bestway to
makeachoice is to test thevector, tree, list, and
keyword indexes introduced in this chapter.
We will ﬁrst create thevector store index:
from llama_index.core import VectorStoreIndex
vector_store_index = VectorStoreIndex.from_documents(documents)
We then display thevector store indexwe created:
print(type(vector_store_index))
We will receivethe following output, which conﬁrms that the engine
was created:
<class 'llama_index.core.indices.vector_store.base.VectorStoreIn We now needaquery engine to retrieveand synthesize the
document(s) retriev ed with an LLM—in our case, an OpenAI model
(installed with !pip install llama-index-vector-stores-
deeplake==0.1.2):
vector_query_engine = vector_store_index.as_query_engine(simila We deﬁned the parameters of the query engine in the User input and
query parameters subsection. We can now query the dataset and
generatearesponse.
Query response and source Let’s deﬁneafunction that will manage the query and return
information on the content of the response:
import pandas as pd
import textwrap
def index_query(input_query):
response = vector_query_engine.query(input_query)
# Optional: Printaformatted view of the response (remove
print(textwrap.fill(str(response), 100))
node_data = []
for node_with_score in response.source_nodes:
node = node_with_score.node
node_info = {
'Node ID': node.id_,
'Score': node_with_score.score,
'Text': node.text
}
node_data.append(node_info)
df = pd.DataFrame(node_data)
# Instead of printing, return the DataFrame and the respons
return df, response,
index_query(input_query) executesaquery usingav ector query
engine and processes the results intoastructured format. The
function takes an input query and retriev es relev ant information,
using the query engine inapandas DataFrame: Node ID, Score, File Path, Filename, and Text.
The code will now call the query:
import time
#start the timer
start_time = time.time()
df, response = index_query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
print(df.to_markdown(index=False, numalign="left", stralign="le We will ev aluate the time it takes for the query to retrievethe relev ant
data and generatearesponse synthesis with the LLM (in this case, an OpenAI model). The output of the semantic search ﬁrst returnsaresponse synthesized by the LLM:
Drones can automatically identify vehicles across different came The output then displays the elapsed time of the query:
Query execution time: 0.8831 seconds The output now displays node information. The score of each node of
three k=3 documentswas retriev ed with their text excerpts:
Figure 3.4: Node information output The ID of the node guarantees full transparency and can be traced
back to the original document, ev en when the index engines re-index
the dataset. We can obtain the node source of the ﬁrst node, for
example, with the following code:
nodeid=response.source_nodes[0].node_id
nodeid The output provides the node ID:
4befdb13-305d-42db-a616-5d9932c17ac8
We can drill down and retrievethe full text of the node containing the
document thatwas synthesized by the LLM:
response.source_nodes[0].get_text()
The output will display the following text:
['These activities can be carried out with different approaches We can also peek into the nodes and retrievetheir chunk size.
Optimized chunking We can predeﬁne the chunk size, or we can let LlamaIndex select it
for us. In this case, the code determines the chunk size automatically:
for node_with_score in response.source_nodes:
node = node_with_score.node # Extract the Node object from
chunk_size = len(node.text)
print(f"Node ID: {node.id_}, Chunk Size: {chunk_size} chara The adv antage of an automated chunk size is that it can bevariable.
For example, in this case, the chunk size shown in the size of the
output nodes is probably in the 4000-to-5500-character range:
Node ID: 83a135c6-dddd-402e-9423-d282e6524160, Chunk Size: 4417
Node ID: 7b7b55fe-0354-45bc-98da-0a715ceaaab0, Chunk Size: 1806
Node ID: 18528a16-ce77-46a9-bbc6-5e8f05418d95, Chunk Size: 3258
The chunking function does not linearly cut content but optimizes the
chunks for semantic search.
Performance metric We will also implementaperformance metric based on the accuracy
of the queries and the time elapsed. This function calculates and
printsaperformance metric foraquery, along with its execution
time. The metric is based on theweighted av erage relev ance scores of
the retriev ed information, divided by the time it took to get the
results. Higher scores indicate beer performance.
We ﬁrst calculate the sum of the scores and the av erage score, and
thenwe divide theweighted av erage by the time elapsed to perform
the query:
import numpy as np
def info_metrics(response):
# Calculate the performance (handling None scores)
scores = [node.score for node in response.source_nodes if nod
if scores: # Check if there are any valid scores
weights = np.exp(scores) / np.sum(np.exp(scores))
perf = np.average(scores, weights=weights) / elapsed_time
else:
perf = 0 # Or some other default value if all scores are The result isaratio based on the av erageweight divided by the
elapsed time:
perf = np.average(scores, weights=weights) / elapsed_time We can then call the function:
info_metrics(response)
The output provides an estimation of the quality of the response:
Average score: 0.8374
Query execution time: 1.3266 seconds Performance metric: 0.6312
This performance metric is not an absolutevalue. It’s an indicator
thatwe can use to compare this output with the other index engines.
It may alsovary from one run to another, due to the stochastic nature
of machine learning algorithms. Additionally, the quality of the
output depends on the user ’s subjectiveperception. In any case, this
metric will help compare the query engines’ performances in this
chapter.
We can already see that the av erage score is satisfactory, ev en though
we loaded heterogeneous and sometimes unrelated documents in the
dataset. The integrated retriev er and synthesizer functionality of LlamaIndex, Deep Lake, and OpenAI haveprov en to be highly
eﬀectiv e.
Tree index query engine The tree index in LlamaIndex createsahierarchical structure for
managing and querying text documents eﬃciently. How ever, think
of something other thanaclassical hierarchical structure! The tree
index engine optimizes the hierarchy, content, and order of the
nodes, as shown in Figure 3.5 :
Figure 3.5: Optimized tree index The tree index organizes documents inatree structure, with broader
summaries at higher lev els and detailed information at low er lev els.
Each node in the tree summarizes the text it cov ers. The tree index is
eﬃcient for large datasets and queries large collections of documents
rapidly by breaking them down into manageable optimized chunks.
Thus, the optimization of the tree structure allows for rapid retriev al
by trav ersing the relev ant nodes withoutwasting time.
Organizing this part of the pipeline and adjusting parameters such as
tree depth and summary methods can beaspecialized task forateam
member. Depending on the project and workload, working on the
tree structure could be part of Pipeline 2 when creating and
populatingav ector store. Alternativ ely, the tree structure can be
created in memory at the beginning of each session. The ﬂexibility of
the structure and implementation of tree structures and index
engines, in general, can beafascinating andvaluable specialization in
a RAG-driv en generativ e AI team.
In this index model, the LLM (an OpenAI model in this case) acts like
it is answ eringamultiple-choice question when selecting the best
nodes duringaquery. It analyzes the query, compares it with the
summaries of the current node’s children, and decides which path to
follow to ﬁnd the most relev ant information.
The integrated LlamaIndex-Deep Lake-OpenAI process in this
chapter is industrializing components seamlessly, taking AI to
another lev el. LLM models can now be used for embedding,
document ranking, and conv ersational agents. The market oﬀers
various language models from providers like OpenAI, Cohere, AI21
Labs, and Hugging Face. LLMs haveevolv ed from the early days of
being perceiv ed as magic to becoming industrialized, seamless,
multifunctional, and integrated components of broader AI pipelines.
Let’s createatree index in two lines of code:
from llama_index.core import TreeIndex
tree_index = TreeIndex.from_documents(documents)
The code then checks the classwe just created:
print(type(tree_index))
The output conﬁrms thatwe are in the TreeIndex class:
<class 'llama_index.core.indices.tree.base.TreeIndex'>
We can now make our tree index the query engine:
tree_query_engine = tree_index.as_query_engine(similarity_top_k The parameters of the LLM are those deﬁned in the User input and
query parameters section. The code now calls the query, measures the
time elapsed, and processes the response:
import time
import textwrap
# Start the timer
start_time = time.time()
response = tree_query_engine.query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
print(textwrap.fill(str(response), 100))
The query time and the response are both satisfactory:
Query execution time: 4.3360 seconds Drones identify vehicles using computer vision technology relate
technology involves detecting instances of semantic objects ofadigital images and videos. Drones can be equipped with object de
models trained on datasets like COCO, to detect vehicles in real
captured by the drone's cameras.
Let’s applyaperformance metric to the output.
Performance metric This performance metric will calculate the cosine similarity deﬁned in
the Cosine similarity metric section betw een the user input and the
response of our RAG pipeline:
similarity_score = calculate_cosine_similarity_with_embeddings(
print(f"Cosine Similarity Score: {similarity_score:.3f}")
print(f"Query execution time: {elapsed_time:.4f} seconds")
performance=similarity_score/elapsed_time
print(f"Performance metric: {performance:.4f}")
The output shows that although the quality of the responsewas
satisfactory, the execution timewas slow, which brings the
performance metric down:
Cosine Similarity Score: 0.731
Query execution time: 4.3360 seconds Performance metric: 0.1686
Of course, the execution time depends on the serv er (pow er) and the
data (noise). As established earlier, the execution times mightvary
from one run to another, due to the stochastic algorithms used. Also,
when the dataset increases in volume, the execution times of all the
indexing types may change.
The list index query engine may or may not be beer in this case.
Let’s run it to ﬁnd out.
List index query engine Don’t think of ListIndex as simplyalist of nodes. The query engine
will process the user input and each document asaprompt for an LLM. The LLM will ev aluate the semantic similarity relationship
betw een the documents and the query, thus implicitly ranking and
selecting the most relev ant nodes. LlamaIndex will ﬁlter the
documents based on the rankings obtained, and it can also take the
task further by synthesizing information from multiple nodes and
documents.
We can see that the selection process with an LLM is not rule-based.
Nothing is predeﬁned, which means that the selection is prompt-
based by combining the user input withacollection of documents.
The LLM ev aluates each document in the list independently , assigningascore based on its perceiv ed relev ance to the query. This score isn’t
relativeto other documents; it’sameasure of howwell the LLM
thinks the current document answ ers the question. Then, the top-k
documents are retained by the query engine ifwe wish, as in the
function used in this section.
Like the tree index, the list index can also be created in two lines of
code:
from llama_index.core import ListIndex
list_index = ListIndex.from_documents(documents)
The codeveriﬁes the class thatwe are using:
print(type(list_index))
The output conﬁrms thatwe are in the list class:
<class 'llama_index.core.indices.list.base.SummaryIndex'>
The list index is a SummaryIndex, which shows the large amount of
document summary optimization that is running under the hood! We
can now utilize our list index asaquery engine in the seamless
framework provided by LlamaIndex:
list_query_engine = list_index.as_query_engine(similarity_top_k The LLM parameters remain unchanged so thatwe can compare the
indexing types. We can now run our query, wrap the response up,
and display the output:
#start the timer
start_time = time.time()
response = list_query_engine.query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
print(textwrap.fill(str(response), 100))
The output showsalonger execution time but an acceptable
response:
Query execution time: 16.3123 seconds Drones can identify vehicles through computer vision systems tha
cameras mounted on the drones. These systems use techniques like
analyze the images and identify specific objects, such as vehicl
features. By processing the visual data in real-time, drones can
their surroundings.
The execution time is longer because the query goes throughalist,
not an optimized tree. How ever, wecannot draw conclusions from
this because each project or ev en each sub-task ofaproject has
diﬀerent requirements. Next, let’s apply the performance metric.
Performance metric We will use the cosine similarity, aswe did for the tree index, to
evaluate the similarity score:
similarity_score = calculate_cosine_similarity_with_embeddings(
print(f"Cosine Similarity Score: {similarity_score:.3f}")
print(f"Query execution time: {elapsed_time:.4f} seconds")
performance=similarity_score/elapsed_time
print(f"Performance metric: {performance:.4f}")
The performance metric is low er than the tree index due to the longer
execution time:
Cosine Similarity Score: 0.775
Query execution time: 16.3123 seconds Performance metric: 0.0475
Again, remember that this execution time mayvary from one run to
another, due to the stochastic algorithms implemented.
If we look back at the performance metric of each indexing type, wecan see that, for the moment, thevector store indexwas the fastest.
Once again, let’s not jump to conclusions. Each project might produce
surprising results, depending on the type and complexity of the data
processed. Next, let’s examine the keyword index.
Keyword index query engine KeywordTableIndex isatype of index in LlamaIndex, designed to
extract keywords from your documents and organize them inatable-
like structure. This structure makes it easier to query and retrieverelev ant information based on speciﬁc keywords or topics. Once
again, don’t think about this function asasimple list of extracted
keywords. The extracted keywords are organized intoatable-like
format where each keyword is associated with an ID that points to
the related nodes.
The program creates the keyword index in two lines of code:
from llama_index.core import KeywordTableIndex
keyword_index = KeywordTableIndex.from_documents(documents)
Let’s extract the data and createapandas DataFrame to see how the
index is structured:
# Extract data for DataFrame
data = []
for keyword, doc_ids in keyword_index.index_struct.table.items(
for doc_id in doc_ids:
data.append({"Keyword": keyword, "Document ID": doc_id}
# Create the DataFrame
df = pd.DataFrame(data)
df The output shows that each keyword is associated with an ID that
containsadocument orasummary, depending on theway LlamaIndex optimizes the index:
Figure 3.6: Keywords linked to document IDs in a DataFrame We now deﬁne the keyword index as the query engine:
keyword_query_engine = keyword_index.as_query_engine(similarity Let’s run the keyword query and see howwell and fast it can
producearesponse:
import time
# Start the timer
start_time = time.time()
# Execute the query (using .query() method)
response = keyword_query_engine.query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
print(textwrap.fill(str(response), 100))
The output is satisfactory, aswell as the execution time:
Query execution time: 2.4282 seconds Drones can identify vehicles through various means such as visua We can now measure the output withaperformance metric.
Performance metric The code runs the same metric as for the tree and list index:
similarity_score = calculate_cosine_similarity_with_embeddings(
print(f"Cosine Similarity Score: {similarity_score:.3f}")
print(f"Query execution time: {elapsed_time:.4f} seconds")
performance=similarity_score/elapsed_time
print(f"Performance metric: {performance:.4f}")
The performance metric is acceptable:
Cosine Similarity Score: 0.801
Query execution time: 2.4282 seconds Performance metric: 0.3299
Once again, wecan draw no conclusions. The results of all the
indexing types are relativ ely satisfactory. How ever, each project
comes with its dataset complexity and machine pow er av ailability.
Also, the execution times mayvary from one run to another, due to
the stochastic algorithms employ ed.
With that, wehavereview ed some of the main indexing types and
retriev al strategies. Let’s summarize the chapter and moveon to
multimodal modular retriev al and generation strategies.
Summary This chapter explored the transformativeimpact of index-based
search on RAG and introducedapivotal adv ancement: full
traceability . The documents become nodes that contain chunks of
data, with the source ofaquery leading us all theway back to the
original data. Indexes also increase the speed of retriev als, which is
critical as the volume of datasets increases. Another pivotal adv ance
is the integration of technologies such as LlamaIndex, Deep Lake, and OpenAI, which are emerging in another era of AI. The most adv anced AI models, such as OpenAI GPT-4o, Hugging Face, and Cohere, are
becoming seamless components in a RAG-driv en generativ e AI
pipeline, like GPUs inacomputer.
We started by detailing the architecture of an index-based RAG
generativ e AI pipeline, illustrating how these sophisticated
technologies can be seamlessly integrated to boost the creation of
advanced indexing and retriev al systems. The complexity of AI
implementation is changing thewaywe organize separate pipelines
and functionality forateam working in parallel on projects that scale
and involvelarge amounts of data. We saw how ev ery response
generated can be traced back to its source, providing clear visibility
into the origins and accuracy of the information used. We illustrated
the adv anced RAG technology implemented through drone
technology.
Throughout the chapter, weintroduced the essential tools to build
these systems, includingvector stores, datasets, chunking,
embedding, node creation, ranking, and indexing methods. We
implemented the LlamaIndex framework, Deep Lakevector stores,
and OpenAI’s models. We also built a Python program that collects
data and adds critical metadata to pinpoint the origin of ev ery chunk
of data inadataset. We highlighted the pivotal role of indexes
(vector, tree, list, and keyword types) in giving us greater control
over generativ e AI applications, enabling precise adjustments and
improv ements.
We then thoroughly examined indexed-based RAG through detailed
walkthroughs in Python notebooks, guiding you through seing up
vector stores, conducting adv anced queries, and ensuring the
traceability of AI-generated responses. We introduced metrics based
on the quality ofaresponse and the time elapsed to obtain it.
Exploring drone technology with LLMs show ed us the new skillsets
required to build solid AI pipelines, andwe learned how drone
technology involv es computer vision and, thus, multimodal nodes.
In the upcoming chapter, weinclude multimodal data in our datasets
and expand multimodular RAG.
Questions Answ er the following questions with Yes or No:
Do indexes increase precision and speed in retriev al-augmented
generativ e AI?
Can indexes oﬀer traceability for RAG outputs?
Is index-based search slow er thanvector-based search for large
datasets?
Does LlamaIndex integrate seamlessly with Deep Lake and OpenAI?
Are tree, list, v ector, and keyword indexes the only types of
indexes?
Does the keyword index rely on semantic understanding to
retrievedata?
Is LlamaIndex capable of automatically handling chunking and
embedding?
Are metadata enhancements crucial for ensuring the traceability
of RAG-generated outputs?
Can real-time updates easily be applied to an index-based search
system?
Is cosine similarityametric used in this chapter to ev aluate
query accuracy?
References LlamaIndex: https://docs.llamaindex.ai/en/stable/
Activ eloop Deep Lake: https://docs.activeloop.ai/
OpenAI: https://platform.openai.com/docs/overview Further reading High-Lev el Concepts (RAG), LlamaIndex:
https://docs.llamaindex.ai/en/stable/getting_started
/concepts/
Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag

4
Multimodal Modular RAG for Drone Technology We will take generativ e AI to the next lev el with modular RAG in this
chapter. We will buildasystem that uses diﬀerent components or
modules to handle diﬀerent types of data and tasks. For example, one
module processes textual information using LLMs, aswe havedone
until the last chapter, while another module manages image data,
identifying and labeling objects within images. Imagine using this
technology in drones, which havebecome crucial acrossvarious
industries, oﬀering enhanced capabilities for aerial photography,
eﬃcient agricultural monitoring, and eﬀectivesearch and rescue
operations. They ev en use adv anced computer vision technology and
algorithms to analyze images and identify objects like pedestrians,
cars, trucks, and more. We can then activ ate an LLM agent to retriev e,
augment, and respond toauser ’s question.
In this chapter, wewill buildamultimodal modular RAG program to
generate responses to queries about drone technology using text and
image data from multiple sources. We will ﬁrst deﬁne the main
aspects of modular RAG, multimodal data, multisource retriev al,
modular generation, and augmented output. We will then buildamultimodal modular RAG-driv en generativ e AI system in Python
applied to drone technology with LlamaIndex, Deep Lake, and OpenAI.
Our system will use two datasets: the ﬁrst one containing textual
information about drones thatwe built in the previous chapter and
the second one containing drone images and labels from Activ eloop.
We will use Deep Lake to work with multimodal data, LlamaIndex
for indexing and retriev al, and generativequeries with OpenAI
LLMs. We will add multimodal augmented outputs with text and
images. Finally, wewill build performance metrics for the text
responses and introduce an image recognition metric with GPT-4o,
OpenAI’s pow erful Multimodal LLM (MMLLM ). By the end of the
chapter, you will know how to buildamultimodal modular RAG
workﬂow lev eraging innov ative multimodal and multisource
functionalities.
This chapter cov ers the following topics:
Multimodal modular RAG
Multisource retriev al OpenAI LLM-guided multimodal multisource retriev al Deep Lake multimodal datasets Image metadata-based retriev al Augmented multimodal output Let’s begin by deﬁning multimodal modular RAG.
What is multimodal modular RAG?
Multimodal data combines diﬀerent forms of information, such as
text, images, audio, and video, to enrich data analysis and
interpretation. Meanwhile, a system isamodular RAG system when
it utilizes distinct modules for handling diﬀerent data types and
tasks. Each module is specialized; for example, one module will focus
on text and another on images, demonstratingasophisticated
integration capability that enhances response generation with
retriev ed multimodal data.
The program in this chapter will also be multisource through the two
datasetswe will use. We will use the LLM dataset on the drone
technology built in the previous chapter. We will also use the Deep Lake multimodal V isDrone dataset, which contains thousands of
labeled images captured by drones.
We haveselected drones for our example since drones havebecome
crucial acrossvarious industries, oﬀering enhanced capabilities for
aerial photography, eﬃcient agricultural monitoring, and eﬀectivesearch and rescue operations. They also facilitate wildlife tracking,
streamline commercial deliv eries, and enable safer infrastructure
inspections. Additionally, drones support environmental research,
traﬃc management, and ﬁreﬁghting. They can enhance surv eillance
for law enforcement, revolutionizing multiple ﬁelds by improving
accessibility, safety, and cost-eﬃciency.
Figure 4.1 contains the workﬂowwe will implement in this chapter. It
is based on the generativ e RAG ecosystem illustrated in Figure 1.3
from Chapter 1 , Why Retriev al-Augmented Generation? . We added
embedding and indexing functionality in the previous chapters, but
this chapter will focus on retriev al and generation. The systemwe
will build blurs the lines betw een retriev al and generation since the
generator is intensiv ely used for retrieving (seamless scoring and
ranking) aswell as generating in the chapter ’s notebook.
Figure 4.1: A multimodal modular RAG system This chapter aims to build an educational modular RAG question-
answ ering system focused on drone technology. You can rely on the
functionality implemented in the notebooks of the preceding
chapters, such as Deep Lake forvectors in Chapter 2 , RAG Embedding Vector Stores with Deep Lake and OpenAI , and indices with LlamaIndex
in Chapter 3 , Building Index-based RAG with LlamaIndex, Deep Lake, and OpenAI . If necessary, take your time to go back to the previous
chapters and havea look.
Let’s go through the multimodal, multisource, modular RAG
ecosystem in this chapter, represented in Figure 4.1 . We will use the
titles and subsections in this chapter represented in italics. Also, each
phase is preceded by its location in Figure 4.1 .
(D4) Loading the LLM dataset created in Chapter 3 , which contains
textual data on drones.
(D4) Initializing the LLM query engine with a LlamaIndexvector
store index using VectorStoreIndex and seing the created index
for the query engine, which ov erlaps with (G4) as botharetriev er andagenerator with the OpenAI GPT model.
(G1) Deﬁning the user input for multimodal modular RAG for both
the LLM query engine (for the textual dataset) and the
multimodal query engine (for the VisDrone dataset).
Once the textual dataset has been loaded, the query engine has
been created, and the user input has been deﬁned asabaseline
query for the textual dataset and the multimodal dataset, the
process continues by generatingaresponse for the textual
dataset created in Chapter 2 .
While querying the textual dataset , (G1) , (G2) , and (G4) overlap in
the same seamless LlamaIndex process that retriev es data and
generates content. The response is sav ed as llm_response for the
duration of the session.
Now, the multimodal VisDrone dataset will be loaded into memory
and queried:
(D4) The multimodal process begins by loading and visualizing the
multimodal dataset . The program then continues by navigating the
multimodal dataset structure , selecting an image , and adding
bounding boxes .
The same process as for the textual dataset is then applied to the VisDrone multimodal dataset:
(D4) Buildingamultimodal query engine with LlamaIndex by
creatingav ector store index based on VisDrone data using VectorStoreIndex and seing the created index for the query
engine, which ov erlaps with (G4) as botharetriev er andagenerator with OpenAI GPT .
(G1) The user input for the multimodal search engine is the same
as the user input for multimodal modular RAG since it is used for
both the LLM query engine (for the textual dataset) and the
multimodal query engine (for the VisDrone dataset).
The multimodal VisDrone dataset will now be loaded and indexed,
and the query engine is ready. The purpose of (G1) user input is for
the LlamaIndex query engine to retrieverelev ant documents from VisDrone using an LLM—in this case, an OpenAI model. Then, the
retriev al functions will trace the response back to its source in the
multimodal dataset to ﬁnd the image of the source nodes. We are, in
fact, using the query engine to reach an image through its textual
response:
(G1) , (G2) , and (G4) overlap inaseamless LlamaIndex query
when runningaquery on the VisDrone multimodal dataset.
Processing the response (G4) to ﬁnd the source node and retrieveits image leads us back to (D4) for image retriev al. This leads to
selecting and processing the image of the source node.
At this point, wenow havethe textual and the image response. We
can then buildasummary and apply an accuracy performance metric
after having visualized the time elapsed for each phase aswe built
the program:
(G4) We presentamerged output with the LLM response and
the augmented output with the image of the multimodal
response inamultimodal modular summary .
(E) Finally, wecreate an LLM performance metric andamultimodal
performance metric . We then sum them up asamultimodal modular RAG performance metric .
We can draw two conclusions from this multimodal modular RAG
system:
The systemwe are building in this chapter is one of the many
ways RAG-driv en generativ e AI can be designed in real-life
projects. Each project will haveits speciﬁc needs and
architecture.
The rapid evolution from generativ e AI to the complexity of RAG-driv en generativ e AI requires the corresponding
development of seamlessly integrated cross-platform
components such as LlamaIndex, Deep Lake, and OpenAI in this
chapter. These platforms are also integrated with many other
frameworks, such as Pinecone and LangChain, whichwe will
discuss in Chapter 6 , Scaling RAG Bank Customer Data with Pinecone .
Now, let’s diveinto Python and build the multimodal modular RAG
program.
Buildingamultimodal modular RAG
program for drone technology In the following sections, wewill buildamultimodal modular RAG-
driven generativesystem from scratch in Python, step by step. We
will implement:
LlamaIndex-managed OpenAI LLMs to process and understand
text about drones Deep Lake multimodal datasets containing images and labels of
drone images taken Functions to display images and identify objects within them
using bounding boxes A system that can answ er questions about drone technology
using both text and images Performance metrics aimed at measuring the accuracy of the
modular multimodal responses, including image analysis with GPT-4o Also, make sure you havecreated the LLM dataset in Chapter 2 since
we will be loading it in this section. How ever, you can read this
chapter without running the notebook since it is self-contained with
code and explanations. Now, let’s get to work!
Open the Multimodal_Modular_RAG_Drones.ipynb notebook in the GitHub repository for this chapter at
https://github.com/Denis2054/RAG-Driven-Generative-
AI/tree/main/Chapter04 . The packages installed are the same as
those listed in the Installing the environment section of the previous
chapter. Each of the following sections will guide you through
building the multimodal modular notebook, starting with the LLM
module. Let’s go through each section of the notebook step by step.
Loading the LLM dataset We will load the drone dataset created in Chapter 3 . Make sure to
insert the path to your dataset:
import deeplake
dataset_path_llm = "hub://denis76/drone_v2"
ds_llm = deeplake.load(dataset_path_llm)
The output will conﬁrm that the dataset is loaded and will display
the link to your dataset:
This dataset can be visualized in Jupyter Notebook by ds.visuali
hub://denis76/drone_v2 loaded successfully.
The program now createsadictionary to hold the data to load it intoapandas DataFrame to visualize it:
import json
import pandas as pd
import numpy as np
# Createadictionary to hold the data
data_llm = {}
# Iterate through the tensors in the dataset
for tensor_name in ds_llm.tensors:
tensor_data = ds_llm[tensor_name].numpy()
# Check if the tensor is multi-dimensional
if tensor_data.ndim > 1:
# Flatten multi-dimensional tensors
data_llm[tensor_name] = [np.array(e).flatten().tolist()
else:
# Convert 1D tensors directly to lists and decode text
if tensor_name == "text":
data_llm[tensor_name] = [t.tobytes().decode('utf-8'
else:
data_llm[tensor_name] = tensor_data.tolist()
# Create a Pandas DataFrame from the dictionary
df_llm = pd.DataFrame(data_llm)
df_llm The output shows the text dataset with its structure: embedding
(vectors), id (unique string identiﬁer ), metadata (in this case, the
source of the data), and text, which contains the content:
Figure 4.2: Output of the text dataset structure and content We will now initialize the LLM query engine.
Initializing the LLM query engine As in Chapter 3 , Building Indexed-Based RAG with LlamaIndex, Deep Lake, and OpenAI , we will initializeav ector store index from the
collection of drone documents ( documents_llm) of the dataset ( ds). The GPTVectorStoreIndex.from_documents() method creates an index that
increases the retriev al speed of documents based onvector similarity:
from llama_index.core import VectorStoreIndex
vector_store_index_llm = VectorStoreIndex.from_documents(docume The as_query_engine() method conﬁgures this index asaquery
engine with the speciﬁc parameters, as in Chapter 3 , for similarity and
retriev al depth, allowing the system to answ er queries by ﬁnding the
most relev ant documents:
vector_query_engine_llm = vector_store_index_llm.as_query_engin Now, the program introduces the user input.
User input for multimodal modular RAG
The goal of deﬁning the user input in the context of the modular RAG
system is to formulateaquery that will eﬀectiv ely utilize both the
text-based and image-based capabilities. This allows the system to
generateacomprehensiveand accurate response by lev eraging
multiple information sources:
user_input="How do drones identifyatruck?"
In this context, the user input is the baseline , the starting point, orastandard query used to assess the system’s capabilities. It will
establish the initial frame of reference for howwell the system can
handle and respond to queries utilizing its av ailable resources (e.g.,
text and image data fromvarious datasets). In this example, the
baseline is empirical and will serveto ev aluate the system from that
reference point.
Querying the textual dataset We will run thevector query engine request aswe did in Chapter 3 :
import time
import textwrap
#start the timer
start_time = time.time()
llm_response = vector_query_engine_llm.query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
print(textwrap.fill(str(llm_response), 100))
The execution time is satisfactory:
Query execution time: 1.5489 seconds The output content is also satisfactory:
Drones can identifyatruck using visual detection and tracking The program now loads the multimodal drone dataset.
Loading and visualizing the multimodal
dataset We will use the existing pubic V isDrone dataset av ailable on Deep Lake:
https://datasets.activeloop.ai/docs/ml/datasets/visdrone
-dataset/ . We will not createav ector store but simply load the
existing dataset in memory:
import deeplake
dataset_path = 'hub://activeloop/visdrone-det-train'
ds = deeplake.load(dataset_path) # Returns a Deep Lake Dataset The output will displayalink to the online dataset that you can
explore with SQL, or natural language processing commands if you
prefer, with the tools provided by Deep Lake:
Opening dataset in read-only mode as you don't have write permis This dataset can be visualized in Jupyter Notebook by ds.visuali
hub://activeloop/visdrone-det-train loaded successfully.
Let’s display the summary to explore the dataset in code:
ds.summary()
The output provides useful information on the structure of the
dataset:
Dataset(path='hub://activeloop/visdrone-det-train', read_only=Tr
tensor htype shape dtype compress
------ ----- ----- ----- -------
boxes bbox (6471, 1:914, 4) float32 N
images image (6471, 360:1500,
480:2000, 3) uint8 j
labels class_label (6471, 1:914) uint32 N
The structure contains images, boxes for the boundary boxes of the
objects in the image, and labels describing the images and boundary
boxes. Let’s visualize the dataset in code:
ds.visualize()
The output shows the images and their boundary boxes:
Figure 4.3: Output showing boundary boxes Now, let’s go further and display the content of the dataset inapandas DataFrame to see what the images look like:
import pandas as pd
# Create an empty DataFrame with the defined structure
df = pd.DataFrame(columns=['image', 'boxes', 'labels'])
# Iterate through the samples using enumerate
for i, sample in enumerate(ds):
# Image data (choose either path or compressed representati
# df.loc[i, 'image'] = sample.images.path # Store image pa
df.loc[i, 'image'] = sample.images.tobytes() # Store compr
# Bounding box data (asalist of lists)
boxes_list = sample.boxes.numpy(aslist=True)
df.loc[i, 'boxes'] = [box.tolist() for box in boxes_list]
# Label data (asalist)
label_data = sample.labels.data()
df.loc[i, 'labels'] = label_data['text']
df The output in Figure 4.4 shows the content of the dataset:
Figure 4.4: Excerpt of the VisDrone dataset There are 6,471 rows of images in the dataset and 3 columns:
The image column contains the image. The format of the image in
the dataset, as indicated by the byte sequence
b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00...', is JPEG.
The bytes b'\xff\xd8\xff\xe0' speciﬁcally signify the start of a JPEG image ﬁle.
The boxes column contains the coordinates and dimensions of
bounding boxes in the image, which are normally in the format
[x, y, width, height].
The labels column contains the label of each bounding box in
the boxes column.
We can display the list of labels for the images:
labels_list = ds.labels.info['class_names']
labels_list The output provides the list of labels, which deﬁnes the scope of the
dataset:
['ignored regions',
'pedestrian',
'people',
'bicycle',
'car',
'van',
'truck',
'tricycle',
'awning-tricycle',
'bus',
'motor',
'others']
With that, wehavesuccessfully loaded the dataset and will now
explore the multimodal dataset structure.
Navigating the multimodal dataset
structure In this section, wewill select an image and display it using the
dataset’s image column. To this image, wewill then add the
bounding boxes ofalabel thatwe will choose. The program ﬁrst
selects an image.
Selecting and displaying an image We will select the ﬁrst image in the dataset:
# choose an image
ind=0
image = ds.images[ind].numpy() # Fetch the first image and retu Now, let’s display it with no bounding boxes:
import deeplake
from IPython.display import display
from PIL import Image
import cv2 # Import OpenCV
image = ds.images[0].numpy()
# Convert from BGR to RGB (if necessary)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
# Create PIL Image and display
img = Image.fromarray(image_rgb)
display(img)
The image display ed contains trucks, pedestrians, and other types of
objects:
Figure 4.5: Output displaying objects Now that the image is display ed, the program will add bounding
boxes.
Adding bounding boxes and saving the
image We havedisplay ed the ﬁrst image. The program will then fetch all the
labels for the selected image:
labels = ds.labels[ind].data() # Fetch the labels in the select
print(labels)
The output displays value, which contains the numerical indices ofalabel, and text, which contains the corresponding text labels ofalabel:
{'value': array([1, 1, 7, 1, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6,
1, 1, 1, 1, 1, 1, 6, 6, 3, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1,
1, 6, 6, 6], dtype=uint32), 'text': ['pedestrian', 'pedes We can display thevalues and the corresponding text in two
columns:
values = labels['value']
text_labels = labels['text']
# Determine the maximum text label length for formatting
max_text_length = max(len(label) for label in text_labels)
# Print the header
print(f"{'Index':<10}{'Label':<{max_text_length + 2}}")
print("-" * (10 + max_text_length + 2)) # Addaseparator line
# Print the indices and labels in two columns
for index, label in zip(values, text_labels):
print(f"{index:<10}{label:<{max_text_length + 2}}")
The output giv es usaclear representation of the content of the labels
of an image:
Index Label
----------------------
1 pedestrian
1 pedestrian
7 tricycle
1 pedestrian
1 pedestrian
1 pedestrian
1 pedestrian
6 truck
6 truck …
We can group the class names (labels in plain text) of the images:
ds.labels[ind].info['class_names'] # class names of the selecte We can now group and display all the labels that describe the image:
ds.labels[ind].info['class_names'] #class names of the selected We can see all the classes the image contains:
['ignored regions',
'pedestrian',
'people',
'bicycle',
'car',
'van',
'truck',
'tricycle',
'awning-tricycle',
'bus',
'motor',
'others']
The number of label classes sometimes exceeds whatahuman eyecan see in an image.
Let’s now add bounding boxes. We ﬁrst createafunction to add the
bounding boxes, display them, and savethe image:
def display_image_with_bboxes(image_data, bboxes, labels, label
#Displays an image with bounding boxes foraspecific label
image_bytes = io.BytesIO(image_data)
img = Image.open(image_bytes)
# Extract class names specifically for the selected image
class_names = ds.labels[ind].info['class_names']
# Filter for the specific label (or display all if class na
if class_names is not None:
try:
label_index = class_names.index(label_name)
relevant_indices = np.where(labels == label_index)[
except ValueError:
print(f"Warning: Label '{label_name}' not found. Di
relevant_indices = range(len(labels))
else:
relevant_indices = [] # No labels found, so display no
# Draw bounding boxes
draw = ImageDraw.Draw(img)
for idx, box in enumerate(bboxes): # Enumerate over bboxes
if idx in relevant_indices: # Check if this box is re
x1, y1, w, h = box
x2, y2 = x1 + w, y1 + h
draw.rectangle([x1, y1, x2, y2], outline="red", wid
draw.text((x1, y1), label_name, fill="red")
# Save the image
save_path="boxed_image.jpg"
img.save(save_path)
display(img)
We can add the bounding boxes foraspeciﬁc label. In this case, weselected the "truck" label:
import io
from PIL import ImageDraw
# Fetch labels and image data for the selected image
labels = ds.labels[ind].data()['value']
image_data = ds.images[ind].tobytes()
bboxes = ds.boxes[ind].numpy()
ibox="truck" # class in image
# Display the image with bounding boxes for the label chosen
display_image_with_bboxes(image_data, bboxes, labels, label_nam The image display ed now contains the bounding boxes for trucks:
Figure 4.6: Output displaying bounding boxes Let’s now activ ateaquery engine to retrieveand obtainaresponse.
Buildingamultimodal query engine In this section, wewill query the V isDrone dataset and retrievean
image that ﬁts the user inputwe entered in the User input for
multimodal modular RAG section of this notebook. To achievethis
goal, wewill:
1. Createav ector index for each row of the df DataFrame
containing the images, boxing data, and labels of the V isDrone
dataset.
2. Createaquery engine that will query the text data of the dataset,
retrieverelev ant image information, and provideatext response.
3. Parse the nodes of the response to ﬁnd the keywords related to
the user input.
4. Parse the nodes of the response to ﬁnd the source image.
5. Add the bounding boxes of the source image to the image.
6. Save the image.
Creatingavector index and query
engine The code ﬁrst createsadocument that will be processed to createavector store index for the multimodal drone dataset. The df DataFramewe created in the Loading and visualizing the multimodal
dataset section of the notebook on GitHub does not haveunique
indices or embeddings. We will create them in memory with LlamaIndex.
The program ﬁrst assignsaunique ID to the DataFrame:
# The DataFrame is named 'df'
df['doc_id'] = df.index.astype(str) # Create unique IDs from t This line addsanew column to the df DataFrame called doc_id. It
assigns unique identiﬁers to each row by conv erting the DataFrame’s
row indices to strings. An empty list named documents is initialized,
whichwe will use to createav ector index:
# Create documents (extract relevant text for each image's labe
documents = []
Now, the iterrows() method iterates through each row of the DataFrame, generatingasequence of index and row pairs:
for _, row in df.iterrows():
text_labels = row['labels'] # Each label is nowastring
text = " ".join(text_labels) # Join text labels intoasing
document = Document(text=text, doc_id=row['doc_id'])
documents.append(document)
documents is appended with all the records in the dataset, and a DataFrame is created:
# The DataFrame is named 'df'
df['doc_id'] = df.index.astype(str) # Create unique IDs from t
# Create documents (extract relevant text for each image's labe
documents = []
for _, row in df.iterrows():
text_labels = row['labels'] # Each label is nowastring
text = " ".join(text_labels) # Join text labels intoasing
document = Document(text=text, doc_id=row['doc_id'])
documents.append(document)
The documents are now ready to be indexed with GPTVectorStoreIndex:
from llama_index.core import GPTVectorStoreIndex
vector_store_index = GPTVectorStoreIndex.from_documents(documen The dataset is then seamlessly equipped with indices thatwe can
visualize in the index dictionary:
vector_store_index.index_struct The output shows that an index has now been added to the dataset:
IndexDict(index_id='4ec313b4-9a1a-41df-a3d8-a4fe5ff6022c', summa We can now runaquery on the multimodal dataset.
Runningaquery on the VisDrone
multimodal dataset We now set vector_store_index as the query engine, aswe did in the Vector store index query engine section in Chapter 3 :
vector_query_engine = vector_store_index.as_query_engine(simila We can also runaquery on the dataset of drone images, just aswe
did in Chapter 3 on an LLM dataset:
import time
start_time = time.time()
response = vector_query_engine.query(user_input)
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Query execution time: {elapsed_time:.4f} seconds")
The execution time is satisfactory:
Query execution time: 1.8461 seconds We will now examine the text response:
print(textwrap.fill(str(response), 100))
We can see that the output is logical and therefore satisfactory.
Drones usevarious sensors such as cameras, LiDAR, and GPS to
identify and track objects like trucks.
Processing the response We will now parse the nodes in the response to ﬁnd the unique
words in the response and select one for this notebook:
from itertools import groupby
def get_unique_words(text):
text = text.lower().strip()
words = text.split()
unique_words = [word for word, _ in groupby(sorted(words))]
return unique_words
for node in response.source_nodes:
print(node.node_id)
# Get unique words from the node text:
node_text = node.get_text()
unique_words = get_unique_words(node_text)
print("Unique Words in Node Text:", unique_words)
We foundaunique word ( 'truck') and its unique index, which will
lead us directly to the image of the source of the node that generated
the response:
1af106df-c5a6-4f48-ac17-f953dffd2402
Unique Words in Node Text: ['truck']
We could select more words and design this function in many
diﬀerentways depending on the speciﬁcations of each project.
We will now search for the image by going through the source nodes,
just aswe did for an LLM dataset in the Query response and source
section of the previous chapter. Multimodalvector stores and
querying frameworks are ﬂexible. Oncewe learn how to perform
retriev als on an LLM andamultimodal dataset, weare ready for
anything that comes up!
Let’s select and process the information related to an image.
Selecting and processing the image of
the source node Before running the image retriev al and displaying function, let’s ﬁrst
delete the imagewe display ed in the Adding bounding boxes and saving
the image section of this notebook to make surewe are working onanew image:
# deleting any image previously saved
!rm /content/boxed_image.jpg We are now ready to search for the source image, call the bounding
box, and display and savethe functionwe deﬁned earlier:
display_image_with_bboxes(image_data, bboxes, labels, label_nam The program now goes through the source nodes with the keyword
"truck" search, applies the bounding boxes, and displays and sav es
the image:
import io
from PIL import Image
def process_and_display(response, df, ds, unique_words):
"""Processes nodes, finds corresponding images in dataset,
Args:
response: The response object containing source nodes.
df: The DataFrame with doc_id information.
ds: The dataset containing images, labels, and boxes.
unique_words: The list of unique words for filtering.
"""
…
if i == row_index:
image_bytes = io.BytesIO(sample.images.tobytes(
img = Image.open(image_bytes)
labels = ds.labels[i].data()['value']
image_data = ds.images[i].tobytes()
bboxes = ds.boxes[i].numpy()
ibox = unique_words[0] # class in image
display_image_with_bboxes(image_data, bboxes, l
# Assuming you have your 'response', 'df', 'ds', and 'unique_wo
process_and_display(response, df, ds, unique_words)
The output is satisfactory:
Figure 4.7: Displayed satisfactory output Multimodal modular summary We havebuiltamultimodal modular program step by step thatwe
can now assemble inasummary. We will createafunction to display
the source image of the response to the user input, then print the user
input and the LLM output, and display the image.
First, wecreateafunction to display the source image sav ed by the
multimodal retriev al engine:
# 1.user input=user_input
print(user_input)
# 2.LLM response
print(textwrap.fill(str(llm_response), 100))
# 3.Multimodal response
image_path = "/content/boxed_image.jpg"
display_source_image(image_path)
Then, wecan display the user input, the LLM response, and the
multimodal response. The output ﬁrst displays the textual responses
(user input and LLM response):
How do drones identifyatruck?
Drones can identifyatruck using visual detection and tracking Then, the image is display ed with the bounding boxes for trucks in
this case:

Figure 4.8: Output displaying boundary boxes By adding an image toaclassical LLM response, weaugmented the
output. Multimodal RAG output augmentation will enrich generativ e AI by adding information to both the input and output. How ever, as
for all AI programs, designingaperformance metric requires eﬃcient
image recognition functionality.
Performance metric Measuring the performance ofamultimodal modular RAG requires
two types of measurements: text and image. Measuring text is
straightforw ard. How ever, measuring images is quiteachallenge.
Analyzing the image ofamultimodal response is quite diﬀerent. We
extractedakeyword from the multimodal query engine. We then
parsed the response forasource image to display. How ever, wewill
need to build an innov ative approach to ev aluate the source image of
the response. Let’s begin with the LLM performance.
LLM performance metric LlamaIndex seamlessly called an OpenAI model through its query
engine, such as GPT-4, for example, and provided text content in its
response. For text responses, wewill use the same cosine similarity
metric as in the Evaluating the output with cosine similarity section in Chapter 2 , and the Vector store index query engine section in Chapter 3 .
The ev aluation function uses sklearn and sentence_transformers to
evaluate the similarity betw een two texts—in this case, an input and
an output:
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
def calculate_cosine_similarity_with_embeddings(text1, text2):
embeddings1 = model.encode(text1)
embeddings2 = model.encode(text2)
similarity = cosine_similarity([embeddings1], [embeddings2]
return similarity[0][0]
We can now calculate the similarity betw een our baseline user input
and the initial LLM response obtained:
llm_similarity_score = calculate_cosine_similarity_with_embeddi
print(user_input)
print(llm_response)
print(f"Cosine Similarity Score: {llm_similarity_score:.3f}")
The output displays the user input, the text response, and the cosine
similarity betw een the two texts:
How do drones identifyatruck?
How do drones identifyatruck?
Drones can identifyatruck using visual detection and tracking Cosine Similarity Score: 0.691
The output is satisfactory. Butwe now need to designaw ay to
measure the multimodal performance.
Multimodal performance metric To ev aluate the image returned, wecannot simply rely on the labels
in the dataset. For small datasets, wecan manually check the image,
but whenasystem scales, automation is required. In this section, wewill use the computer vision features of GPT-4o to analyze an image,
parse it to ﬁnd the objectswe are looking for, and provideadescription of that image. Then, wewill apply cosine similarity to the
description provided by GPT-4o and the label it is supposed to
contain. GPT-4o isamultimodal generativ e AI model.
Let’s ﬁrst encode the image to simplify data transmission to GPT-4o.
Base64 encoding conv erts binary data (like images) into ASCII
characters, which are standard text characters. This transformation is
crucial because it ensures that the image data can be transmied ov er
protocols (like HTTP) that are designed to handle text data smoothly.
It also avoids issues related to binary data transmission, such as data
corruption or interpretation errors.
The program encodes the source image using Python’s base64
module:
import base64
IMAGE_PATH = "/content/boxed_image.jpg"
# Open the image file and encode it asabase64 string
def encode_image(image_path):
with open(image_path, "rb") as image_file:
return base64.b64encode(image_file.read()).decode("utf-
base64_image = encode_image(IMAGE_PATH)
We now create an OpenAI client and set the model to gpt-4o:
from openai import OpenAI
#Set the API key for the client
client = OpenAI(api_key=openai.api_key)
MODEL="gpt-4o"
The unique word will be the result of the LLM query to the
multimodal datasetwe obtained by parsing the response:
u_word=unique_words[0]
print(u_word)
We can now submit the image to OpenAI GPT-4o:
response = client.chat.completions.create(
model=MODEL,
messages=[
{"role": "system", "content": f"You areahelpful assis
{"role": "user", "content": [
{"type": "text", "text": f"Analyze the following im
{"type": "image_url", "image_url": {
"url": f"data:image/png;base64,{base64_image}"}
}
]}
],
temperature=0.0,
)
response_image = response.choices[0].message.content
print(response_image)
We instructed the system and user roles to analyze images looking
for our target label, u_word—in this case, truck. We then submied
the source node image to the model. The output that describes the
image is satisfactory:
The image contains two trucks within the bounding boxes. Here is
1. **First Truck (Top Bounding Box)**:
- The truck appears to beaflatbed truck.
- It is loaded with various materials, possibly construction
- The truck is parked in an area with other construction mate
2. **Second Truck (Bottom Bounding Box)**:
- This truck also appears to beaflatbed truck.
- It is carrying different types of materials, similar to the
- The truck is situated inasimilar environment, surrounded Both trucks are inaconstruction or industrial area, likely use We can now submit this response to the cosine similarity function by
ﬁrst adding an "s" to align with multiple trucks inaresponse:
resp=u_word+"s"
multimodal_similarity_score = calculate_cosine_similarity_with_
print(f"Cosine Similarity Score: {multimodal_similarity_score:.
The output describes the imagewell but contains many other
descriptions beyond the word “ truck,” which limits its similarity to
the input requested:
Cosine Similarity Score: 0.505
A human observ er might approvethe image and the LLM response.
How ever, ev en if the scorewas very high, the issue would be the
same. Complex images are challenging to analyze in detail and with
precision, although progress is continually made. Let’s now calculate
the ov erall performance of the system.
Multimodal modular RAG performance
metric To obtain the ov erall performance of the system, wewill divide the
sum of the LLM response and the two multimodal response
performances by 2:
score=(llm_similarity_score+multimodal_similarity_score)/2
print(f"Multimodal, Modular Score: {score:.3f}")
The result shows that althoughahuman who observ es the results
may be satisﬁed, it remains diﬃcult to automatically assess the
relev ance ofacomplex image:
Multimodal, Modular Score: 0.598
The metric can be improv ed becauseahuman observ er sees that the
image is relev ant. This explains why the top AI agents, such as ChatGPT , Gemini, and Bing Copilot, alw ays havea feedback process
that includes thumbs up and thumbs down.
Let’s now sum up the chapter and gear up to explore how RAG can
be improv ed ev en further with human feedback.
Summary This chapter introduced us to the world of multimodal modular RAG, which uses distinct modules for diﬀerent data types (text and
image) and tasks. We lev eraged the functionality of LlamaIndex,
Deep Lake, and OpenAI, whichwe explored in the previous chapters.
The Deep Lake V isDrone dataset further introduced us to drone
technology for analyzing images and identifying objects. The dataset
contained images, labels, and bounding box information. Working on
drone technology involv es multimodal data, encouraging us to
develop skills thatwe can use across many domains, such as wildlife
tracking, streamlining commercial deliv eries, and making safer
infrastructure inspections.
We builtamultimodal modular RAG-driv en generativ e AI system.
The ﬁrst stepwas to deﬁneabaseline user query for both LLM and
multimodal queries. We began by querying the Deep Lake textual
dataset thatwe implemented in Chapter 3 . LlamaIndex seamlessly ranaquery engine to retriev e, augment, and generatearesponse. Then,
we loaded the Deep Lake V isDrone dataset and indexed it in memory
with LlamaIndex to create an indexedvector search retriev al pipeline.
We queried it through LlamaIndex, which used an OpenAI model
such as GPT-4 and parsed the text generated forakeyword. Finally,
we searched the source nodes of the response to ﬁnd the source
image, display it, and merge the LLM and image responses into an
augmented output. We applied cosine similarity to the text response.
Evaluating the imagewas challenging, sowe ﬁrst ran image
recognition with GPT-4o on the image retriev ed to obtainatext to
whichwe applied cosine similarity.
The journey into multimodal modular RAG-driv en generativ e AI
took us deep into the cuing edge of AI. Buildingacomplex system
was good preparation for real-life AI projects, which often require
implementing multisource, multimodal, and unstructured data,
leading to modular, complex systems. Thanks to transparent access to
the source ofaresponse, the complexity of RAG can be harnessed,
controlled, and improv ed. We will see howwe can lev erage the
transparency of the sources ofaresponse to introduce human
feedback to improv e AI. The next chapter will take us further into
transparency and precision in AI.
Questions Answ er the following questions with Yes or No:
1. Does multimodal modular RAG handle diﬀerent types of data,
such as text and images?
2. Are drones used solely for agricultural monitoring and aerial
photography?
3. Is the Deep Lake V isDrone dataset used in this chapter for
textual data only?
4. Can bounding boxes be added to drone images to identify
objects such as trucks and pedestrians?
5. Does the modular system retrieveboth text and image data for
query responses?
6. Is buildingav ector index necessary for querying the multimodal VisDrone dataset?
7. Are the retriev ed images processed without adding any labels or
bounding boxes?
8. Is the multimodal modular RAG performance metric based only
on textual responses?
9. Canamultimodal system such as the one described in this
chapter handle only drone-related data?
10. Is ev aluating images as easy as ev aluating text in multimodal RAG?
References LlamaIndex: https://docs.llamaindex.ai/en/stable/
Activ eloop Deep Lake: https://docs.activeloop.ai/
OpenAI: https://platform.openai.com/docs/overview Further reading Retriev al-Augmented Multimodal Language Modeling,
Yasunaga et al. (2023), https://arxiv.org/pdf/2211.12561
Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag

5
Boosting RAG P erformance with Expert Human F eedback Human feedback (HF) is not just useful for generativ e AI—it’s
essential, especially when it comes to models using RAG. A
generativ e AI model uses information from datasets withvarious
documents during training. The data that trained the AI model is set
in stone in the model’s parameters; wecan’t change it unlesswe train
it again. How ever, in the world of retriev al-based text and
multimodal datasets, there is informationwe can see and tw eak. That
is where HF comes in. By providing feedback on what the AI model
pulls from its datasets, HF can directly inﬂuence the quality of its
future responses. Engaging with this process makes humans an activeplay er in the RAG’s dev elopment. It addsanew dimension to AI
projects: adaptiv e RAG.
We haveexplored and implemented naïv e, adv anced, and modular RAG so far. Now, wewill add adaptiv e RAG to our generativ e AI
toolbox. We know that ev en the best generativ e AI system with the
best metrics cannot convinceadissatisﬁed user that it is helpful if it
isn’t. We will introduce adaptiv e RAG with an HF loop. The system
thus becomes adaptivebecause the documents used for retriev al are
updated. Integrating HF in RAG leads toapragmatic hybrid
approach because it involv es humans in an otherwise automated
generativeprocess. We will thus lev erage HF, whichwe will use to
buildahybrid adaptiv e RAG program in Python from scratch, going
through the key steps of building a RAG-driv en generativ e AI system
from the ground up. By the end of this chapter, you will havea
theoretical understanding of the adaptiv e RAG framework and
practical experience in building an AI model based on HF.
This chapter cov ers the following topics:
Deﬁning the adaptiv e RAG ecosystem Applying adaptiv e RAG to augmented retriev al queries Automating augmented generativ e AI inputs with HF
Automating end-user feedback rankings to trigger expert HF
Creating an automated feedback system forahuman expert Integrating HF with adaptiv e RAG for GPT-4o Let’s begin by deﬁning adaptiv e RAG.
Adaptive RAG
No, RAG cannot solveall our problems and challenges. RAG, just
like any generativemodel, can also produce irrelev ant and incorrect
output! RAG might beauseful option, how ever, becausewe feed
pertinent documents to the generativ e AI model that inform its
responses. Nonetheless, the quality of RAG outputs depends on the
accuracy and relev ance of the underlying data, which calls for
veriﬁcation! That’s where adaptiv e RAG comes in. Adaptiv e RAG
introduces human, real-life, pragmatic feedback that will improvea RAG-driv en generativ e AI ecosystem.
The core information inagenerativ e AI model is parametric (stored
asweights). But in the context of RAG, this data can be visualized
and controlled, aswe saw in Chapter 2 , RAG Embedding V ector Stores
with Deep Lake and OpenAI . Despite this, challenges remain; for
example, the end-user might write fuzzy queries, or the RAG data
retriev al might be faulty. An HF process is, therefore, highly
recommended to ensure the system’s reliability.
Figure 1.3 from Chapter 1 , Why Retriev al Augmented Generation? ,
represents the complete RAG framework and ecosystem. Let’s zoom
in on the adaptiv e RAG ecosystem and focus on the key processes
that come into play, as shown in the following ﬁgure:
Figure 5.1: A variant of an adaptive RAG ecosystem Thevariant of an adaptiv e RAG ecosystem in this chapter includes
the following components, as shown in Figure 5.1 , for the retriev er:
D1: Collect and process Wikipedia articles on LLMs by fetching
and cleaning the data D4: Retriev al query to query the retriev al dataset The generator ’s components are:
G1: Input entered by an end-user G2: Augmented input with HF that will augment the user ’s
initial input and prompt engineering to conﬁgure the GPT-4o
model’s prompt G4: Generation and output to run the generativ e AI model and
obtainaresponse The ev aluator ’s components are:
E1: Metrics to applyacosine similarity measurement E2: Human feedback to obtain and process the ultimate
measurement ofasystem through end-user and expert feedback In this chapter, wewill illustrate adaptiv e RAG by buildingahybrid
adaptiv e RAG program in Python on Google Colab. We will build
this program from scratch to acquireaclear understanding of an
adaptiveprocess, which mayvary depending onaproject’s goals, but
the underlying principles remain the same. Through this hands-on
experience, you will learn how to dev elop and customize a RAG
system whenaready-to-use one fails to meet the users’ expectations.
This is important because human users can be dissatisﬁed witharesponse no maer what the performance metrics show. We will also
explore the incorporation of human user rankings to gather expert
feedback on our RAG-driv en generativ e AI system. Finally, wewill
implement an automated ranking system that will decide how to
augment the user input for the generativemodel, oﬀering practical
insights into how a RAG-driv en system can be successfully
implemented inacompany.
We will dev elopaproof of concept forahypothetical company called Company C . This company would like to deployaconv ersational
agent that explains what AI is. The goal is for the employ ees of this
company to understand the basic terms, concepts, and applications of AI. The ML engineer in charge of this RAG-driv en generativ e AI
example would like future users to acquireabeer know ledge of AI
while implementing other AI projects across the sales, production,
and deliv ery domains.
Company C currently faces serious issues with customer support.
Withagrowing number of products and services, their product line
of smartphones of the C-phone series has been experiencing technical
problems with too many customer requests. The IT department
would like to set upaconv ersational agent for these customers.
How ever, the teams are not convinced. The IT department has thus
decided to ﬁrst set upaconv ersational agent to explain what an LLM
is and how it can be helpful in the C-phone series customer support
service.
The program will be hybrid and adaptiveto fulﬁll the needs of Company C:
Hybrid : Real-life scenarios go beyond theoretical frameworks
and conﬁgurations. The system is hybrid becausewe are
integrating HF within the retriev al process that can be processed
in real time. How ever, wewill not parse the content of the
documents withakeyword alone. We will label the documents
(which are W ikipedia URLs in this case), which can be done
automatically, controlled, and improv ed byahuman , if necessary.
Aswe show in this chapter, some documents will be replaced by
human-expert feedback and relabeled. The program will
automatically retrievehuman-expert feedback documents and
raw retriev ed documents to formahybrid (human-machine)
dynamic RAG system.
Adaptiv e: We will introduce human user ranking, expert
feedback, and automated document re-ranking. This HF loop
takes us deep into modular RAG and adaptiv e RAG. Adaptiv e RAG lev erages the ﬂexibility of a RAG system to adapt its
responses to the queries. In this case, wewant HF to be triggered
to improvethe quality of the output.
Real-life projects will inevitably require an ML engineer to go beyond
the boundaries of pre-determined categories. Pragmatism and
necessity encourage creativeand innov ative solutions. For example,
for the hybrid, dynamic, and adaptiveaspects of the system, ML
engineers could imagine any process that works with any type of
algorithm: classical softw are functions, ML clustering algorithms, or
any function that works. In real-life AI, what works, works!
It’s time to buildaproof of concept to show Company C’s
management how hybrid adaptiv e RAG-driv en generativ e AI can
successfully help their teams by:
Proving that AI can work withaproof of concept before scaling
and inv esting inaproject Showing that an AI system can be customized foraspeciﬁc
project Developing solid ground-up skills to face any AI challenge Building the company’s data gov ernance and control of AI
systems Laying solid grounds to scale the system by solving the
problems that will come up during the proof of concept Let’s go to our keyboards!
Building hybrid adaptive RAG in Python Let’s now start building the proof of concept ofahybrid adaptiv e RAG-driv en generativ e AI conﬁguration. Open Adaptive_RAG.ipynb
on GitHub. We will focus on HF and, as such, will not use an existing
framework. We will build our own pipeline and introduce HF.
As established earlier, the program is divided into three separate
parts: the retriev er, generator , and evaluator functions, which can be
separate agents inareal-life project’s pipeline. Try to separate these
functions from the start because, inaproject, sev eral teams might be
working in parallel on separate aspects of the RAG framework.
The titles of each of the following sections correspond
exactly to the names of each section in the program on GitHub. The retriev er functionality comes ﬁrst.
1. Retriever We will ﬁrst outline the initial steps required to set up the
environment for a RAG-driv en generativ e AI model. This process
begins with the installation of essential softw are components and
libraries that facilitate the retriev al and processing of data. We
speciﬁcally cov er the downloading of crucial ﬁles and the installation
of packages needed for eﬀectivedata retriev al andweb scraping.
1.1. Installing the retriever’s
environment Let’s begin by downloading grequests.py from the commons directory
of the GitHub repository. This repository contains resources that can
be common to sev eral programs in the repository, thus avoiding
redundancy.
The download is standard and built around the request:
url = "https://raw.githubusercontent.com/Denis2054/RAG-Driven-G
output_file = "grequests.py"
We will only need two packages for the retriev er sincewe are
building a RAG-driv en generativ e AI model from scratch. We will
install:
requests, the HTTP library to retriev e Wikipedia documents:
!pip install requests==2.32.3
beautifulsoup4, to scrape information fromweb pages:
!pip install beautifulsoup4==4.12.3
We now needadataset.
1.2.1. Preparing the dataset For this proof of concept, wewill retriev e Wikipedia documents by
scraping them through their URLs. The dataset will contain
automated or human-crafted labels for each document, which is the
ﬁrst step tow ard indexing the documents ofadataset:
import requests
from bs4 import BeautifulSoup
import re
# URLs of the Wikipedia articles mapped to keywords
urls = {
"prompt engineering": "https://en.wikipedia.org/wiki/Prompt
"artificial intelligence":"https://en.wikipedia.org/wiki/Ar
"llm": "https://en.wikipedia.org/wiki/Large_language_model"
"llms": "https://en.wikipedia.org/wiki/Large_language_model
}
One or more labels precede each URL. This approach might be
suﬃcient forarelativ ely small dataset.
For speciﬁc projects, includingaproof of concept, this approach can
provideasolid ﬁrst step to go from naïv e RAG (content search with
keywords) to searchingadataset with indexes (the labels in this case).
We now haveto process the data.
1.2.2. Processing the data We ﬁrst applyastandard scraping and text-cleaning function to the
document that will be retriev ed:
def fetch_and_clean(url):
# Fetch the content of the URL
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')
# Find the main content of the article, ignoring side boxes
content = soup.find('div', {'class': 'mw-parser-output'})
# Remove less relevant sections such as "See also", "Refere
for section_title in ['References', 'Bibliography', 'Extern
section = content.find('span', {'id': section_title})
if section:
for sib in section.parent.find_next_siblings():
sib.decompose()
section.parent.decompose()
# Focus on extracting and cleaning text from paragraph tags
paragraphs = content.find_all('p')
cleaned_text = ' '.join(paragraph.get_text(separator=' ', s
cleaned_text = re.sub(r'\[\d+\]', '', cleaned_text) # Remo
return cleaned_text The code fetches the document’s content based on its URL, which is,
in turn, based on its label. This straightforw ard approach may satisfyaproject’s needs depending on its goals. An ML engineer or
developer must alw ays be careful not to ov erloadasystem with
costly and unproﬁtable functions. Moreov er, labelingwebsite URLs
can guidearetriev er pipeline to the correct locations to process data,
regardless of the techniques (load balancing, API call optimization,
etc.) applied. In the end, each project or sub-project will require one
or sev eral techniques, depending on its speciﬁc needs.
Once the fetching and cleaning function is ready, wecan implement
the retriev al process for the user ’s input.
1.3. Retrieval process for user input The ﬁrst step here involv es identifyingakeyword within the user ’s
input. The function process_query takes two parameters: user_input
and num_words. The number of words to retrieveis restricted by
factors like the input limitations of the model, cost considerations,
and ov erall system performance:
import textwrap
def process_query(user_input, num_words):
user_input = user_input.lower()
# Check for any of the specified keywords in the input
matched_keyword = next((keyword for keyword in urls if keyw Upon ﬁndingamatch betw eenakeyword in the user input and the
keywords associated with URLs, the following functions for fetching
and cleaning the data are triggered:
if matched_keyword:
print(f"Fetching data from: {urls[matched_keyword]}")
cleaned_text = fetch_and_clean(urls[matched_keyword])

# Limit the display to the specified number of words fromtwords = cleaned_text.split() # Split the text into words
first_n_words = ' '.join(words[:num_words]) # Join the fir The num_words parameter helps in chunking the text. While this basic
approach may work for use cases withamanageable volume of data,
it’s recommended to embed the data intovectors for more complex
scenarios.
The cleaned and truncated text is then formaed for display:
# Wrap the firstnwords to 80 characters wide for display
wrapped_text = textwrap.fill(first_n_words, width=80)
print("\nFirst {} words of the cleaned text:".format(num_wo
print(wrapped_text) # Print the firstnwords asawell-fo
# Use the exact same first_n_words for the GPT-4 prompt to
prompt = f"Summarize the following information about {match
wrapped_prompt = textwrap.fill(prompt, width=80) # Wrap pr
print("\nPrompt for Generator:", wrapped_prompt)
# Return the specified number of words
return first_n_words
else:
print("No relevant keywords found. Please enteraquery rel
return None Note that the function ultimately returns the ﬁrstnwords, providingaconcise and relev ant snippet of information based on the user ’s
query. This design allows the system to manage data retriev al
eﬃciently while also maintaining user engagement.
2. Generator The generator ecosystem contains sev eral components, sev eral of
which ov erlap with the retriev er functions and user interfaces in the RAG-driv en generativ e AI frameworks:
2.1. Adaptiv e RAG selection based on human rankings : This
will be based on the ratings ofauser panel ov er time. Inareal-
life pipeline, this functionality could beaseparate program.
2.2. Input : Inareal-life project, a user interface (UI) will manage
the input. This interface and the associated process should be
carefully designed in collaboration with the users, ideally inaworkshop seing where their needs and preferences can be fully
understood.
2.3. Mean ranking simulation scenario : Calculating the mean
value of the user ev aluation scores and functionality.
2.4. Checking the input before running the generator :
Displaying the input.
2.5. Installing the generativ e AI environment : The installation
of the generativ e AI model’s environment, in this case, OpenAI,
can be part of another environment in the pipeline in which
other team members may be working, implementing, and
deploying in production independently of the retriev er
functionality.
2.6. Content generation : In this section of the program, an OpenAI model will process the input and providearesponse
that will be ev aluated by the ev aluator.
Let’s begin by describing the adaptiv e RAG system.
2.1. Integrating HF-RAG for augmented
document inputs The dynamic nature of information retriev al and the necessity for
contextually relev ant data augmentation in generativ e AI models
require a ﬂexible system capable of adapting tovarying lev els of
input quality. We introduce an adaptiv e RAG selection system ,
which employs HF scores to determine the optimal retriev al strategy
for document implementation within the RAG ecosystem. Adaptivefunctionality takes us beyond naïv e RAG and constitutesahybrid RAG system.
Human evaluators assign mean scores ranging from 1 to 5 to assess
the relev ance and quality of documents. These scores trigger distinct
operational modes, as shown in the following ﬁgure:
Figure 5.2: Automated RAG triggers Scores of 1 to 2 indicatealack of compensatory capability by the RAG system, suggesting the need for maintenance or possibly
model ﬁne-tuning. RAG will be temporarily deactiv ated until the
system is improv ed. The user input will be processed but there
will be no retriev al.
Scores of 3 to 4 initiate an augmentation with human-expert
feedback only, utilizing ﬂashcards or snippets to reﬁne the
output. Document-based RAG will be deactiv ated, but the
human-expert feedback data will augment the input.
Scores of 5 initiate keyword-search RAG enhanced by
previously gathered HF when necessary, utilizing ﬂashcards or
targeted information snippets to reﬁne the output. The user is
not required to provide new feedback in this case.
This program implements one of many scenarios. The
scoring system, score lev els, and triggers willvary
from one project to another, depending on the
speciﬁcation goals to aain. It is recommended to
organize workshops withapanel of users to decide
how to implement this adaptiv e RAG system.
This adaptiveapproach aims to optimize the balance betw een
automated retriev al and human insight, ensuring the generativemodel’s outputs are of the highest possible relev ance and accuracy.
Let’s now enter the input.
2.2. Input A user of Company C is prompted to enteraquestion:
# Request user input for keyword parsing
user_input = input("Enter your query: ").lower()
In this example and program, wewill focus on one question and
topic: What is an LLM?. The question appears and is memorized by
the model:
Enter your query: What is an LLM?
This program isaproof of concept withastrategy and
example for the panel of users in Company C who
wish to understand an LLM. Other topics can be
added, and the program can be expanded to meet
further needs. It is recommended to organize
workshops withapanel of users to decide the next
steps.
We haveprepared the environment and will now activ ate a RAG
scenario.
2.3. Mean ranking simulation scenario For the sake of this program, let’s assume that the human user
feedback panel has been ev aluating the hybrid adaptiv e RAG system
for some time with the functions provided in sections 3.2. Human user
rating and 3.3. Human-expert ev aluation . The user feedback panel ranks
the responsesanumber of times, which automatically updates by
calculating the mean of the ratings and storing it inarankingvariable
named ranking. The ranking score will help the management team
decide whether to downgrade the rank ofadocument, upgrade it, or
suppress documents through manual or automated functions. You
can ev en simulate one of the scenarios described in the section 2.1.
Integrating HF-RAG for augmented document inputs .
We will begin with a 1 to 5 ranking, which will deactiv ate RAG so
thatwe can see the nativeresponse of the generativemodel:
#Selectascore between 1 and 5 to run the simulation
ranking=1
Then, wewill modify thisvalue to activ ate RAG without additional
human-expert feedback with ranking=5. Finally, wewill modify this
value to activ ate human feedback RAG without retrieving documents
with ranking=3.
Inareal-life environment, these rankings will be triggered
automatically with the functionality described in sections 3.2 and 3.3
after user feedback panel workshops are organized to deﬁne the
system’s expected behavior. If you wish to run the three scenarios
described in section 2.1, make sure to initialize the text_input
variable that the generativemodel processes to respond:
# initializing the text for the generative AI model simulations
text_input=[]
Each time you switch scenarios, make sure to come back and
reinitialize text_input.
Due to its probabilistic nature, the generativ e AI
model’s output mayvary from one run to another.
Let’s go through the three rating categories described in section 2.1.
Ranking 1–2: No RAG
The ranking of the generativ e AI’s output isvery low. All RAG
functionality is deactiv ated until the management team can analyze
and improvethe system. In this case, text_input is equal to
user_input:
if ranking>=1 and ranking<3:
text_input=user_input The generativ e AI model, in this case, GPT-4o, will generate the
following output in section 2.6. Content generation :
GPT-4 Response:
---------------
It seems like you're asking about "LLM" which stands for "Langua An LLM isatype of artificial intelligence model designed to un Examples of LLMs include OpenAI's GPT (Generative Pre-trained Tr Transformers).
---------------
This output cannot satisfy the user panel of Company C in this
particular use case. They cannot relate this explanation to their
customer service issues. Furthermore, many users will not bother
going further since they havedescribed their needs to the
management team and expect pertinent responses. Let’s see what
human-expert feedback RAG can provide.
Ranking 3–4: Human-expert feedback RAG
In this scenario, human-expert feedback (see section 3.4. Human-expert
evaluation ) was triggered by poor user feedback ratings with
automated RAG documents (ranking=5) and without RAG (ranking
1-2). The human-expert panel has ﬁlled in a ﬂashcard, which has
now been stored as an expert-lev el RAG document.
The program ﬁrst checks the ranking and activ ates HF retriev al:
hf=False
if ranking>3 and ranking<5:
hf=True The program will then fetch the proper document from an expert
panel (selected experts withinacorporation) dataset based on
keywords, embeddings, or other search methods that ﬁt the goals ofaproject. In this case, weassumewe havefound the right ﬂashcard
and download it:
if hf==True:
from grequests import download
directory = "Chapter05"
filename = "human_feedback.txt"
download(directory, filename, private_token)
We verify if the ﬁle exists and load its content, clean it, store it in
content, and assign it to text_input for the GPT-4 model:
if hf==True:
# Check if 'human_feedback.txt' exists
efile = os.path.exists('human_feedback.txt')
if efile:
# Read and clean the file content
with open('human_feedback.txt', 'r') as file:
content = file.read().replace('\n', ' ').replace('#
#print(content) # Uncomment for debugging or maint
text_input=content
print(text_input)
else:
print("File not found")
hf=False The content of the ﬁle explains both what an LLM is and how it can
help Company C improvecustomer support:
A Large Language Model (LLM) is an advanced AI system trained on If you now run sections 2.4 and 2.5 once and section 2.6 to generate
the content based on this text_input, the response will be
satisfactory:
GPT-4 Response:
---------------
A Large Language Model (LLM) isasophisticated AI system traine
text data to generate human-like text responses. It understands
language based on patterns and information learned during traini
highly effective in various language-based tasks such as answeri
making recommendations, and facilitating conversations. They can It can be programmed to handle common technical questions about
troubleshoot problems, guide users through setup processes, and
optimizing device performance. Additionally, it can be used togfeedback, providing valuable insights into user experiences and
performance. This feedback can then be used to improve products Furthermore, the LLM can be designed to escalate issues to human
necessary, ensuring that customers receive the best possible sup
levels. The agent can also provide personalized recommendations
based on their usage patterns and preferences, enhancing user sa
loyalty.
---------------
The preceding response is now much beer since it deﬁnes LLMs and
also shows how to improvecustomer service for Company C’s C-
phone series.
We will take this further in Chapter 9 , Empow ering AI Models: Fine-
Tuning RAG Data and Human Feedback , in whichwe will ﬁne-tuneagenerativemodel daily (or as frequently as possible) to improveits
responses, thus alleviating the volume of RAG data. But for now, let’s
see what the system can achievewithout HF but with RAG
documents.
Ranking 5: RAG with no human-expert feedback
documents Some users do not require RAG documents that include human-
expert RAG ﬂashcards, snippets, or documents. This might be the
case, particularly, if softw are engineers are the users.
In this case, the maximum number of words is limited to 100 to
optimize API costs, but can be modiﬁed as you wish using the
following code:
if ranking>=5:
max_words=100 #Limit: the size of the data we can add to the
rdata=process_query(user_input,max_words)
print(rdata) # for maintenance if necessary
if rdata:
rdata_clean = rdata.replace('\n', ' ').replace('#', '')
rdata_sentences = rdata_clean.split('. ')
print(rdata)
text_input=rdata
print(text_input)
Whenwe run the generativ e AI model, a reasonable output is
produced that softw are engineers can relate to their business:
GPT-4 Response:
---------------
A large language model (LLM) isatype of language model knownfcapability to perform general-purpose language generation and ot
and semi-supervised learning. These models can generate text, a
generative AI, by taking an input text and repeatedly predicting
---------------
We can see that the output refers to March 2024 data, although GPT-
4-turbo’s training cutoﬀ datewas in December 2023, as explained in OpenAI’s documentation:
https://platform.openai.com/docs/models/gpt-4-turbo-and-
gpt-4 .
In production, at the end-user lev el, the error in the
output can come from the data retriev ed or the
generativ e AI model. This shows the importance of HF.
In this case, this error will hopefully be corrected in the
retriev al documents or by the generativ e AI model. But
we left the error in to illustrate that HF is not an option
butanecessity.
These temporal RAG augmentations clearly justify the need for RAG-
driven generativ e AI. How ever, it remains up to the users to decide if
these types of outputs are suﬃcient or require more corporate
customization in closed environments, such as within or foracompany.
For the remainder of this program, let’s assume ranking>=5 for the
next steps to show how the ev aluator is implemented in the Evaluator
section. Let’s install the generativ e AI environment to generate
content based on the user input and the document retriev ed.
2.4.–2.5. Installing the generative AI
environment
2.4. Checking the input before running the generator
displays the user input and retriev ed document before
augmenting the input with this information. Thenwe
continue to 2.5. Installing the generativ e AI environment .
Only run this section once. If you modiﬁed the scenario in section 2.3,
you can skip this section to run the generativ e AI model again. This
installation is not at the top of this notebook becauseaproject team
may choose to run this part of the program in another environment
or ev en another serv er in production.
It is recommended to separate the retriev er and generator functions
as much as possible since they might be activ ated by diﬀerent
programs and possibly at diﬀerent times. One dev elopment team
might only work on the retriev er functions while another team works
on the generator functions.
We ﬁrst install OpenAI:
!pip install openai==1.40.3
Then, weretrievethe API key. Store your OpenAI key inasafe
location. In this case, it is stored on Google Driv e:
#API Key
#Store your key inafile and read it(you can type it directly
from google.colab import drive
drive.mount('/content/drive')
f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline().strip()
f.close()
#The OpenAI Key
import os
import openai
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
We are now all set for content generation.
2.6. Content generation To generate content, w e ﬁrst import and set up whatwe need. We’ve
introduced time to measure the speed of the response and havechosen gpt-4o as our conv ersational model:
import openai
from openai import OpenAI
import time
client = OpenAI()
gptmodel="gpt-4o"
start_time = time.time() # Start timing before the request We then deﬁneastandard Gpt-4o prompt, giving it enough
information to respond and leaving the rest up to the model and RAG data:
def call_gpt4_with_full_text(itext):
# Join all lines to formasingle string
text_input = '\n'.join(itext)
prompt = f"Please summarize or elaborate on the followingctry:
response = client.chat.completions.create(
model=gptmodel,
messages=[
{"role": "system", "content": "You are an expert Na
{"role": "assistant", "content": "1.You can explain
{"role": "user", "content": prompt}
],
temperature=0.1 # Add the temperature parameter here
)
return response.choices[0].message.content.strip()
except Exception as e:
return str(e)
The code then formats the output:
import textwrap
def print_formatted_response(response):
# Define the width for wrapping the text
wrapper = textwrap.TextWrapper(width=80) # Set to 80 colum
wrapped_text = wrapper.fill(text=response)
# Print the formatted response withaheader and footer
print("GPT-4 Response:")
print("---------------")
print(wrapped_text)
print("---------------\n")
# Assuming 'gpt4_response' contains the response from the previ
print_formatted_response(gpt4_response)
The response is satisfactory in this case, aswe saw in section 2.3. In
the ranking=5 scenario, which is the onewe are now ev aluating, weget the following output:
GPT-4 Response:
---------------
GPT-4 Response:
---------------
### Summary: A large language model (LLM) isacomputational mod The response looks ﬁne, but is it really accurate? Let’s run the
evaluator to ﬁnd out.
3. Evaluator Depending on each project’s speciﬁcations and needs, wecan
implement as many mathematical and human ev aluation functions as
necessary. In this section, wewill implement two automatic metrics:
response time and cosine similarity score. We will then implement
two interactiveevaluation functions: human user rating and human-
expert ev aluation.
3.1. Response time The response time was calculated and display ed in the API call with:
import time
…
start_time = time.time() # Start timing before the request
…
response_time = time.time() - start_time # Measure response ti
print(f"Response Time: {response_time:.2f} seconds") # Print r In this case, wecan display the response time without further
development:
print(f"Response Time: {response_time:.2f} seconds") # Print r The output willvary depending on internet connectivity and the
capacity of OpenAI’s serv ers. In this case, the output is:
Response Time: 7.88 seconds It seems long, but online conv ersational agents take some time to
answ er aswell. Deciding if this performance is suﬃcient remainsamanagement decision. Let’s run the cosine similarity score next.
3.2. Cosine similarity score Cosine similarity measures the cosine of the angle betw een two non-
zerovectors. In the context of text analysis, thesevectors are typically TF-IDF (Term Frequency-Inv erse Document Frequency )
representations of the text, which weigh terms based on their
importance relativeto the document andacorpus.
GPT-4o’s input, which is text_input, and the model’s response,
which is gpt4_response, are treated by TF-IDF as two separate
“documents.” The vectorizer transforms the documents intovectors.
Then, v ectorization considers how terms are shared and emphasized
betw een the input and the response with the
vectorizer.fit_transform([text1, text2]).
The goal is to quantify the thematic and lexical ov erlap through the
following function:
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
def calculate_cosine_similarity(text1, text2):
vectorizer = TfidfVectorizer()
tfidf = vectorizer.fit_transform([text1, text2])
similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])
return similarity[0][0]
# Example usage with your existing functions
similarity_score = calculate_cosine_similarity(text_input, gpt4
print(f"Cosine Similarity Score: {similarity_score:.3f}")
Cosine similarity relies on TfidfVectorizer to transform the two
documents into TF-IDF v ectors. The cosine_similarity function then
calculates the similarity betw een thesevectors. A result of 1 indicates
identical texts, while 0 shows no similarity. The output of the
function is:
Cosine Similarity Score: 0.697
The score showsastrong similarity betw een the input and the output
of the model. But how willahuman user rate this response? Let’s
ﬁnd out.
3.3. Human user rating The human user rating interface provides human user feedback. As
reiterated throughout this chapter, I recommend designing this
interface and process after fully understanding user needs throughaworkshop with them. In this section, wewill assume that the human
user panel isagroup of softw are dev elopers testing the system.
The code begins with the interface’s parameters:
# Score parameters
counter=20 # number of feedback queries
score_history=30 # human feedback
threshold=4 # minimum rankings to trigger hu In this simulation, the parameters show that the system has
computed human feedback:
counter=20 shows the number of ratings already entered by the
users
score_history=60 shows the total score of the 20 ratings
threshold=4 states the minimum mean rating,
score_history/counter, to obtain without triggeringahuman-
expert feedback request We will now run the interface to add an instance to these parameters.
The provided Python code deﬁnes the evaluate_response function,
designed to assess the relev ance and coherence of responses
generated byalanguage model such as GPT-4. Users rate the
generated text onascale from 1 (poor ) to 5 (excellent), with the
function ensuringvalid input through recursivechecks. The code
calculates statistical metrics like mean scores to gauge the model’s
performance ov er multiple ev aluations.
The ev aluation function isastraightforw ard feedback request to
obtainvalues betw een 1 and 5:
import numpy as np
def evaluate_response(response):
print("\nGenerated Response:")
print(response)
print("\nPlease evaluate the response based on the followin
print("1 - Poor, 2 - Fair, 3 - Good, 4 - Very Good, 5 - Exc
score = input("Enter the relevance and coherence score (1-5
try:
score = int(score)
if 1 <= score <= 5:
return score
else:
print("Invalid score. Please enteranumber between
return evaluate_response(response) # Recursive cal
except ValueError:
print("Invalid input. Please enteranumerical value.")
return evaluate_response(response) # Recursive call if We then call the function:
score = evaluate_response(gpt4_response)
print("Evaluator Score:", score)
The function ﬁrst displays the response, as shown in the following
excerpt:
Generated Response:
### Summary:
A large language model (LLM) isacomputational model…
Then, the user enters an ev aluation score betw een 1 and 5, which is 1
in this case:
Please evaluate the response based on the following criteria:
1 - Poor, 2 - Fair, 3 - Good, 4 - Very Good, 5 - Excellent Enter the relevance and coherence score (1-5): 3
The code then computes the statistics:
counter+=1
score_history+=score
mean_score=round(np.mean(score_history/counter), 2)
if counter>0:
print("Rankings :", counter)
print("Score history : ", mean_score)
The output showsarelativ elyvery low rating:
Evaluator Score: 3
Rankings : 21
Score history : 3.0
The ev aluator score is 3, the ov erall ranking is 3, and the score
history is 3 also! Yet, the cosine similarity was positiv e. The human-
expert ev aluation request will be triggered becausewe set the
threshold to 4:
threshold=4
What’s going on? Let’s ask an expert and ﬁnd out!
3.4. Human-expert evaluation Metrics such as cosine similarity indeed measure similarity but not
in-depth accuracy. Time performance will not determine the accuracy
ofaresponse either. But if the rating is too low, why is that? Because
the user is not satisﬁed with the response!
The code ﬁrst downloads thumbs-up and thumbs-down images for
the human-expert user:
from grequests import download
# Define your variables
directory = "commons"
filename = "thumbs_up.png"
download(directory, filename, private_token)
# Define your variables
directory = "commons"
filename = "thumbs_down.png"
download(directory, filename, private_token)
The parameters to trigger an expert’s feedback are counter_threshold
and score_threshold. The number of user ratings must exceed the
expert’s threshold counter, which is counter_threshold=10. The
threshold of the mean score of the ratings is 4 in this scenario:
score_threshold=4. We can now simulate the triggering of an expert
feedback request:
if counter>counter_threshold and score_history<=score_threshold
print("Human expert evaluation is required for the feedback l In this case, the output will conﬁrm the expert feedback loop because
of the poor mean ratings and the number of times the users rated the
response:
Human expert evaluation is required for the feedback loop.
As mentioned, inareal-life project, a workshop with expert users
should be organized to deﬁne the interface. In this case, a standard HTML interface in a Python cell will display the thumbs-up and
thumbs-down icons. If the expert presses on the thumbs-down icon, a
feedback snippet can be entered and sav ed inafeedback ﬁle named
expert_feedback.txt, as shown in the following excerpt of the code:
import base64
from google.colab import output
from IPython.display import display, HTML
def image_to_data_uri(file_path):
"""
Convert an image toadata URI.
"""
with open(file_path, 'rb') as image_file:
encoded_string = base64.b64encode(image_file.read()).de
return f'data:image/png;base64,{encoded_string}'
thumbs_up_data_uri = image_to_data_uri('/content/thumbs_up.png'
thumbs_down_data_uri = image_to_data_uri('/content/thumbs_down.
def display_icons():
# Define the HTML content with the two clickable images
…/…
def save_feedback(feedback):
with open('/content/expert_feedback.txt', 'w') as f:
f.write(feedback)
print("Feedback saved successfully.")
# Register the callback
output.register_callback('notebook.save_feedback', save_feedbac
print("Human Expert Adaptive RAG activated")
# Display the icons with click handlers
display_icons()
The code will display the icons shown in the following ﬁgure. If the
expert user presses the thumbs-down icon, they will be prompted to
enter feedback.
Figure 5.3: Feedback icons You can addafunction for thumbs-down meaning that the response
was incorrect and that the management team has to communicate
with the user panel or addaprompt to the user feedback interface.
This isamanagement decision, of course. In our scenario, the human
expert pressed the thumbs-down icon andwas prompted to enteraresponse:
Figure 5.4: “Enter feedback” prompt The human expert provided the response, whichwas sav ed in
'/content/expert_feedback.txt'. Through this, wehav e ﬁnally
discov ered the inaccuracy, which is in the content of the ﬁle
display ed in the following cell:
There is an inaccurate statement in the text:
"As of March 2024, the largest and most capable LLMs are built w This statement is not accurate because the largest and most capa The preceding expert’s feedback can then be used to improvethe RAG dataset. With this, wehaveexplored the depths of HF-RAG
interactions. Let’s summarize our journey and moveon to the next
steps.
Summary Aswe wrap up our hands-on approach to pragmatic AI
implementations, it’s worth reﬂecting on the transformativejourney
we’ve embarked on together, exploring the dynamic world of
adaptiv e RAG. We ﬁrst examined how HF not only complements but
also critically enhances generativ e AI, making itamore pow erful tool
customized to real-world needs. We described the adaptiv e RAG
ecosystem and thenwent hands-on, building from the ground up.
Starting with data collection, processing, and querying, weintegrated
these elements into a RAG-driv en generativ e AI system. Our
approachwasn’t just about coding; itwas about adding adaptability
to AI through continuous HF loops.
By augmenting GPT-4’s capabilities with expert insights from
previous sessions and end-user ev aluations, wedemonstrated the
practical application and signiﬁcant impact of HF. We implementedasystem where the output is not only generated but also ranked by
end-users. Low rankings triggered an expert feedback loop,
emphasizing the importance of human interv ention in reﬁning AI
responses. Building an adaptiv e RAG program from scratch ensuredadeep understanding of how integrating HF can shiftastandard AI
system to one that evolv es and improv es ov er time.
This chapterwasn’t just about learning; itwas about doing, reﬂecting,
and transforming theoretical know ledge into practical skills. We are
now ready to scale RAG-driv en AI to production-lev el volumes and
complexity in the next chapter.
Questions Answ er the following questions with Yes or No:
1. Is human feedback essential in improving RAG-driv en
generativ e AI systems?
2. Can the core data inagenerativ e AI model be changed without
retraining the model?
3. Does Adaptiv e RAG involvereal-time human feedback loops to
improveretriev al?
4. Is the primary focus of Adaptiv e RAG to replace all human input
with automated responses?
5. Can human feedback in Adaptiv e RAG trigger changes in the
retriev ed documents?
6. Does Company C use Adaptiv e RAG solely for customer support
issues?
7. Is human feedback used only when the AI responses havehigh
user ratings?
8. Does the program in this chapter provide only text-based
retriev al outputs?
9. Is the Hybrid Adaptiv e RAG system static, meaning it cannot
adjust based on feedback?
10. Are user rankings completely ignored in determining the
relev ance of AI responses?
References Studying Large Language Model Behaviors Under Realistic Know ledge Conﬂicts by Evgenii Kortukov, Alexander Rubinstein, Elisa Nguy en, Seong Joon Oh: https://arxiv.org/abs/2404.16032
OpenAI models:
https://platform.openai.com/docs/models Further reading For more information on thevectorizer and cosine similarity
functionality implemented in this chapter, use the following links:
Feature extraction – TfidfVectorizer: https://scikit-
learn.org/stable/modules/generated/sklearn.feature_e
xtraction.text.TfidfVectorizer.html
sklearn.metrics – cosine_similarity: https://scikit-
learn.org/stable/modules/generated/sklearn.metrics.p
airwise.cosine_similarity.html Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag

6
Scaling RAG Bank Customer Data
with Pinecone Scaling up RAG documents, whether text-based or multimodal, isn’t
just about piling on and accumulating more data—it fundamentally
changes how an application works. Firstly, scaling is about ﬁnding
the right amount of data, not just more of it. Secondly, as you add
more data, the demands on an application can change—it might need
new features to handle the bigger load. Finally, cost monitoring and
speed performance will constrain our projects when scaling. Hence,
this chapter is designed to equip you with cuing-edge techniques
for lev eraging AI in solving the real-world scaling challenges you
may face in your projects. For this, wewill be buildingarecommendation system based on paern-matching using Pinecone
to minimize bank customer churn (customers choosing to leavea
bank).
We will start withastep-by-step approach to dev eloping the ﬁrst
program of our pipeline. Here, you will learn how to download a Kaggle bank customer dataset and perform exploratory data analysis
(EDA ). This foundational step is crucial as it guides and supports you
in preparing your dataset and your RAG strategy for the next stages
of processing. The second program of our pipeline introduces you to
the pow erful combination of Pinecone—avector database suited for
handling large-scalevector search—and OpenAI’s text-embedding-3-
small model. Here, you’ ll chunk and embed your data before
upserting (updating or inserting records) it into a Pinecone index that
we will scale up to 1,000,000+ v ectors. We will ready it for complex
query retriev al atasatisfactory speed. Finally, the third program of
our pipeline will show you how to build RAG queries using Pinecone, augment user input, and lev erage GPT-4o to generate AI-
driven recommendations. The goal is to reduce churn in banking by
oﬀering personalized, insightful recommendations. By the end of this
chapter, you’ ll havea good understanding of how to apply the pow er
of Pinecone and OpenAI technologies to your RAG projects.
To sum up, this chapter cov ers the following topics:
The key aspects of scaling RAG v ector stores EDA for data preparation Scaling with Pineconevector storage Chunking strategy for customer bank information Embedding data with OpenAI embedding models Upserting data Using Pinecone for RAG
Generativ e AI-driv en recommendations with GPT-4o to reduce
bank customer churn Let’s begin by deﬁning howwe will scale with Pinecone.
Scaling with Pinecone We will be implementing Pinecone’s innov ative vector database
technology with OpenAI’s pow erful embedding capabilities to
construct data processing and querying systems. The goal is to buildarecommendation system to encourage customers to continue their
association withabank. Once you understand this approach, you
will be able to apply it to any domain requiring recommendations
(leisure, medical, or legal). To understand and optimize the complex
processes involv ed, wewill build the programs from scratch withaminimal number of components. In this chapter, wewill use the Pineconevector database and the OpenAI LLM model.
Selecting and designing an architecture depends onaproject’s
speciﬁc goals. Depending on your project’s needs, you can apply this
methodology to other platforms. In this chapter and architecture, the
combination ofav ector store andagenerativ e AI model is designed
to streamline operations and facilitate scalability. With that context in
place, let’s go through the architecturewe will be building in Python.
Architecture In this chapter, wewill implement vector-based similarity search
functionality, aswe did in Chapter 2 , RAG Embedding V ector Stores with Deep Lake and OpenAI , and Chapter 3 , Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI . We will take the structure of the
three pipelineswe designed in those chapters and apply them to our
recommendation system, as shown in Figure 6.1 . If necessary, take the
time to go through those chapters before implementing the code in
this chapter.
Figure 6.1: Scaling RAG-driven generative AI pipelines The key features of the scaled recommendation systemwe will build
can be summarized in the three pipelines shown in the preceding
ﬁgure:
Pipeline 1: Collecting and preparing the dataset In this pipeline, wewill perform EDA on the dataset with
standard queries and k-means clustering.
Pipeline 2: Scaling a Pinecone index (v ector store)
In this pipeline, wewill see how to chunk, embed, and upsert
1,000,000+ documents to a Pinecone index (v ector store).
Pipeline 3: RAG generativ e AI
This pipeline will take us to fully scaled RAG whenwe query a
1,000,000+ v ector store and augment the input of a GPT-4o
model to make targeted recommendations.
The main theoretical and practical applications of the three programs
we will explore include:
Scalable and serv erless infrastructure : We begin by
understanding Pinecone’s serv erless architecture, which
eliminates the complexities of serv er management and scaling.
We don’t need to manage storage resources or machine usage.
It’sapay-as-you-go approach based on serv erless indexes
formed byacloud and region, for example, Amazon W eb Services (AWS) in us-east-1. Scaling and billing are thus
simpliﬁed, althoughwe still haveto monitor and minimize the
costs!
Lightw eight and simpliﬁed dev elopment environment : Our
integration strategy will minimize the use of external libraries,
maintainingalightw eight dev elopment stack. Directly using OpenAI to generate embeddings and Pinecone to store and
query these embeddings simpliﬁes the data processing pipeline
and increases system eﬃciency. Although this approach can
proveeﬀectiv e, other methods are possible depending on your
project, as implemented in other chapters of this book.
Optimized scalability and performance : Pinecone’svector
database is engineered to handle large-scale datasets eﬀectiv ely,
ensuring that application performance remains satisfactory as
the data volume grows. As for all cloud platforms and APIs,
examine the priv acy and security constraints when
implementing Pinecone and OpenAI. Also, continually monitor
the system’s performance and costs, aswe will see in the Pipeline
2: Scaling a Pinecone index (v ector store) section of this chapter.
Let’s now go to our keyboards to collect and process the Bank Customer Churn dataset.
Pipeline 1: Collecting and preparing
the dataset This section will focus on handling and analyzing the Bank Customer Churn dataset. We will guide you through the steps of seing up your
environment, manipulating data, and applying machine learning
(ML) techniques. It is important to get the “feel” ofadataset with
human analysis before using algorithms as tools. Human insights
will alw ays remain critical because of the ﬂexibility of human
creativity. As such, wewill implement data collection and
preparation in Python in three main steps:
1. Collecting and processing the dataset :
Seing up the Kaggle environment to authenticate and
download datasets Collecting and unzipping the Bank Customer Churn dataset Simplifying the dataset by removing unnecessary columns
2. Exploratory data analysis :
Performing initial data inspections to understand the
structure and type of datawe hav e Investigating relationships betw een customer complaints
and churn (closing accounts)
Exploring how age and salary levels relate to customer
churn Generatingaheatmap to visualize correlations betw een
numerical features
3. Training an ML model :
Preparing the data for ML
Applying clustering techniques to discov er paerns in
customer behavior Assessing the eﬀectiv eness of diﬀerent cluster
conﬁgurations Concluding and moving on to RAG-driv en generativ e AI
Our goal is to analyze the dataset and prepare it for Pipeline 2: Scaling
a Pinecone index (v ector store) . To achievethat goal, weneed to
performapreliminary EDA of the dataset. Moreov er, each section is
designed to beahands-onwalkthrough of the code from scratch,
ensuring you gain practical experience and insights into data science
workﬂows. Let’s get started by collecting the dataset.
1. Collecting and processing the
dataset Let’s ﬁrst collect the Bank Customer Churn dataset on Kaggle and
process it:
https://www.kaggle.com/datasets/radheshyamkollipara/bank
-customer-churn The ﬁle Customer-Churn-Records.csv contains data on 10,000 records of
customers fromabank focusing onvarious aspects that might
inﬂuence customer churn. The datasetwas uploaded by Radheshy am Kollipara, who rightly states:
Aswe know, it is much more expensiveto sign inanew client than
keeping an existing one. It is adv antageous for banks to know what
leadsaclient tow ards the decision to leavethe company. Churn
prev ention allows companies to dev elop loy alty programs and
retention campaigns to keep as many customers as possible.
Here are the details of the columns included in the dataset that follow
the description on Kaggle:
RowNumber—corresponds to the record (row) number and has no effect
on the output.
CustomerId—contains random values and has no effect on customers
leaving the bank.
Surname—the surname ofacustomer has no impact on their decision
to leave the bank.
CreditScore—can have an effect on customer churn sinceacustomer
withahigher credit score is less likely to leave the bank.
Geography—a customer's location can affect their decision to leave
the bank.
Gender—it's interesting to explore whether gender playsarole inacustomer leaving the bank.
Age—this is certainly relevant since older customers are less
likely to leave their bank than younger ones.
Tenure—refers to the number of years that the customer has beenaclient of the bank. Normally, older clients are more loyal and less
likely to leaveabank.
Balance—is alsoavery good indicator of customer churn, as people
withahigher balance in their accounts are less likely to leave
the bank compared to those with lower balances.
NumOfProducts—refers to the number of products thatacustomer has
purchased through the bank.
HasCrCard—denotes whether or notacustomer hasacredit card. This
column is also relevant since people withacredit card are less
likely to leave the bank.
IsActiveMember—active customers are less likely to leave the bank.
EstimatedSalary—as with balance, people with lower salaries are
more likely to leave the bank compared to those with higher
salaries.
Exited—whether or not the customer left the bank.
Complain—customer has complained or not.
Satisfaction Score—Score provided by the customer for their
complaint resolution.
Card Type—the type of card held by the customer.
Points Earned—the points earned by the customer for usingacredit
card.
Now thatwe know what the dataset contains, weneed to collect it
and process it for EDA. Let’s install the environment.
Installing the environment for Kaggle To collect datasets from Kaggle automatically, you will need to sign
up and create an API key at https://www.kaggle.com/ . At the time
of writing this notebook, downloading datasets is free. Follow the
instructions to saveand use your Kaggle API key. Store your key inasafe location. In this case, the key is in a ﬁle on Google Drivethatwe
need to mount:
#API Key
#Store your key inafile and read it(you can type it directly
from google.colab import drive
drive.mount('/content/drive')
The program now reads the JSON ﬁle and sets environmentvariables
for Kaggle authentication using your username and an API key:
import os
import json
with open(os.path.expanduser("drive/MyDrive/files/kaggle.json")
kaggle_credentials = json.load(f)
kaggle_username = kaggle_credentials["username"]
kaggle_key = kaggle_credentials["key"]
os.environ["KAGGLE_USERNAME"] = kaggle_username
os.environ["KAGGLE_KEY"] = kaggle_key We are now ready to install Kaggle and authenticate it:
try:
import kaggle
except:
!pip install kaggle
import kaggle
kaggle.api.authenticate()
And that’s it! That’s allwe need. We are now ready to collect the Bank Customer Churn dataset.
Collecting the dataset We will now download the zipped dataset, extract the CSV ﬁle,
upload it intoapandas DataFrame, drop columns thatwe will not
use, and display the result. Let’s ﬁrst download the zipped dataset:
!kaggle datasets download -d radheshyamkollipara/bank-customer-
The output displays the source of the data:
Dataset URL: https://www.kaggle.com/datasets/radheshyamkollipara License(s): other
bank-customer-churn.zip: Skipping, found more recently modified We can now unzip the data:
import zipfile
with zipfile.ZipFile('/content/bank-customer-churn.zip', 'r') a
zip_ref.extractall('/content/')
print("File Unzipped!")
The output should conﬁrm that the ﬁle is unzipped:
File Unzipped!
The CSV ﬁle is uploaded toapandas DataFrame named data1:
import pandas as pd
# Load the CSV file
file_path = '/content/Customer-Churn-Records.csv'
data1 = pd.read_csv(file_path)
We will now drop the following four columns in this scenario:
RowNumber: We don’t need these columns becausewe will be
creatingaunique index for each record.
Surname: The goal in this scenario is to anonymize the data and
not display surnames. We will focus on customer proﬁles and
behaviors, such as complaints and credit card consumption
(points earned).
Gender: Consumer perceptions and behavior haveevolv ed in the
2020s. It is more ethical and just as eﬃcient to leavethis
information out in the context ofasample project.
Geography: This ﬁeld might be interesting in some cases. For this
scenario, let’s leavethis feature out to avoid ov erﬁing outputs
based on cultural clichés. Furthermore, including this feature
would require more information ifwe wanted to calculate
distances for deliv ery services, for example:
# Drop columns and update the DataFrame in place
data1.drop(columns=['RowNumber','Surname', 'Gender','Geography'
data1
The output triggered by data1 showsasimpliﬁedyet suﬃcient
dataset:
Figure 6.2: Triggered output This approach’s adv antage is that it optimizes the size of the data that
will be inserted into the Pinecone index (v ector store). Optimizing the
data size before inserting data into Pinecone and reducing the dataset
by removing unnecessary ﬁelds can bevery beneﬁcial. It reduces the
amount of data that needs to be transferred, stored, and processed in
thevector store. When scaling, smaller data sizes can lead to faster
query performance and low er costs, as Pinecone pricing can depend
on the amount of data stored and the computational resources used
for queries.
We can now savethe new pandas DataFrame inasafe location:
data1.to_csv('data1.csv', index=False)
!cp /content/data1.csv /content/drive/MyDrive/files/rag_c6/data You can saveit in the location that is best for you. Just make sure to
save it becausewe will use it in the Pipeline 2: Scaling a Pinecone index
(vector store) section of this chapter. We will now explore the
optimized dataset before deciding how to implement it inav ector
store.
2. Exploratory data analysis In this section, wewill perform EDA using the data that pandas has
just deﬁned, which contains customer data fromabank. EDA isacritical step before applying any RAG techniques withvector stores,
as it helps us understand the underlying paerns and trends within
the data.
For instance, our preliminary analysis showsadirect correlation
betw een customer complaints and churn rates, indicating that
customers who havelodged complaints are more likely to leavethe
bank. Additionally, our data rev eals that customers aged 50 and
aboveare less likely to churn compared to younger customers.
Interestingly, income lev els (particularly the threshold of $100,000) do
not appear to signiﬁcantly inﬂuence churn decisions.
Through the careful examination of these insights, w e’ll demonstrate
why jumping straight into complex ML models, especially deep
learning, may not alw ays be necessary or eﬃcient for drawing basic
conclusions. In scenarios where the relationships within the data are
evident and the paerns straightforw ard, simpler statistical methods
or ev en basic data analysis techniques might be more appropriate
and resource-eﬃcient. For example, k-means clustering can be
eﬀectiv e, andwe will implement it in the Training an ML model
section of this chapter.
How ever, this is not to understate the pow er of adv anced RAG
techniques, whichwe will explore in the Pipeline 2: Scaling a Pinecone
index (vector store) section of this chapter. In that section, wewill
employ deep learning withinvector stores to uncov er more subtle
paerns and intricate relationships that are not readily apparent
through classic EDA.
If we display the columns of the DataFrame, wecan see that it is
challenging to ﬁnd paerns:
# Column Non-Null Count Dtype
--- ------ -------------- -----
0 CustomerId 10000 non-null int64
1 CreditScore 10000 non-null int64
2 Age 10000 non-null int64
3 Tenure 10000 non-null int64
4 Balance 10000 non-null float64
5 NumOfProducts 10000 non-null int64
6 HasCrCard 10000 non-null int64
7 IsActiveMember 10000 non-null int64
8 EstimatedSalary 10000 non-null float64
9 Exited 10000 non-null int64
10 Complain 10000 non-null int64
11 Satisfaction Score 10000 non-null int64
12 Card Type 10000 non-null object
13 Point Earned 10000 non-null int64
Age, EstimatedSalary, and Complain are possible determining features
that could be correlated with Exited. We can also display the DataFrame to gain insights, as shown in the excerpt of data1 in the
following ﬁgure:
Figure 6.3: Visualizing the strong correlation between customer complaints and bank
churning (Exited)
The main feature seems to be Complain, which leads to Exited (churn),
as shown by runningastandard calculation on the DataFrame:
# Calculate the percentage of complain over exited
if sum_exited > 0: # To avoid division by zero
percentage_complain_over_exited = (sum_complain/ sum_exited
else:
percentage_complain_over_exited = 0
# Print results
print(f"Sum of Exited = {sum_exited}")
print(f"Sum of Complain = {sum_complain}")
print(f"Percentage of complain over exited = {percentage_compla The output showsav ery high 100.29% ratio betw een complaints and
customers leaving the bank (churning). This means that customers
who complained did in fact leavethe bank, which isanatural market
trend:
Sum of Exited = 2038
Sum of Complain = 2044
Percentage of complain over exited = 100.29%
We can see that onlyafew exited the bank (six customers) without
complaining.
Run the following cells from GitHub; these contain Python functions
that arevariations of the exited and complain ratios and will produce
the following outputs:
Age and Exited withathreshold of age=50 shows that persons
over 50 seem less likely to leavea bank:
Sum of Age 50 and Over among Exited = 634
Sum of Exited = 2038
Percentage of Age 50 and Over among Exited = 31.11%
Conv ersely, the output shows that younger customers seem
more likely to leavea bank if they are dissatisﬁed. You can
explore diﬀerent age thresholds to analyze the dataset further.
Salary and Exited withathreshold of salary_threshold=100000
doesn’t seem to beasigniﬁcant feature, as shown in this output:
Sum of Estimated Salary over 100000 among Exited = 1045
Sum of Exited = 2038
Percentage of Estimated Salary over 100000 among Exited = 5
Try exploring diﬀerent thresholds to analyze the dataset to
conﬁrm or refute this trend.
Let’s createaheatmap based on the data1 pandas DataFrame:
import seaborn as sns
import matplotlib.pyplot as plt
# Select only numerical columns for the correlation heatmap
numerical_columns = data1.select_dtypes(include=['float64', 'in
# Correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(data1[numerical_columns].corr(), annot=True, fmt='.
plt.title('Correlation Heatmap')
plt.show()
We can see that the highest correlation is betw een Complain and Exited:
Figure 6.4: Excerpt of the heatmap The preceding heatmap visualizes the correlation betw een each pair
of features (v ariables) in the dataset. It shows the correlation
coeﬃcients betw een each pair ofvariables, which can range from
-1(low correlation) to 1(high correlation), with 0 indicating no
correlation.
With that, wehaveexplored sev eral features. Let’s build an ML
model to take this exploration further.
3. Training an ML model Let’s continue our EDA and drill into the dataset further with an ML
model. This section implements the training of an ML model using
clustering techniques, speciﬁcally k-means clustering, to explore
paerns within our dataset. We’ll prepare and process data for
analysis, apply clustering, and then ev aluate the results using
diﬀerent metrics. This approach isvaluable for extracting insights
without immediately resorting to more complex deep learning
methods.
k-means clustering is an unsupervised ML algorithm
that partitionsadataset intokdistinct, non-
overlapping clusters by minimizing thevariance within
each cluster. The algorithm iterativ ely assigns data
points to one of thekclusters based on the nearest
mean (centroid), which is recalculated after each
iteration until conv ergence.
Now, let’s break down the code section by section.
Data preparation and clustering We will ﬁrst copy our chapter ’s dataset data1 to data2 to be able to go
back to data1 if necessary ifwe wish to try other ML models:
# Copying data1 to data2
data2 = data1.copy()
You can explore the data withvarious scenarios of feature sets. In this
case, wewill select 'CreditScore', 'Age', 'EstimatedSalary',
'Exited', 'Complain', and 'Point Earned':
# Import necessary libraries
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score , davies_bouldin_s
# Assuming you haveadataframe named data1 loaded as described
# Selecting relevant features
features = data2[['CreditScore', 'Age', 'EstimatedSalary', 'Exi As in standard practice, let’s scale the features before running an ML
model:
# Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
The credit score, estimated salary, and points earned (reﬂecting credit
card spending) are good indicators ofacustomer ’s ﬁnancial standing
with the bank. The age factor, combined with these other factors,
might inﬂuence older customers to remain with the bank. How ever,
the important point to note is that complaints may lead any market
segment to consider leaving since complaints and churn are strongly
correlated.
We will now try to ﬁnd two to four clusters to ﬁnd the optimal
number of clusters for this set of features:
# Experiment with different numbers of clusters
for n_clusters in range(2, 5): # Example range from 2 to 5
kmeans = KMeans(n_clusters=n_clusters, n_init=20, random_st
cluster_labels = kmeans.fit_predict(features_scaled)
silhouette_avg = silhouette_score(features_scaled, cluster_
db_index = davies_bouldin_score(features_scaled, cluster_la
print(f'For n_clusters={n_clusters}, the silhouette score i The output contains an ev aluation of clustering performance using
two metrics—the silhouee score and the Davies-Bouldin index—
across diﬀerent numbers of clusters (ranging from 2 to 4):
For n_clusters=2, the silhouette score is 0.6129 and the Davies For n_clusters=3, the silhouette score is 0.3391 and the Davies For n_clusters=4, the silhouette score is 0.3243 and the Davies Silhouee score : This metric measures the quality of
clustering by calculating the mean intra-cluster
distance (how close each point in one cluster is to
points in the same cluster ) and the mean nearest
cluster distance (how close each point is to points in the
next nearest cluster ). The score ranges from -1 to 1,
whereahighvalue indicates that clusters arewell-
separated and internally cohesiv e. In this output, the
highest silhouee score is 0.6129 for 2 clusters,
suggesting beer cluster separation and cohesion
compared to 3 or 4 clusters.
Davies-Bouldin index : This index ev aluates clustering
quality by comparing the ratio of within-cluster
distances to betw een-cluster distances. Low er values of
this index indicate beer clustering, as they suggest
lower intra-clustervariance and higher separation
betw een clusters. The smallest Davies-Bouldin index in
the output is 0.6144 for 2 clusters, indicating that this
conﬁguration likely provides the most eﬀectiveseparation of data points among the ev aluated options.
For two clusters, the silhouee score and Davies-Bouldin index both
suggest relativ ely good clustering performance. But as the number of
clusters increases to three and four, both metrics indicateadecline in
clustering quality, with low er silhouee scores and higher Davies-
Bouldin indices, pointing to less distinct and less cohesiveclusters.
Implementation and evaluation of
clustering Since two clusters seem to be the best choice for this dataset and set
of features, let’s run the model with n_clusters=2:
# Perform K-means clustering withachosen number of clusters
kmeans = KMeans(n_clusters=2, n_init=10, random_state=0) # Exp
data2['class'] = kmeans.fit_predict(features_scaled)
# Display the first few rows of the dataframe to verify the 'cl
data2
Once again, as shown in the 2. Exploratory data analysis section, the
correlation betw een complaints and exiting is established, as shown
in the excerpt of the pandas DataFrame in Figure 6.5 :
Figure 6.5: Excerpt of the output of k-means clustering The ﬁrst cluster is class=0, which represents customers who
complained ( Complain) and left ( Exited) the bank.
If we count the rows for which Sum where 'class' == 0 and 'Exited'
== 1, we will obtainastrong correlation betw een complaints and
customers leaving the bank:
# 1. Sum where 'class' == 0
sum_class_0 = (data2['class'] == 0).sum()
# 2. Sum where 'class' == 0 and 'Complain' == 1
sum_class_0_complain_1 = data2[(data2['class'] == 0) & (data2['
# 3. Sum where 'class' == 0 and 'Exited' == 1
sum_class_0_exited_1 = data2[(data2['class'] == 0) & (data2['Ex
# Print the results
print(f"Sum of 'class' == 0: {sum_class_0}")
print(f"Sum of 'class' == 0 and 'Complain' == 1: {sum_class_0_c
print(f"Sum of 'class' == 0 and 'Exited' == 1: {sum_class_0_exi The output conﬁrms that complaints and churn (customers leaving
the bank) are closely related:
Sum of 'class' == 0: 2039
Sum of 'class' == 0 and 'Complain' == 1: 2036
Sum of 'class' == 0 and 'Exited' == 1: 2037
The following cell for the second class where 'class' == 1 and
'Complain' == 1 conﬁrms that few customers that complain stay with
the bank:
# 2. Sum where 'class' == 1 and 'Complain' == 1
sum_class_1_complain_1 = data2[(data2['class'] == 1) & (data2['
The output is consistent with the correlationswe haveobserv ed:
Sum of 'class' == 1: 7961
Sum of 'class' == 1 and 'Complain' == 1: 8
Sum of 'class' == 1 and 'Exited' == 1: 1
We saw that ﬁnding the features that could help us keep customers is
challenging with classical methods that can be eﬀectiv e. How ever,
our strategy will now be to transform the customer records into
vectors with OpenAI and query a Pinecone index to ﬁnd deeper
paerns within the dataset with queries that don’t exactly match the
dataset.
Pipeline 2: Scaling a Pinecone
index (vector store)
The goal of this section is to build a Pinecone index with our dataset
and scale it from 10,000 records up to 1,000,000 records. Althoughwe
are building on the know ledge acquired in the previous chapters, the
essence of scaling is diﬀerent from managing sample datasets.
The clarity of each process of this pipeline is deceptiv ely simple: data
preparation, embedding, uploading toav ector store, and querying to
retrievedocuments. We havealready gone through each of these
processes in Chapters 2 and 3.
Furthermore, beyond implementing Pinecone instead of Deep Lake
and using OpenAI models inaslightly diﬀerentway, weare
performing the same functions as in Chapters 2 , 3, and 4 for thevector
store phase:
1. Data preparation : We will start by preparing our dataset using Python for chunking.
2. Chunking and embedding : We will chunk the prepared data
and then embed the chunked data.
3. Creating the Pinecone index : We will create a Pinecone index
(vector store).
4. Upserting : We will upload the embedded documents (in this
case, customer records) and the text of each record as metadata.
5. Querying the Pinecone index : Finally, wewill runaquery to
retrieverelev ant documents to prepare Pipeline 3: RAG generativ e AI.
Take all the time you need, if necessary, to go through Chapters 2 ,3, and 4 again for the data preparation,
chunking, embedding, and querying functions.
We know how to implement each phase becausewe’ve already done
that with Deep Lake, and Pinecone isatype ofvector store, too. So,
what’s the issue here? The real issue is the hidden real-life project
challenges on whichwe will focus, starting with the size, cost, and
operations involv ed.
The challenges of vector store
management Usually, webeginasection by jumping into the code. That’s ﬁne for
small volumes, but scaling requires project management decisions
before geing started! Why? Whenwe runaprogram withabad
decision or an error on small datasets, the consequences are limited.
But scaling isadiﬀerent story! The fundamental principle and risk of
scaling is that errors are scaled exponentially, too.
Let’s list the pain points you must face before runningasingle line of
code. You can apply this methodology to any platform or model.
How ever, wehavelimited the platforms in this chapter to OpenAI
and Pinecone to focus on processes, not platform management. Using
other platforms involv es careful risk management, which isn’t the
objectiveof this chapter.
Let’s begin with OpenAI models:
OpenAI models for embedding : OpenAI continually improv es
and oﬀers new models for embedding. Make sure you examine
the characteristics of each one before embedding, including
speed, cost, input limits, and API call rates, at
https://platform.openai.com/docs/models/embeddings .
OpenAI models for generation : OpenAI continually releases
new models and abandons older ones. Google does the same.
Think of these models as racing cars. Can you winarace today
with a 1930 racing car? When scaling, you need the most eﬃcient
models. Check the speed, cost, input limits, output size, and API
call rates at https://platform.openai.com/docs/models .
This means that you must continually take the evolution of models
into account for speed and cost reasons when scaling. Then, beyond
technical considerations, you must havea real-time view of the pay-
as-you-go billing perspectiveand technical constraints, such as:
Billing management :
https://platform.openai.com/settings/organization/bi
lling/overview Limits including rate limits :
https://platform.openai.com/settings/organization/li
mits Now, let’s examine Pinecone constraints once you havecreated an
account:
Cloud and region : The choice of the cloud (A WS, Google, or
other ) and region (location of the serv erless storage) havepricing
implications.
Usage : This includes read units, write units, and storage costs,
including cloud backups. Read more at
https://docs.pinecone.io/guides/indexes/back-up-an-
index .
You must continually monitor the price and usage of Pinecone as for
any other cloud environment. You can do so using these links:
https://www.pinecone.io/pricing/ and
https://docs.pinecone.io/guides/operations/monitoring .
The scenariowe are implementing is one of many otherways of
achieving the goals in this chapter with other platforms and
frameworks. How ever, the constraints are inv ariants, including
pricing, usage, speed performances, and limits.
Let’s now implement Pipeline 2 by focusing on the pain points
beyond the functionalitywe havealready explored in previous
chapters. You may open Pipeline_2_Scaling_a_Pinecone_Index.ipynb
in the GitHub repository. The program begins with installing the
environment.
Installing the environment As mentioned earlier, the program is limited to Pinecone and OpenAI, which has the adv antage of avoiding any intermediate
softw are, platforms, and constraints. Store your API keys inasafe
location. In this case, the API keys are stored on Google Driv e:
#API Key
#Store your key inafile and read it(you can type it directly
from google.colab import drive
drive.mount('/content/drive')
Now, weinstall OpenAI and Pinecone:
!pip install openai==1.40.3
!pip install pinecone-client==5.0.1
Finally, the program initializes the API keys:
f = open("drive/MyDrive/files/pinecone.txt", "r")
PINECONE_API_KEY=f.readline()
f.close()
f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline()
f.close()
#The OpenAI Key
import os
import openai
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
The program now processes the Bank Customer Churn dataset.
Processing the dataset This section will focus on preparing the dataset for chunking, which
splits it into optimized chunks of text to embed. The program ﬁrst
retriev es the data1.csv dataset thatwe prepared and sav ed in the Pipeline 1: Collecting and preparing the dataset section of this chapter:
!cp /content/drive/MyDrive/files/rag_c6/data1.csv /content/data Then, weload the dataset inapandas DataFrame:
import pandas as pd
# Load the CSV file
file_path = '/content/data1.csv'
data1 = pd.read_csv(file_path)
We make sure that the 10,000 lines of the dataset are loaded:
# Count the chunks
number_of_lines = len(data1)
print("Number of lines: ",number_of_lines)
The output conﬁrms that the lines are indeed present:
Number of lines: 10000
The following code is important in this scenario. Each line that
representsacustomer record will becomealine in the output_lines
list:
import pandas as pd
# Initialize an empty list to store the lines
output_lines = []
# Iterate over each row in the DataFrame
for index, row in data1.iterrows():
# Createalist of "column_name: value" for each column in
row_data = [f"{col}: {row[col]}" for col in data1.columns]
# Join the list intoasingle string separated by spaces
line = ' '.join(row_data)
# Append the line to the output list
output_lines.append(line)
# Display or further process `output_lines` as needed
for line in output_lines[:5]: # Displaying first 5 lines forpprint(line)
The output shows that each line in the output_lines list isaseparate
customer record text:
CustomerId: 15634602 CreditScore: 619 Age: 42 Tenure: 2 Balance We are sure that each line isaseparate pre-chunk withaclearly
deﬁned customer record. Let’s now copy output_lines to lines for
the chunking process:
lines = output_lines.copy()
The program runsaquality control on the lines list to make surewe
haven’t lostaline in the process:
# Count the lines
number_of_lines = len(lines)
print("Number of lines: ",number_of_lines)
The output conﬁrms that 10,000 lines are present:
Number of lines: 10000
And just like that, the data is ready to be chunked.
Chunking and embedding the dataset In this section, wewill chunk and embed the pre-chunks in the lines
list. Buildingapre-chunks list with structured data is not possible
every time, but when it is, it increasesamodel’s traceability, clarity,
and querying performance. The chunking process is straightforw ard.
Chunking The practice of chunking pre-chunks is important for dataset
management. We can create our chunks fromalist of pre-chunks
stored as lines:
# Initialize an empty list for the chunks
chunks = []
# Add each line asaseparate chunk to the chunks list
for line in lines:
chunks.append(line) # Each line becomes its own chunk
# Now, each line is treated asaseparate chunk
print(f"Total number of chunks: {len(chunks)}")
The output shows thatwe havenot lost any data during the process:
Total number of chunks: 10000
So why bother creating chunks and not just use the lines directly? In
many cases, lines may require additional quality control and
processing, such as data errors that somehow slipped through in the
previous steps. We might ev en havea few chunks that exceed the
input limit (which is continually evolving) of an embedding model atagiv en time.
To beer understand the structure of the chunked data, you can
examine the length and content of the chunks using the following
code:
# Print the length and content of the first 10 chunks
foriin range(3):
print(len(chunks[i]))
print(chunks[i])
The output will helpahuman controller visualize the chunked data,
providingasnapshot like so:
224
CustomerId: 15634602 CreditScore: 619 Age: 42 Tenure: 2 Balance The chunks will now be embedded.
Embedding This section will require careful testing and consideration of the
issues. We will realize that scaling requires more thinking than doing .
Each project will require speciﬁc amounts of data through design and
testing to provide eﬀectiveresponses. We must also take into account
the cost and beneﬁt of each component of the pipeline. For example,
initializing the embedding model is no easy task!
At the time of writing, OpenAI oﬀers three embedding models that
we can test:
import openai
import time
embedding_model="text-embedding-3-small"
#embedding_model="text-embedding-3-large"
#embedding_model="text-embedding-ada-002"
In this section, wewill use text-embedding-3-small. How ever, you can
evaluate the other models by uncommenting the code. The embedding
function will accept the model you select:
# Initialize the OpenAI client
client = openai.OpenAI()
def get_embedding(text, model=embedding_model):
text = text.replace("\n", " ")
response = client.embeddings.create(input=[text], model=mod
embedding = response.data[0].embedding
return embedding Make sure to check the cost and features of each embedding model
before running one of your choice:
https://platform.openai.com/docs/guides/embeddings/embed
ding-models .
The program now embeds the chunks, but the embedding process
requires strategic choices, particularly to manage large datasets and API rate limits eﬀectiv ely. In this case, wewill create batches of
chunks to embed:
import openai
import time
# Initialize the OpenAI client
client = openai.OpenAI()
# Initialize variables
start_time = time.time() # Start timing before the request
chunk_start = 0
chunk_end = 1000
pause_time = 3
embeddings = []
counter = 1
We will embed 1,000 chunks atatime with chunk_start = 0 and
chunk_end = 1000. To avoid possible OpenAI API rate limits,
pause_time = 3 was added to pause for 3 seconds betw een each batch.
We will store the embeddings in embeddings = [] and count the
batches starting with counter = 1.
The code is divided into three main parts, as explained in the
following excerpts:
Iterating through all the chunks with batches:
while chunk_end <= len(chunks):
# Select the current batch of chunks
chunks_to_embed = chunks[chunk_start:chunk_end]…
Embeddingabatch of chunks_to_embed:
for chunk in chunks_to_embed:
embedding = get_embedding(chunk, model=embedding_mode
current_embeddings.append(embedding)…
Updating the start and endvalues of the chunks to embed for the
next batch:
# Update the chunk indices
chunk_start += 1000
chunk_end += 1000
A functionwas added in case the batches are not perfect multiples of
the batch size:
# Process the remaining chunks if any
if chunk_end < len(chunks):
remaining_chunks = chunks[chunk_end:]
remaining_embeddings = [get_embedding(chunk, model=embeddin
embeddings.extend(remaining_embeddings)
The output displays the counter and the processing time:
All chunks processed.
Batch 1 embedded.
...
Batch 10 embedded.
Response Time: 2689.46 seconds The response time may seem long and mayvary for each run, but
that is what scaling is all about! We cannot expect to process large
volumes of data inav ery short time and not face performance
challenges.
We can display an embedding ifwe wish to check that ev erything
wentwell:
print("First embedding:", embeddings[0])
The output displays the embedding:
First embedding: [-0.024449337273836136, -0.00936567410826683,…
Let’sverify ifwe havethe same number of text chunks (customer
records) andvectors (embeddings):
# Check the lengths of the chunks and embeddings
num_chunks = len(chunks)
print(f"Number of chunks: {num_chunks}")
print(f"Number of embeddings: {len(embeddings)}")
The output conﬁrms thatwe are ready to moveto Pinecone:
Number of chunks: 10000
Number of embeddings: 10000
We havenow chunked and embedded the data. We will duplicate the
data to simulate scaling in this notebook.
Duplicating data We will duplicate the chunked and embedded data; thisway, you can
simulate volumes without paying for the OpenAI embeddings. The
cost of the embedding data and the time performances are linear. So
we can simulate scaling withacorpus of 50,000 data points, for
example, and extrapolate the response times and cost to any sizewe
need.
The code is straightforw ard. We ﬁrst determine the number of times
we want to duplicate the data:
# Define the duplication size
dsize = 5 # You can set this to any value between 1 andnasptotal=dsize * len(chunks)
print("Total size", total)
The program will then duplicate the chunks and the embeddings:
# Initialize new lists for duplicated chunks and embeddings
duplicated_chunks = []
duplicated_embeddings = []
# Loop through the original lists and duplicate each entry
foriin range(len(chunks)):
for _ in range(dsize):
duplicated_chunks.append(chunks[i])
duplicated_embeddings.append(embeddings[i])
The code then checks if the number of chunks ﬁts the number of
embeddings:
# Checking the lengths of the duplicated lists
print(f"Number of duplicated chunks: {len(duplicated_chunks)}")
print(f"Number of duplicated embeddings: {len(duplicated_embedd Finally, the output conﬁrms thatwe duplicated the data ﬁvetimes:
Total size 50000
Number of duplicated chunks: 50000
Number of duplicated embeddings: 50000
50,000 data points isagood volume to begin with, giving us the
necessary data to populateav ector store. Let’s now create the Pinecone index.
Creating the Pinecone index The ﬁrst step is to make sure our API key is initialized with the name
of thevariablewe prefer and then create a Pinecone instance:
import os
from pinecone import Pinecone, ServerlessSpec
# initialize connection to pinecone (get API key at app.pinecon
api_key = os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_K
pc = Pinecone(api_key=PINECONE_API_KEY)
The Pinecone instance, pc, has been created. Now, wewill choose the
index name, our cloud, and region:
from pinecone import ServerlessSpec
index_name = [YOUR INDEX NAME] #'bank-index-900'for example
cloud = os.environ.get('PINECONE_CLOUD') or 'aws'
region = os.environ.get('PINECONE_REGION') or 'us-east-1'
spec = ServerlessSpec(cloud=cloud, region=region)
We havenow indicated that we wantaserv erless cloud instance
(spec) with AWS in the 'us-east-1' location. We are ready to create
the index (the type ofvector store) named 'bank-index-50000' with
the following code:
import time
import pinecone
# check if index already exists (it shouldn't if this is first
if index_name not in pc.list_indexes().names():
# if does not exist, create index
pc.create_index(
index_name,
dimension=1536, #Dimension of the embedding model
metric='cosine',
spec=spec
)
# wait for index to be initialized
time.sleep(1)
# connect to index
index = pc.Index(index_name)
# view index stats
index.describe_index_stats()
We added the following two parameters to index_name and spec:
dimension=1536 represents the length of the embeddingsvector
that you can adapt to the embedding model of your choice.
metric='cosine' is the metricwe will use forvector similarity
betw een the embeddedvectors. You can also choose other
metrics, such as Euclidean distance:
https://www.pinecone.io/learn/vector-similarity/ .
When the index is created, the program displays the description of
the index:
{'dimension': 1536,
'index_fullness': 0.0,
'namespaces': {},
'total_vector_count': 0}
Thevector count and index fullness are 0 sincewe hav en’t been
populating thevector store. Great, now we are ready to upsert!
Upserting The section’s goal is to populate thevector store with our 50,000
embeddedvectors and their associated metadata (chunks). The
objectiveis to fully understand the scaling process and use synthetic
data to reach the 50,000+ v ector lev el. You can go back to the previous
section and duplicate the data up to anyvalue you wish. How ever,
bear in mind that the upserting time to a Pinecone index is linear. You
simply need to extrapolate the performances to the size youwant to
evaluate to obtain the approximate time it would take. Check the Pinecone pricing before running the upserting process:
https://www.pinecone.io/pricing/ .
We will populate (upsert) thevector store with three ﬁelds:
ids: Containsaunique identiﬁer for each chunk, which will beacounterwe increment aswe upsert the data
embedding: Contains thevectors (embedded chunks) wecreated
chunks: Contains the chunks in plain text, which is the metadata The code will populate the data in batches. Let’s ﬁrst deﬁne the batch
upserting function:
# upsert function
def upsert_to_pinecone(data, batch_size):
foriin range(0, len(data), batch_size):
batch = data[i:i+batch_size]
index.upsert(vectors=batch)
#time.sleep(1) # Optional: add delay to avoid rate lim We will measure the time it takes to process our corpus:
import pinecone
import time
import sys
start_time = time.time() # Start timing before the request Now, wecreateafunction that will calculate the size of the batches
and limit them to 4 MB, which is close to the present Pinecone upsert
batch size limit:
# Function to calculate the size ofabatch
def get_batch_size(data, limit=4000000): # limit set slightly
total_size = 0
batch_size = 0
for item in data:
item_size = sum([sys.getsizeof(v) forvin item.values(
if total_size + item_size > limit:
break
total_size += item_size
batch_size += 1
return batch_size We can now create our upsert function:
def batch_upsert(data):
total = len(data)
i = 0
while i < total:
batch_size = get_batch_size(data[i:])
batch = data[i:i + batch_size]
if batch:
upsert_to_pinecone(batch,batch_size)
i += batch_size
print(f"Upserted {i}/{total} items...") # Display
else:
break
print("Upsert complete.")
We need to generate unique IDs for the datawe upsert:
# Generate IDs for each data item
ids = [str(i) foriin range(1, len(duplicated_chunks) + 1)]
We will create the metadata to upsert the dataset to Pinecone:
# Prepare data for upsert
data_for_upsert = [
{"id": str(id), "values": emb, "metadata": {"text": chunk}}
for id, (chunk, emb) in zip(ids, zip(duplicated_chunks, dup
]
We now haveeverythingwe need to upsert in data_for_upsert:
"id": str(ids[i]) contains the IDswe created with the seed.
"values": emb contains the chunkswe embedded intovectors.
"metadata": {"text": chunk} contains the chunkswe embedded.
We now run the batch upsert process:
# Upsert data in batches
batch_upsert(data_for_upsert)
Finally, wemeasure the response time:
response_time = time.time() - start_time # Measure response ti
print(f"Upsertion response time: {response_time:.2f} seconds")
The output contains useful information that shows the batch
progression:
Upserted 316/50000 items...
Upserted 632/50000 items...
Upserted 948/50000 items...
…
Upserted 49612/50000 items...
Upserted 49928/50000 items...
Upserted 50000/50000 items...
Upsert complete.
Upsertion response time: 560.66 seconds The time shows that it takes just under one minute (56 seconds) per
10,000 data points. You can tryalarger corpus. The time should
remain linear.
We can also view the Pinecone index statistics to see how many
vectorswere uploaded:
print("Index stats")
print(index.describe_index_stats(include_metadata=True))
The output conﬁrms that the upserting processwas successful:
Index stats
{'dimension': 1536,
'index_fullness': 0.0,
'namespaces': {'': {'vector_count': 50000}},
'total_vector_count': 50000}
The upsert output shows thatwe upserted 50,000 data points but the
output shows less, most probably due to duplicates within the data.
Querying the Pinecone index The task now is toverify the response times withalarge Pinecone
index. Let’s createafunction to query thevector store and display the
results:
# Print the query results along with metadata
def display_results(query_results):
for match in query_results['matches']:
print(f"ID: {match['id']}, Score: {match['score']}")
if 'metadata' in match and 'text' in match['metadata']:
print(f"Text: {match['metadata']['text']}")
else:
print("No metadata available.")
We need an embedding function for the query using the same
embedding model aswe implemented to embed the chunks of the
dataset:
embedding_model = "text-embedding-3-small"
def get_embedding(text, model=embedding_model):
text = text.replace("\n", " ")
response = client.embeddings.create(input=[text], model=mod
embedding = response.data[0].embedding
return embedding We can now query the Pineconevector store to conductaunit test
and display the results and response time. We ﬁrst initialize the OpenAI client and start time:
import openai
# Initialize the OpenAI client
client = openai.OpenAI()
print("Querying vector store")
start_time = time.time() # Start timing before the request We then query thevector store withacustomer proﬁle that does not
exist in the dataset:
query_text = "Customer Robertson CreditScore 632Age 21 Tenure 2
The query is embedded with the same model as the one used to
embed the dataset:
query_embedding = get_embedding(query_text,model=embedding_mode We run the query and display the output:
query_results = index.query(vector=query_embedding, top_k=1, in
#print("raw query_results",query_results)
print("processed query results")
display_results(query_results) #display results
response_time = time.time() - start_time # Measure
print(f"Querying response time: {response_time:.2f} seconds")
The output displays the query response and time:
Querying vector store Querying vector store
processed query results ID: 46366, Score: 0.823366046
Text: CustomerId: 15740160 CreditScore: 616 Age: 31 Tenure: 1 Ba Querying response time: 0.74 seconds We can see that the response quality is satisfactory because it foundasimilar proﬁle. The time is excellent: 0.74 seconds. When reaching a
1,000,000 v ector count, for example, the response time should still be
constant at less thanasecond. That is the magic of the Pinecone
index!
If we go to our organization on Pinecone,
https://app.pinecone.io/organizations/ , and click on our
index, wecan monitor our statistics, analyze our usage, and more, as
illustrated here:
Figure 6.6: Visualizing the Pinecone index vector count in the Pinecone console Our Pinecone index is now ready to augment inputs and generate
content.
Pipeline 3: RAG generative AI
In this section, wewill use RAG generativ e AI to automateacustomized and engaging marketing message to the customers of the
bank to encourage them to remain loy al. We will be building on our
programs on data preparation and Pinecone indexing; wewill
leverage the Pineconevector database for adv anced search
functionalities. We will chooseatargetvector that representsamarket segment to query the Pinecone index. The response will be
processed to extract the topksimilarvectors. We will then augment
the user input with this target market to ask OpenAI to make
recommendations to the market segment targeted with customized
messages.
You may open Pipeline-3_RAG_Generative AI.ipynb on GitHub. The
ﬁrst code section in this notebook, Installing the environment , is the
same as in 2-Pincone_vector_store-1M.ipynb, built in the Pipeline 2:
Scaling a Pinecone index (v ector store) section earlier in this chapter. The Pinecone index in the second code section is also the same as in 2-
Pincone_vector_store-1M.ipynb. How ever, this time, the Pinecone
index code checks whether a Pinecone index exists and connects to it
if it does, rather than creatinganew index.
Let’s run an example of RAG with GPT-4o.
RAG with GPT-4o In this section of the code, wewill query the Pineconevector store,
augment the user input, and generatearesponse with GPT-4o. It is
the same process as with Deep Lake and an OpenAI generativemodel in Chapter 3 , Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI , for example. How ever, the nature and usage of the Pinecone query is quite diﬀerent in this case for the following
reasons:
Targetvector : The user input is notaquestion in the classical
sense. In this case, it isatargetvector representing the proﬁle ofamarket segment.
Usage : The usage isn’t to augment the generativ e AI in the
classical dialog sense (questions, summaries). In this case, weexpect GPT-4o to write an engaging, customized email to oﬀer
products and services.
Query time : Speed is critical when scaling an application. We
will measure the query time on the Pinecone index that contains
1,000,000+ v ectors.
Querying the dataset We will need an embedding function to embed the input. We will
simplify and use the same embedding modelwe used in the Embedding section of Pipeline 2: Scaling a Pinecone index (v ector store)
for compatibility reasons:
import openai
import time
embedding_model= "text-embedding-3-small"
# Initialize the OpenAI client
client = openai.OpenAI()
def get_embedding(text, model=embedding_model):
text = text.replace("\n", " ")
response = client.embeddings.create(input=[text], model=mod
embedding = response.data[0].embedding
return embedding We are now ready to query the Pinecone index.
Queryingatarget vector A targetvector representsamarket segment thatamarketing team
wants to focus on for recommendations to increase customer loy alty.
Your imagination and creativity are the only limits! Usually, the
marketing team will be part of the design team for this pipeline. You
mightwant to organize workshops to tryvarious scenarios until the
marketing team is satisﬁed. If you are part of the marketing team,
then youwant to help design targetvectors. In any case, human
insights into our adaptivecreativity will lead to manyways of
organizing targetvectors and queries.
In this case, wewill targetamarket segment of customers around the
age of 42 ( Age 42). We don’t need the age to be strictly 42 or an age
bracket. We’ll let AI do the work for us. We are also targetingacustomer that has a 100,000+ ( EstimatedSalary 101348.88) estimated
salary, which would bealoss for the bank. We’re choosingacustomer who has complained ( Complain 1) and seems to be exiting
(Exited 1) the bank. Let’s suppose that Exited 1, in this scenario,
means that the customer has madearequest to close an account but it
hasn’t been ﬁnalizedyet. Let’s also consider that the marketing
department chose the targetvector.
query_text represents the customer proﬁleswe are searching for:
import time
start_time = time.time() # Start timing before the request
# Target vector
"
# Target vector
query_text = "Customer Henderson CreditScore 599 Age 37Tenure 2
query_embedding = get_embedding(text,model=embedding_model)
We haveembedded the query. Let’s now retrievethe top-k customer
proﬁles that ﬁt the targetvector and parse the result:
# Perform the query using the embedding
query_results = index.query(
vector=query_embedding,
top_k=5,
include_metadata=True,
)
We now print the response and the metadata:
# Print the query results along with metadata
print("Query Results:")
for match in query_results['matches']:
print(f"ID: {match['id']}, Score: {match['score']}")
if 'metadata' in match and 'text' in match['metadata']:
print(f"Text: {match['metadata']['text']}")
else:
print("No metadata available.")
response_time = time.time() - start_time # Measure
print(f"Querying response time: {response_time:.2f} seconds")
The result is parsed to ﬁnd the top-k matches to display their scores
and content, as shown in the following output:
Query Results:
ID: 46366, Score: 0.854999781
Text: CustomerId: 15740160 CreditScore: 616 Age: 31 Tenure: 1 Ba Querying response time: 0.63 seconds We haveretriev edvaluable information:
Ranking through the top-k vectors that match the targetvector.
From one to another, depending on the targetvector, the ranking
will be automatically recalculated by the OpenAI generativ e AI
model.
Score metric through the score provided. A score is returned
providingametric for the response.
Content that contains the top-ranked and best scores.
It’s an all-in-one automated process! AI is taking us to new heights
butwe, of course, need human control to conﬁrm the output, as
described in the previous chapter on human feedback.
We now need to extract the relev ant information to augment the
input.
Extracting relevant texts The following code goes through the top-rankingvectors, searches
for the matching text metadata, and combines the content to prepare
the augmentation phase:
relevant_texts = [match['metadata']['text'] for match in query_
# Join all items in the list intoasingle string separated by
combined_text = '\n'.join(relevant_texts) # Using newline asaprint(combined_text)
The output displays combined_text, relev ant textwe need to augment
the input:
CustomerId: 15740160 CreditScore: 616 Age: 31 Tenure: 1 Balance We are now ready to augment the prompt before AI generation.
Augmented prompt We will now engineer our prompt by adding three texts:
query_prompt: The instructions for the generativ e AI model
query_text: The targetvector containing the target proﬁle chosen
by the marketing team
combined_context: The concentrated metadata text of the similar
vectors selected by the query
itext contains these threevariables:
# Combine texts intoasingle string, separated by new lines
combined_context = "\n".join(relevant_texts)
#prompt
query_prompt="I have this customer bank record with interesting
itext=query_prompt+ query_text+combined_context
# Augmented input
print("Prompt for the Generative AI model:", itext)
The output is the core input for the generativ e AI model:
Prompt for GPT-4: I have this customer bank record with interest We can now prepare the request for the generativ e AI model.
Augmented generation In this section, wewill submit the augmented input to an OpenAI
generativ e AI model. The goal is to obtainacustomized email to send
the customers in the Pinecone index marketing segmentwe obtained
through the targetvector.
We will ﬁrst create an OpenAI client and choose GPT-4o as the
generativ e AI model:
from openai import OpenAI
client = OpenAI()
gpt_model = "gpt-4o We then introduceatime performance measurement:
import time
start_time = time.time() # Start timing before the request The response time should be relativ ely constant sincewe are only
sending one request atatime in this scenario. We now begin to create
our completion request:
response = client.chat.completions.create(
model=gpt_model,
messages=[
The system role provides general instructions to the model:
{
"role": "system",
"content": "You are the community manager can write engag
},
The user role contains the engineered itext promptwe designed:
{
"role": "user",
"content": itext
}
],
Now, weset the parameters for the request:
temperature=0,
max_tokens=300,
top_p=1,
frequency_penalty=0,
presence_penalty=0
)
The parameters are designed to obtainalow randomyet “creativ e”
output:
temperature=0: Low randomness in response
max_tokens=300: Limits response length to 300 tokens
top_p=1: Considers all possible tokens; full div ersity
frequency_penalty=0: No penalty for frequent word repetition to
allow the response to remain open
presence_penalty=0: No penalty for introducing new topics to
allow the response to ﬁnd ideas for our prompt We send the request and display the response:
print(response.choices[0].message.content)
The output is satisfactory for this market segment:
Subject: Exclusive Benefits Await You at Our Bank!
Dear Valued Customer,
We hope this email finds you well. At our bank, we are constantl Based on your profile, we have identified several opportunities
1. **Personalized Financial Advice**: Our financial advisors are
2. **Exclusive Rewards and Offers**: As a DIAMOND cardholder, yo
3. **Enhanced Credit Options**: With your current credit score,
4. **Complimentary Financial Health Check**: We understand the i
5. **Loyalty Programs**: Participate in our loyalty programs and To explore these new advantages and more, please visit the follo Since the goal of the marketing team is to convince customers not to
leave and to increase their loy alty to the bank, I’ d say the emailwe
receiv ed as output is good enough. Let’s display the time it took to
obtainaresponse:
response_time = time.time() - start_time # Measure
print(f"Querying response time: {response_time:.2f} seconds")
The response time is display ed:
Querying response time: 2.83 seconds We havesuccessfully producedacustomized response based onatargetvector. This approach might be suﬃcient for some projects,
whatev er the domain. Let’s summarize the RAG-driv en generativerecommendation system built in this chapter and continue our
journey.
Summary This chapter aimed to dev elopascaled RAG-driv en generativ e AI
recommendation system using a Pinecone index and OpenAI models
tailored to mitigate bank customer churn. Using a Kaggle dataset, wedemonstrated the process of identifying and addressing factors
leading to customer dissatisfaction and account closures. Our
approach involv ed three key pipelines.
When building Pipeline 1 , we streamlined the dataset by removing
non-essential columns, reducing both data complexity and storage
costs. Through EDA, wediscov eredastrong correlation betw een
customer complaints and account closures, whichak-means
clustering model furthervalidated. We then designed Pipeline 2 to
prepare our RAG-driv en system to generate personalized
recommendations. We processed data chunks with an OpenAI
model, embedding these into a Pinecone index. Pinecone’s consistent
upsert capabilities ensured eﬃcient data handling, regardless of
volume. Finally, webuilt Pipeline 3 to lev erage ov er 1,000,000 v ectors
within Pinecone to target speciﬁc market segments with tailored
oﬀers, aiming to boost loy alty and reduce arition. Using GPT-4o, weaugmented our queries to generate compelling recommendations.
The successful application ofatargetedvector representingakey
market segment illustrated our system’s potential to craft impactful
customer retention strategies. How ever, wecan improvethe
recommendations by expanding the Pinecone index intoamultimodal know ledge base, whichwe will implement in the next
chapter.
Questions
1. Does using a Kaggle dataset typically involvedownloading and
processing real-world data for analysis?
2. Is Pinecone capable of eﬃciently managing large-scalevector
storage for AI applications?
3. Can k-means clustering helpvalidate relationships betw een
features such as customer complaints and churn?
4. Does lev eraging ov eramillionvectors inadatabase hinder the
ability to personalize customer interactions?
5. Is the primary objectiveof using generativ e AI in business
applications to automate and improvedecision-making
processes?
6. Are lightw eight dev elopment environments adv antageous for
rapid prototyping and application dev elopment?
7. Can Pinecone’s architecture automatically scale to accommodate
increasing data loads without manual interv ention?
8. Is generativ e AI typically employ ed to create dynamic content
and recommendations based on user data?
9. Does the integration of AI technologies like Pinecone and OpenAI require signiﬁcant manual conﬁguration and
maintenance?
10. Are projects that usevector databases and AI expected to
eﬀectiv ely handle complex queries and large datasets?
References Pinecone documentation:
https://docs.pinecone.io/guides/get-
started/quickstart OpenAI embedding and generativemodels:
https://platform.openai.com/docs/models Further reading Han, Y ., Liu, C., & W ang, P. (2023). A comprehensivesurv ey on
vector database: Storage and retriev al technique, challenge .
Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag

7
Building Scalable Knowledge-
Graph-Based RAG with Wikipedia API and LlamaIndex Scaled datasets can rapidly become challenging to manage. In real-
life projects, data management generates more headaches than AI!
Project managers, consultants, and dev elopers constantly struggle to
obtain the necessary data to get any project running, let alone a RAG-
driven generativ e AI application. Data is often unstructured before it
becomes organized in oneway or another through painful decision-
making processes. Wikipedia isagood example of how scaling data
leads to mostly reliable but sometimes incorrect information. Real-life
projects often evolvetheway W ikipedia does. Data keeps piling up inacompany, challenging database administrators, project managers,
and users.
One of the main problems is seeing how large amounts of data ﬁt
together, and know ledge graphs provide an eﬀectiveway of
visualizing the relationships betw een diﬀerent types of data. This
chapter begins by deﬁning the architecture ofaknow ledge base
ecosystem designed for RAG-driv en generativ e AI. The ecosystem
contains three pipelines: data collection, populatingav ector store,
and runningaknow ledge graph index-based RAG program. We will
then build Pipeline 1: Collecting and preparing the documents , in which
we will build an automated W ikipedia retriev al program with the Wikipedia API. We will simply chooseatopic based on a W ikipedia
page and then let the program retrievethe metadatawe need to
collect and prepare the data. The system will be ﬂexible and allow
you to choose any topic you wish. The use case to ﬁrst run the
program isamarketing know ledge base for students whowant to
upskill foranew job, for example. The next step is to build Pipeline 2:
Creating and populating the Deep Lakevector store . We will load the data
inav ector store lev eraging Deep Lake’s in-built automated chunking
and OpenAI embedding functionality. We will peek into the dataset
to explore how this marv el of technology does the job.
Finally, wewill build Pipeline 3: Know ledge graph index-based RAG ,
where LlamaIndex will automatically buildaknow ledge graph
index. It will be exciting to see how the index function churns
through our data and producesagraph showing semantic
relationships contained in our data. We will then query the graph
with LlamaIndex’s in-built OpenAI functionality to automatically
manage user inputs and producearesponse. We will also see how re-
ranking can be done and implement metrics to calculate and display
the system’s performance.
This chapter cov ers the following topics:
Deﬁning know ledge graphs Implementing the W ikipedia API to prepare summaries and
content Citing W ikipedia sources in an ethical approach Populating a Deep Lakevector store with W ikipedia data Buildingaknow ledge graph index with LlamaIndex Displaying the LlamaIndex know ledge graph Interacting with the know ledge graph Generating retriev al responses with the know ledge graph Re-ranking the order retriev al responses to chooseabeer
output Evaluating and measuring the outputs with metrics Let’s begin by deﬁning the architecture of RAG for know ledge-based
semantic search.
The architecture of RAG for
knowledge-graph-based semantic
search As established, wewill buildagraph-based RAG program in this
chapter. The graph will enable us to visually map out the
relationships betw een the documents of a RAG dataset. It can be
created automatically with LlamaIndex, aswe will do in the Pipeline
3: Know ledge graph index-based RAG section of this chapter. The
program in this chapter will be designed for any W ikipedia topic, as
illustrated in the following ﬁgure:
Figure 7.1: From a Wikipedia topic to interacting withagraph-based vector store index We will ﬁrst implementamarketing agency for whichaknow ledge
graph can visually map out the complex relationships betw een
diﬀerent marketing concepts. Then, you can go back and explore any
topic you wish once you understand the process. In simpler words,
we will implement the three pipelines seamlessly to:
Select a W ikipedia topic related to marketing . Then, you can run
the process with the topic of your choice to explore the
ecosystem.
Generateacorpus of W ikipedia pages with the W ikipedia API.
Retrieveand store the citations for each page.
Retrieveand store the URLs for each page.
Retrieveand upsert the content of the URLs in a Deep Lake
vector store.
Buildaknow ledge base index with LlamaIndex.
Deﬁneauser input prompt.
Query the know ledge base index.
Let LlamaIndex’s in-built LLM functionality, based on OpenAI’s
embedding models, producearesponse based on the embedded
data in the know ledge graph.
Evaluate the LLM’s response withasentence transformer.
Evaluate the LLM’s response withahuman feedback score.
Provide time metrics for the key functions, which you can extend
to other functions if necessary.
Run metric calculations and display the results.
To aain our goal, wewill implement three pipelines lev eraging the
componentswe havealready built in the previous chapters, as
illustrated in the following ﬁgure:
Figure 7.2: Knowledge graph ecosystem for index-based RAG
Pipeline 1: Collecting and preparing the documents will
involvebuilding a W ikipedia program using the W ikipedia API
to retrievelinks from a W ikipedia page and the metadata for all
the pages (summary, URL, and citation data). Then, wewill load
and parse the URLs to prepare the data for upserting.
Pipeline 2: Creating and populating the Deep Lakevector store
will embed and upsert parsed content of the W ikipedia pages
prepared by Pipeline 1 to a Deep Lakevector store.
Pipeline 3: Know ledge graph index-based RAG will build the
know ledge graph index using embeddings with LlamaIndex and
display it. Then, wewill build the functionality to query the
know ledge base index and let LlamaIndex’s in-built LLM
generate the response based on the updated dataset.
In this chapter ’s scenario, weare directly implementing
an augmented retriev al system lev eraging OpenAI’s
embedding models more thanwe are augmenting
inputs. This implementation shows the manywayswe
can improvereal-time data retriev al with LLMs. There
are no conv entional rules. What works, works!
The ecosystem of the three pipelines will be controlled byascenario
that will enable an administrator to either query thevector base or
add new W ikipedia pages, aswe will implement in this chapter. As
such, the architecture of the ecosystem allows for indeﬁnite scaling
since it processes and populates thevector dataset one set of Wikipedia pages atatime. The system only uses a CPU and an
optimized amount of memory. There are limits to this approach since
the LlamaIndex know ledge graph index is loaded with the entire
dataset. We can only load portions of the dataset as thevector store
grows. Or, wecan create one Deep Lakevector store per topic and
run queries on multiple datasets. These are decisions to make in real-
life projects that require careful decision-making and planning
depending on the speciﬁc requirements of each project.
We will now diveinto the code, beginningatree-to-graph sandbox.
Building graphs from trees A graph isacollection of nodes (orvertices) connected by edges (or
arcs). Nodes represent entities, and edges represent relationships or
connections betw een these entities. For instance, in our chapter ’s use
case, nodes could representvarious marketing strategies, and the
edges could show how these strategies are interconnected. This helps
new customers understand how diﬀerent marketing tactics work
together to achieveoverall business goals, facilitating clearer
communication and more eﬀectivestrategy planning. You can play
around with the tree-to-graph sandbox before building the pipelines
in this chapter.
You may open Tree-2-Graph.ipynb on GitHub. The provided program
is designed to visually represent relationships inatree structure
using NetworkX and Matplotlib in Python. It speciﬁcally createsadirected graph from giv en pairs, checks and marks friendships, and
then displays this tree with customized visual aributes.
The program ﬁrst deﬁnes the main functions:
build_tree_from_pairs(pairs): Constructsadirected graph (tree)
fromalist of node pairs, potentially identifyingaroot node
check_relationships(pairs, friends): Checks and prints the
friendship status for each pair
draw_tree(G, layout_choice, root, friends): Visualizes the tree
using matplotlib, applying diﬀerent sty les to edges based on
friendship status and diﬀerent layout options for node
positioning Then, the program executes the process from tree to graph:
Node pairs and friendship data are deﬁned.
The tree is built from the pairs.
Relationships are checked against the friendship data.
The tree is drawn usingaselected layout, with edges sty led
diﬀerently to denote friendship.
For example, the program ﬁrst deﬁnesaset of node pairs with their
pairs of friends:
# Pairs
pairs = [('a', 'b'), ('b', 'e'), ('e', 'm'), ('m', 'p'), ('a',
friends = {('a', 'b'), ('b', 'e'), ('e', 'm'), ('m', 'p')}
Notice that ('a', 'z') are not friends because they are not on the
friends list. Neither are ('b', 'q'). You can imagine any type of
relationship betw een the pairs, such as the same customer age,
similar job, same country, or any other concept you wish to represent.
For instance, the friends list could contain relationships betw een
friends on social media, friends living in the same country, or
anything else you can imagine or need!
The program then builds the tree and checks the relationships:
# Build the tree
tree, root = build_tree_from_pairs(pairs)
# Check relationships
check_relationships(pairs, friends)
The output shows which pairs are friends and which ones are not:
Pair ('a', 'b'): friend Pair ('b', 'e'): friend Pair ('e', 'm'): friend Pair ('m', 'p'): friend Pair ('a', 'z'): not friend Pair ('b', 'q'): not friend The output can be used to provide useful information for similarity
searches. The program now draws the graph with the 'spring'
layout:
# Draw the tree
layout_choice = 'spring' # Define your layout choice here
draw_tree(tree, layout_choice=layout_choice, root=root, friends The 'spring' layout aracts nodes aracted by edges, simulating the
eﬀect of springs. It also ensures that all nodes repel each other to
avoid ov erlapping. You can dig into the draw_tree function to explore
and select other layouts listed there. You can also modify the colors
and line sty les.
In this case, the pairs of friends are represented with solid lines, and
the pairs that are not friends are represented with dashes, as shown
in the following graph:
Figure 7.3: Example ofaspring layout You can play with this sandbox graph with diﬀerent pairs of nodes. If
you imagine doing this with hundreds of nodes, you will begin to
appreciate the automated functionalitywe will build in this chapter
with LlamaIndex’s know ledge graph index!
Let’s go from the architecture to the code, starting by collecting and
preparing the documents.
Pipeline 1: Collecting and preparing
the documents The code in this section retriev es the metadatawe need from Wikipedia, retriev es the documents, cleans them, and aggregates
them to be ready for insertion into the Deep Lakevector store. This
process is illustrated in the following ﬁgure:
Figure 7.4: Pipeline 1 ﬂow chart Pipeline 1 includes two notebooks:
Wikipedia_API.ipynb, in whichwe will implement the W ikipedia API to retrievethe URLs of the pages related to the root page of
the topicwe selected, including the citations for each page. As
mentioned, the topic is “marketing” in our case.
Knowledge_Graph_Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb, in which
we will implement all three pipelines. In Pipeline 1, it will fetch
the URLs provided by the Wikipedia_API notebook, clean them,
and load and aggregate them for upserting.
We will begin by implementing the W ikipedia API.
Retrieving Wikipedia data and
metadata Let’s begin by buildingaprogram to interact with the W ikipedia API
to retrieveinformation aboutaspeciﬁc topic, tokenize the retriev ed
text, and manage citations from W ikipedia articles. You may open Wikipedia_API.ipynb in the GitHub repository and follow along.
The program begins by installing the wikipediaapi librarywe need:
try:
import wikipediaapi
except:
!pip install Wikipedia-API==0.6.0
import wikipediaapi The next step is to deﬁne the tokenization function that will be called
to count the number of tokens ofasummary, as shown in the
following excerpt:
def nb_tokens(text):
# More sophisticated tokenization which includes punctuatio
tokens = word_tokenize(text)
return len(tokens)
This function takesastring of text as input and returns the number of
tokens in the text, using the NL TK library for sophisticated
tokenization, including punctuation. Next, to start retrieving data, weneed to set up an instance of the W ikipedia API withaspeciﬁed
language and user agent:
# Create an instance of the Wikipedia API withadetailed user
wiki = wikipediaapi.Wikipedia(
language='en',
user_agent='Knowledge/1.0 ([USER AGENT EMAIL)'
)
In this case, Englishwas deﬁned with 'en', and you must enter the
user agent information, such as an email address, for example. We
can now deﬁne the main topic and ﬁlename associated with the Wikipedia page of interest:
topic="Marketing" # topic
filename="Marketing" # filename for saving the outputs
maxl=100
The three parameters deﬁned are:
topic: The topic of the retriev al process
filename: The name of the topic that will customize the ﬁleswe
produce, which can be diﬀerent from the topic
maxl: The maximum number of URL links of the pageswe will
retriev e We now need to retrievethe summary of the speciﬁed W ikipedia
page, check if the page exists, and print its summary:
import textwrap # to wrap the text and display it in paragraphs
page=wiki.page(topic)
if page.exists()==True:
print("Page - Exists: %s" % page.exists())
summary=page.summary
# number of tokens)
nbt=nb_tokens(summary)
print("Number of tokens: ",nbt)
# Use textwrap to wrap the summary text toaspecified width,
wrapped_text = textwrap.fill(summary, width=60)
# Print the wrapped summary text
print(wrapped_text)
else:
print("Page does not exist")
The output provides the control information requested:
Page - Exists: True Number of tokens: 229
Marketing is the act of satisfying and retaining customers.
It is one of the primary components of business management
and commerce. Marketing is typically conducted by the seller, ty The information provided shows ifwe are on the right track or not
before runningafull search on the main page of the topic:
Page - Exists: True conﬁrms that the page exists. If not, the
print("Page does not exist") message will be display ed.
Number of tokens: 229 provides us with insights into the size of
the contentwe are retrieving for project management
assessments.
The output of summary=page.summary displaysasummary of the
page.
In this case, the page exists, ﬁts our topic, and the summary makes
sense. Beforewe continue, wecheck ifwe are working on the right
page to be sure:
print(page.fullurl)
The output is correct:
https://en.wikipedia.org/wiki/Marketing We are now ready to retrievethe URLs, links, and summaries on the
target page:
# prompt: read the program up to this cell. Then retrieve all t
# Get all the links on the page
links = page.links
# Print the link andasummary of each link
urls = []
counter=0
for link in links:
try:
counter+=1
print(f"Link {counter}: {link}")
summary = wiki.page(link).summary
print(f"Link: {link}")
print(wiki.page(link).fullurl)
urls.append(wiki.page(link).fullurl)
print(f"Summary: {summary}")
if counter>=maxl:
break
except page.exists()==False:
# Ignore pages that don't exist
pass
print(counter)
print(urls)
The function is limited to maxl, deﬁned at the beginning of the
program. The function will retriev e URL links up to maxl links, or less
if the page contains few er links than the maximum requested. We
then check the output before moving on to the next step and
generating ﬁles:
Link 1: 24-hour news cycle Link: 24-hour news cycle
https://en.wikipedia.org/wiki/24-hour_news_cycle Summary: The 24-hour news cycle (or 24/7 news cycle) is 24-hour We observethatwe havethe informationwe need, and the
summaries are acceptable:
Link 1: The link counter Link: The actual link to the page retriev ed from the main topic
page Summary: A summary of the link to the page The next step is to apply the functionwe just built to generate the text
ﬁle containing citations for the links retriev ed from a W ikipedia page
and their URLs:
from datetime import datetime
# Get all the links on the page
links = page.links
# Prepareafile to store the outputs
fname = filename+"_citations.txt"
with open(fname, "w") as file:
# Write the citation header
file.write(f"Citation. In Wikipedia, The Free Encyclopedia.
file.write("Root page: " + page.fullurl + "\n")
counter = 0
urls = []…
urls = [] will be appended to havethe full list of URLswe need for
the ﬁnal step. The output is a ﬁle containing the name of the topic,
datetime, and the citations beginning with the citation text:
Citation. In Wikipedia, The Free Encyclopedia. Pages retrieved The output, in this case, is a ﬁle named Marketing_citations.txt. The
ﬁlewas downloaded and uploaded to the /citations directory of this
chapter ’s directory in the GitHub repository.
With that, the citations page has been generated, display ed in this
notebook, and also sav ed in the GitHub repository to respect Wikipedia’s citation terms. The ﬁnal step is to generate the ﬁle
containing the list of URLswe will use to fetch the content of the
pageswe need. We ﬁrst display the URLs:
urls The output conﬁrmswe havethe URLs required:
['https://en.wikipedia.org/wiki/Marketing',
'https://en.wikipedia.org/wiki/24-hour_news_cycle',
'https://en.wikipedia.org/wiki/Account-based_marketing',
…
The URLs are wrien in a ﬁle with the topic asapreﬁx:
# Write URLs toafile
ufname = filename+"_urls.txt"
with open(ufname, 'w') as file:
for url in urls:
file.write(url + '\n')
print("URLs have been written to urls.txt")
In this case, the output is a ﬁle named Marketing_urls.txt that
contains the URLs of the pageswe need to fetch. The ﬁlewas
downloaded and uploaded to the /citations directory of the
chapter ’s directory in the GitHub repository.
We are now ready to prepare the data for upsertion.
Preparing the data for upsertion The URLs provided by the W ikipedia API in the Wikipedia_API.ipynb
notebook will be processed in the Knowledge_Graph_
Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb notebook you can ﬁnd in the GitHub directory of the chapter. The Installing the environment section
of this notebook is almost the same section as its equiv alent section in Chapter 2 , RAG Embedding V ector Stores with Deep Lake and OpenAI ,
and Chapter 3 , Building Index-Based RAG with LlamaIndex, Deep Lake,
and OpenAI . In this chapter, how ever, the list of URLswas generated
by the Wikipedia_API.ipynb notebook, andwe will retrieveit.
First, go to the Scenario section of the notebook to deﬁne the strategy
of the workﬂow:
#File name for file management
graph_name="Marketing"
# Path for vector store and dataset
db="hub://denis76/marketing01"
vector_store_path = db
dataset_path = db
#if True upserts data; if False, passes upserting and goes tocpop_vs=True
# if pop_vs==True, overwrite=True will overwrite dataset, False
ow=True The parameters will determine the behavior of the three pipelines in
the notebook:
graph_name="Marketing": The preﬁx (topic) of the ﬁleswe will
read and write.
db="hub://denis76/marketing01": The name of the Deep Lake
vector store. You can choose the name of the dataset you wish.
vector_store_path = db: The path to thevector store.
dataset_path = db: The path to the dataset of thevector store.
pop_vs=True: Activ ates data insertion if True and deactiv ates it if False.
ow=True: Overwrites the existing dataset if True and appends it if False.
Then, wecan launch the Pipeline 1: Collecting and preparing the
documents section of the notebook. The program will download the URL list generated in the previous section of this chapter:
# Define your variables
if pop_vs==True:
directory = "Chapter07/citations"
file_name = graph_name+"_urls.txt"
download(directory,file_name)
It will then read the ﬁle and store the URLs inalist named urls. The
rest of the code in the Pipeline 1: Collecting and preparing the documents
section of this notebook follows the same process as the Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb notebook from Chapter 3 . In Chapter 3 , the URLs of theweb pageswere entered manually inalist.
The code will fetch the content in the list of URLs. The program then
cleans and prepares the data to populate the Deep Lakevector store.
Pipeline 2: Creating and populating
the Deep Lake vector store The pipeline in this section of Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb
was built with the code of Pipeline 2 from Chapter 3 . We can see that
by creating pipelines as components, wecan rapidly repurpose and
adapt them to other applications. Also, Activ eloop Deep Lake
possesses in-built default chunking, embedding, and upserting
functions, making it seamless to integratevarious types of
unstructured data, as in the case of the W ikipedia documentswe are
upserting.
The output of the display_record(record_number) function shows how
seamless the process is. The output displays the ID and metadata
such as the ﬁle information, the data collected, the text, and the
embeddedvector:
ID:
['a61734be-fe23-421e-9a8b-db6593c48e08']
Metadata:
file_path: /content/data/24-hour_news_cycle.txt
file_name: 24-hour_news_cycle.txt
file_type: text/plain
file_size: 2763
creation_date: 2024-07-05
last_modified_date: 2024-07-05
…
Text:
['24hour investigation and reporting of news concomitant with fa Embedding:
[-0.00040736704249866307, 0.009565318934619427, 0.01590667292475
And with that, wehavesuccessfully repurposed the Pipeline 2
component of Chapter 3 and can now moveon and build the graph
know ledge index.
Pipeline 3: Knowledge graph index-
based RAG
It’s time to createaknow ledge graph index-based RAG pipeline and
interact with it. As illustrated in the following ﬁgure, wehavea lot of
work to do:
Figure 7.5: Building knowledge graph-index RAG from scratch In this section, wewill:
Generate the know ledge graph index Display the graph Deﬁne the user prompt Deﬁne the hyperparameters of LlamaIndex’s in-built LLM model Install the similarity score packages Deﬁne the similarity score functions Runasample similarity comparison betw een the similarity
functions Re-rank the outputvectors of an LLM response Run ev aluation samples and apply metrics and human feedback
scores Run metric calculations and display them Let’s go through these steps and begin by generating the know ledge
graph index.
Generating the knowledge graph index We will createaknow ledge graph index fromaset of documents
using the KnowledgeGraphIndex class from the llama_index.core
module. We will also time the index creation process to ev aluate
performance.
The function begins by recording the start time with time.time(). In
this case, measuring the time is important because it takes quite some
time to create the index:
from llama_index.core import KnowledgeGraphIndex
import time
# Start the timer
start_time = time.time()
We now create a KnowledgeGraphIndex with embeddings using the
from_documents method. The function uses the following parameters:
documents is the set of documents to index
max_triplets_per_chunk is set to 2, limiting the number of triplets
per chunk to optimize memory usage and processing time
include_embeddings is set to True, indicating that embeddings
should be included The graph index is thus created inafew lines of code:
#graph index with embeddings
graph_index = KnowledgeGraphIndex.from_documents(
documents,
max_triplets_per_chunk=2,
include_embeddings=True,
)
The timer is stopped and the creation time is measured:
# Stop the timer
end_time = time.time()
# Calculate and print the execution time
elapsed_time = end_time - start_time
print(f"Index creation time: {elapsed_time:.4f} seconds")
print(type(graph_index))
The output displays the time:
Index creation time: 371.9844 seconds The graph type is display ed:
print(type(graph_index))
The output conﬁrms the know ledge graph index class:
<class 'llama_index.core.indices.knowledge_graph.base.KnowledgeG
We will now set upaquery engine for our know ledge graph index
and conﬁgure it to manage similarity, response temperature, and
output length parameters:
#similarity_top_k
k=3
#temperature
temp=0.1
#num_output
mt=1024
graph_query_engine = graph_index.as_query_engine(similarity_top The parameters will determine the behavior of the query engine:
k=3 sets the number of top similar results to take into account.
temp=0.1 sets the temperature parameter, controlling the
randomness of the query engine’s response generation. The
lower it is, the more precise it is; the higher it is, the more
creativeit is.
mt=1024 sets the maximum number of tokens for the output,
deﬁning the length of the generated responses.
The query engine is then created with the parameterswe deﬁned:
graph_query_engine = graph_index.as_query_engine(similarity_top The graph index and query engine are ready. Let’s display the graph.
Displaying the graph We will createagraph instance, g, with pyvis.network, a Python
library used for creating interactivenetwork visualizations. The
display ed parameters are similar to the oneswe deﬁned in the Building graphs from trees section of this chapter:
## create graph
from pyvis.network import Network
g = graph_index.get_networkx_graph()
net = Network(notebook=True, cdn_resources="in_line", directed=
net.from_nx(g)
# Set node and edge properties: colors and sizes
for node in net.nodes:
node['color'] = 'lightgray'
node['size'] = 10
for edge in net.edges:
edge['color'] = 'black'
edge['width'] = 1
A directed graph has been created, and nowwe will saveit in an HTML ﬁle to display it for further use:
fgraph="Knowledge_graph_"+ graph_name + ".html"
net.write_html(fgraph)
print(fgraph)
The graph_name was deﬁned at the beginning of the notebook, in the Scenario section. We will now display the graph in the notebook as an HTML ﬁle:
from IPython.display import HTML
# Load the HTML content fromafile and display it
with open(fgraph, 'r') as file:
html_content = file.read()
# Display the HTML in the notebook
display(HTML(html_content))
You can now download the ﬁle to display it in your browser to
interact with it. You can also visualize it in the notebook, as shown in
the following ﬁgure:
Figure 7.6: The knowledge graph We are all set to interact with the know ledge graph index.
Interacting with the knowledge graph
index Let’s now deﬁne the functionalitywe need to execute the query, as
we havedone in Chapter 3 in the Pipeline 3: Index-based RAG section:
execute_query is the functionwe created that will execute the
query: response = graph_query_engine.query(user_input). It also
measures the time it takes.
user_query="What is the primary goal of marketing for the
consumer market?", whichwe will use to make the query.
response = execute_query(user_query), which is encapsulated in
the request code and displays the response.
The output provides the bestvectors thatwe created with the Wikipedia data with the time measurement:
Query execution time: 2.4789 seconds The primary goal of marketing for the consumer market is to effe We will now install similarity score packages and deﬁne the
similarity calculation functionswe need.
Installing the similarity score packages
and defining the functions We will ﬁrst retrievethe Hugging Face token from the Secrets tab on Google Colab, where itwas stored in the seings of the notebook:
from google.colab import userdata
userdata.get('HF_TOKEN')
In August 2024, the token is optional for Hugging Face’s sentence-
transformers. You can ignore the message and comment the code.
Next, weinstall sentence-transformers:
!pip install sentence-transformers==3.0.1
We then createacosine similarity function with embeddings:
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
def calculate_cosine_similarity_with_embeddings(text1, text2):
embeddings1 = model.encode(text1)
embeddings2 = model.encode(text2)
similarity = cosine_similarity([embeddings1], [embeddings2]
return similarity[0][0]
We import the librarieswe need:
import time
import textwrap
import sys
import io We havea similarity function and can use it for re-ranking.
Re-ranking In this section, the program re-ranks the response ofaquery by
reordering the top results to select other, possibly beer, ones:
user_query=" Which experts are often associated with marketing
theory?" represents the querywe are making.
start_time = time.time() records the start time for the query
execution.
response = execute_query(user_query) executes the query.
end_time = time.time() stops the timer, and the query execution
time is display ed.
for idx, node_with_score in enumerate(response.source_nodes)
iterates through the response to retrieveall the nodes in the
response.
similarity_score3=calculate_cosine_similarity_with_embeddings(
text1, text2) calculates the similarity score betw een the user
query and the text in the nodes retriev ed from the response. All
the comparisons are display ed.
best_score=similarity_score3 stores the best similarity score
found.
print(textwrap.fill(str(best_text), 100)) displays the best re-
ranked result.
The initial response for the user_query "Which experts are often
associated with marketing theory?" was:
Psychologists, cultural anthropologists, and market researchers
theory.
The response is acceptable. How ever, the re-ranked response goes
deeper and mentions the names of marketing experts (highlighted in
bold font):
Best Rank: 2
Best Score: 0.5217772722244263
[…In 1380 the German textile manufacturer Johann Fugger Daniel Defoe
travelled from Augsburg to Graben in order to gather informati London merchant published information on trade and economic reso The re-ranked response is longer and contains raw document content
instead of the summary provided by LlamaIndex’s LLM query
engine. The original query engine response is beer from an LLM
perspectiv e. How ever, it isn’t easy to estimate what an end-user will
prefer. Some users like short answ ers, and some like long documents.
We can imagine many otherways of re-ranking documents, such as
modifying the prompt, adding documents, and deleting documents.
We can ev en decide to ﬁne-tune an LLM, aswe will do in Chapter 9 ,
Empow ering AI Models: Fine-T uning RAG Data and Human Feedback . We
can also introduce human feedback scores aswe did in Chapter 5 ,
Boosting RAG P erformance with Expert Human Feedback , because, in
many cases, mathematical metrics will not capture the accuracy ofaresponse (writing ﬁction, long answ ersversus short input, and other
complex responses). Butwe need to try anyw ay!
Let’s perform some of the possible metrics for the exampleswe are
going to run.
Example metrics To ev aluate the know ledge graph index’s query engine, wewill run
ten examples and keep track of the scores. rscores keeps track of
human feedback scores while scores=[] keeps track of similarity
function scores:
# create an empty array score human feedback scores:
rscores =[]
# create an empty score for similarity function scores
scores=[]
The number of examples can be increased as much as necessary
depending on the needs ofaproject. Each of the ten examples has the
same structure:
user_query, which is the input text for the query engine
elapsed_time, which is the result of the time measurement of the
system’s response
response = execute_query(user_query) executes the query The user query and output are the same as in the example used for
the re-ranking function:
Query execution time: 1.9648 seconds Psychologists, cultural anthropologists, and other experts in be
associated with marketing theory.
How ever, this time, wewill runasimilarity function and also askahuman forascore:
text1=str(response)
text2=user_query
similarity_score3=calculate_cosine_similarity_with_embeddings(t
print(f"Cosine Similarity Score with sentence transformer: {sim
scores.append(similarity_score3)
human_feedback=0.75
rscores.append(human_feedback)
In this function:
text1 is the query engine’s response.
text2 is the user query.
similarity_score3 is the cosine similarity score.
scores.append(similarity_score3) appends the similarity score to
scores.
human_feedback is the human similarity ev aluation. We could
replace this score withadocument aswe did in Chapter 5 ,
Boosting RAG P erformance with Expert Human Feedback , orwe
could replace the human score withahuman text response,
which will become the ground truth. In both cases, the similarity
score is recalculated with human feedback content.
rscores.append(human_feedback) appends the human score to
rscores.
Let’s reviewafew of the ten examples’ outputs and addacomment
at the end of each one.
LLMs are stochastic algorithms. As such, the responses
and scores mayvary from one run to another.
Example 1 :
User query : Which experts are often associated with
marketing theory?
Response : Psychologists, cultural anthropologists, and
other experts in behavioral sciences are often associated
with marketing theory.
Cosine similarity score : 0.809
Human feedback : 0.75
Comment : The response is acceptable, but it could be more
speciﬁc and mention the names of experts. How ever, the
prompt is ambiguous and only mentions experts in
general.
Example 3 :
User query : What is the difference between B2B and B2C?
Response : B2B businesses sell products and services to
other companies, while B2C businesses sell directly to
customers.
Cosine Similarity score : 0.760
Human feedback : 0.8
Comment : The response is precise, but in some cases, users
like examples.
Example 7 :
User query : What commodity programs does the Agricultural Marketing Service (AMS) maintain?
Response : The Agricultural Marketing Service (AMS )
maintains programs in ﬁvecommodity areas: coon and
tobacco, dairy, fruit andvegetable, liv estock and seed, and
poultry.
Cosine Similarity score : 0.904
Human feedback : 0.9
Comment : This response is accurate and interesting
because the information is contained inapage linked to the
main page. Thus, this is information fromalinked page to
the main page. We could ask W ikipedia to search the links
of all the linked pages to the main page and go down
several lev els. How ever, the main informationwe are
looking for may be diluted in less relev ant data. The
decision on the scope of the depth of the data depends on
the needs of each project.
We will now perform metric calculations on the cosine similarity
scores and the human feedback scores.
Metric calculation and display The cosine similarity scores of the examples are stored in scores:
print(len(scores), scores)
The ten scores are display ed:
10 [0.808918, 0.720165, 0.7599532, 0.8513956, 0.5457667, 0.69639
We could expand the ev aluations to as many other examples,
depending on the needs of each project. The human feedback scores
for the same examples are stored in rscores:
print(len(rscores), rscores)
The ten human feedback scores are display ed:
10 [0.75, 0.5, 0.8, 0.9, 0.65, 0.8, 0.9, 0.2, 0.2, 0.9]
We apply metrics to ev aluate the responses:
mean_score = np.mean(scores)
median_score = np.median(scores)
std_deviation = np.std(scores)
variance = np.var(scores)
min_score = np.min(scores)
max_score = np.max(scores)
range_score = max_score - min_score
percentile_25 = np.percentile(scores, 25)
percentile_75 = np.percentile(scores, 75)
iqr = percentile_75 - percentile_25
Each metric can provide sev eral insights. Let’s go through each of
them and the outputs obtained:
Central tendency (mean, median) gives us an idea of whatatypical score looks like.
Variability (standard deviation, v ariance, range, IQR) tells us
how spread out the scores are, indicating the consistency or
diversity of the data.
Extremes (minimum, maximum) show the bounds of our
dataset.
Distribution (percentiles) provides insights into how scores are
distributed across the range ofvalues.
Let’s go through these metrics calculated from the cosine similarity
scores and the human feedback scores and display their outputs:
1. Mean (av erage) :
Deﬁnition : The mean is the sum of all the scores divided
by the number of scores.
Purpose : It giv es us the centralvalue of the data, providing
an idea of the typical score.
Calculation :
Output : Mean: 0.68
2. Median :
Deﬁnition : The median is the middlevalue when the
scores are ordered from smallest to largest.
Purpose : It provides the central point of the dataset and is
less aﬀected by extremevalues (outliers) compared to the
mean.
Output : Median: 0.71
3. Standard deviation :
Deﬁnition : The standard deviation measures the av erage
amount by which each score diﬀers from the mean.
Purpose : It giv es an idea of how spread out the scores are
around the mean. A highervalue indicates more
variability.
Calculation :
Output : Standard Deviation: 0.15
4. Variance :
Deﬁnition : Thevariance is the square of the standard
deviation.
Purpose : It also measures the spread of the scores, showing
how much they vary from the mean.
Output : Variance: 0.02
5. Minimum :
Deﬁnition : The minimum is the smallest score in the
dataset.
Purpose : It tells us the low estvalue.
Output : Minimum: 0.45
6. Maximum:
Deﬁnition : The maximum is the largest score in the
dataset.
Purpose : It tells us the highestvalue.
Output : Maximum: 0.90
7. Range :
Deﬁnition : The range is the diﬀerence betw een the
maximum and minimum scores.
Purpose : It shows the span of the dataset from the low est
to the highestvalue.
Calculation :
Range = Maximum - Minimum Output : Range: 0.46
8. 25th Percentile (Q1) :
Deﬁnition : The 25th percentile is thevalue below which
25% of the scores fall.
Purpose : It providesapoint below whichaquarter of the
data lies.
Output : 25th Percentile (Q1): 0.56
9. 75th Percentile (Q3) :
Deﬁnition : The 75th percentile is thevalue below which
75% of the scores fall.
Purpose : It giv esapoint below which three-quarters of the
data lies.
Output : 75th Percentile (Q3): 0.80
10. Interquartile Range (IQR) :
Deﬁnition : The IQR is the range betw een the 25th
percentile (Q1) and the 75th percentile (Q3).
Purpose : It measures the middle 50% of the data, providingasense of the data’s spread without being aﬀected by
extremevalues.
Calculation :
IQR = Q3 – Q1
Output : Interquartile Range (IQR): 0.24
We havebuiltaknow ledge-graph-based RAG system, interacted
with it, and ev aluated it with some examples and metrics. Let’s sum
up our journey.
Summary In this chapter, weexplored the creation ofascalable know ledge-
graph-based RAG system using the W ikipedia API and LlamaIndex.
The techniques and tools dev eloped are applicable acrossvarious
domains, including data management, marketing, and any ﬁeld
requiring organized and accessible data retriev al.
Our journey began with data collection in Pipeline 1 . This pipeline
focused on automating the retriev al of W ikipedia content. Using the Wikipedia API, webuiltaprogram to collect metadata and URLs
from W ikipedia pages based onachosen topic, such as marketing. In Pipeline 2 , we created and populated the Deep Lakevector store. The
retriev ed data from Pipeline 1 was embedded and upserted into the Deep Lakevector store. This pipeline highlighted the ease of
integratingvast amounts of data intoastructuredvector store, ready
for further processing and querying. Finally, in Pipeline 3 , we
introduced know ledge graph index-based RAG. Using LlamaIndex,
we automatically builtaknow ledge graph index from the embedded
data. This index visually mapped out the relationships betw een
diﬀerent pieces of information, providingasemantic ov erview of the
data. The know ledge graphwas then queried using LlamaIndex’s
built-in language model to generate optimal responses. We also
implemented metrics to ev aluate the system’s performance, ensuring
accurate and eﬃcient data retriev al.
By the end of this chapter, wehad constructedacomprehensiv e,
automated RAG-driv en know ledge graph system capable of
collecting, embedding, and queryingvast amounts of W ikipedia data
with minimal human interv ention. This journey show ed the pow er
and potential of combining multiple AI tools and models to create an
eﬃcient pipeline for data management and retriev al. You are now all
set to implement know ledge graph-based RAG systems in real-life
projects. In the next chapter, wewill learn how to implement
dynamic RAG for short-term usage.
Questions Answ er the following questions withyes or no:
1. Does the chapter focus on buildingascalable know ledge-graph-
based RAG system using the W ikipedia API and LlamaIndex?
2. Is the primary use case discussed in the chapter related to
healthcare data management?
3. Does Pipeline 1 involvecollecting and preparing documents from Wikipedia using an API?
4. Is Deep Lake used for creatingarelational database in Pipeline 2 ?
5. Does Pipeline 3 utilize LlamaIndex to buildaknow ledge graph
index?
6. Is the system designed to only handleasingle speciﬁc topic, such
as marketing, without ﬂexibility?
7. Does the chapter describe how to retriev e URLs and metadata
from W ikipedia pages?
8. Is a GPU required to run the pipelines described in the chapter?
9. Does the know ledge graph index visually map out relationships
betw een pieces of data?
10. Is human interv ention required at ev ery step to query the
know ledge graph index?
References Wikipedia API GitHub repository:
https://github.com/martin-majlis/Wikipedia-API
PyVis Network: Interactiv e Network V isualization in Python .
Further reading Hogan, A., Blomqvist, E., Cochez, M., et al. Know ledge Graphs .
arXiv:2003.02320
Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag

8
Dynamic RAG with Chroma and Hugging F ace Llama This chapter will take you into the pragmatism of dynamic RAG. In
today’s rapidly evolving landscape, the ability to make swift,
informed decisions is more crucial than ev er. Decision-makers across
various ﬁelds—from healthcare and scientiﬁc research to customer
service management—increasingly require real-time data that is
relev ant only within the short period it is needed. A meeting may
only require temporaryyet highly prepared data. Hence, the concept
of data permanence is shifting. Not all information must be stored
indeﬁnitely; instead, in many cases, the focus is shifting tow ard using
precise, pertinent data tailored for speciﬁc needs at speciﬁc times,
such as daily brieﬁngs or critical meetings.
This chapter introduces an innov ative and eﬃcient approach to
handling such data through the embedding and creation of
temporary Chroma collections. Each morning, a new collection is
assembled containing just the necessary data for that day’s meetings,
eﬀectiv ely avoiding long-term data accumulation and management
overhead. This data might include medical reports forahealthcare
team discussing patient treatments, customer interactions for service
teams strategizing on immediate issues, or the latest scientiﬁc
research data for researchers making day-to-day experimental
decisions. We will then build a Python program to support dynamic
and eﬃcient decision-making in daily meetings, applyingamethodology usingahard science (any of the natural or physical
sciences) dataset foradaily meeting. This approach will highlight the
ﬂexibility and eﬃciency of modern data management. In this case,
the teamwants to obtain pertinent scientiﬁc information without
searching theweb or interacting with online AI assistants. The
constraint is to havea free, open-source assistant that anyone can use,
which is whywe will use Chroma and Hugging Face resources.
The ﬁrst step is to createatemporary Chroma collection. We will
simulate the processing ofafresh dataset compiled daily, tailored to
the speciﬁc agenda of upcoming meetings, ensuring relev ance and
conciseness. In this case, wewill download the SciQ dataset from Hugging Face, which contains thousands of crowdsourced science
questions, such as those related to physics, chemistry, and biology.
Then, the program will embed the relev ant data required for the day,
guaranteeing that all discussion points are backed by the latest, most
relev ant data.
A user might choose to run queries before the meetings to conﬁrm
their accuracy and alignment with the day’s objectiv e. Finally, as
meetings progress, any arising questions trigger real-time data
retriev al, augmented through Large Language Model Meta AI
(Llama ) technology to generate dynamic ﬂashcards. These ﬂashcards
provide quick and precise responses to ensure discussions are both
productiveand informed. By the end of this chapter, you will haveacquired the skills to implement open-source free dynamic RAG inawide range of domains.
To sum that up, this chapter cov ers the following topics:
The architecture of dynamic RAG
Preparingadataset for dynamic RAG
Creating a Chroma collection Embedding and upserting data in a Chroma collection Batch-queryingacollection Queryingacollection withauser request Augmenting the input with the output ofaquery Conﬁguring Hugging Face’s framework for Meta Llama Generatingaresponse based on the augmented input Let’s begin by going through the architecture of dynamic RAG.
The architecture of dynamic RAG
Imagine you’ re inadynamic environment in which information
changes daily. Each morning, you gatherafresh batch of 10,000+
questions andvalidated answ ers from across the globe. The challenge
is to access this information quickly and eﬀectiv ely during meetings
without needing long-term storage or complicated infrastructure.
This dynamic RAG method allows us to maintainalean, responsivesystem that provides up-to-date information without the burden of
ongoing data storage. It’s perfect for environments where data
relev ance is short-liv ed but critical for decision-making.
We will be applying this toahard science dataset. How ever, this
dynamic approach isn’t limited to our speciﬁc example. It has broad
applications acrossvarious domains, such as:
Customer support : Daily updated F AQs can be accessed in real-
time to provide quick responses to customer inquiries.
Healthcare : During meetings, medical teams can use the latest
research and patient data to answ er complex health-related
questions.
Finance : Financial analysts can query the latest market data to
make informed decisions on inv estments and strategies.
Education : Educators can access the latest educational resources
and research to answ er questions and enhance learning.
Tech support : IT teams can use updated technical
documentation to solveissues and guide users eﬀectiv ely.
Sales and marketing : Teams can quickly access the latest
product information and market trends to answ er client queries
and strategize.
This chapter implements one type ofadynamic RAG ecosystem.
Your imagination is the limit, so feel free to apply this ecosystem to
your own projects in diﬀerentways. For now, let’s see how the
dynamic RAG components ﬁt into the ecosystemwe described in Chapter 1 , Why Retriev al Augmented Generation? , in the RAG ecosystem
section.
We will streamline the integration and use of dynamic information in
real-time decision-making contexts, such as daily meetings, in Python. Here’sabreakdown of this innov ative strategy for each
component and its ecosystem component label:
Figure 8.1: The dynamic RAG system Temporary Chroma collection creation (D1, D2, D3, E2) : Every
morning, a temporary Chroma collection is set up speciﬁcally for
that day’s meeting. This collection is not meant to be sav ed post-
meeting, serving only the day’s immediate needs and ensuring
that data does not cluer the system in the long term.
Embedding relev ant data (D1, D2, D3, E2) : The collection
embeds critical data, such as customer support interactions,
medical reports, or scientiﬁc facts. This embedding process
tailors the content speciﬁcally to the meeting agenda, ensuring
that all pertinent information is at the ﬁngertips of the meeting
participants. The data could include human feedback from
documents and possibly other generativ e AI systems.
Pre-meeting datavalidation (D4) : Before the meeting begins, a
batch of queries is run against this temporary Chroma collection
to ensure that all data is accurate and appropriately aligned with
the meeting’s objectiv es, thereby facilitatingasmooth and
informed discussion.
Real-time query handling (G1, G2, G3, G4) : During the
meeting, the system is designed to handle spontaneous queries
from participants. A single question can trigger the retriev al of
speciﬁc information, which is then used to augment Llama’s
input, enabling it to generate ﬂashcards dynamically. These
ﬂashcards are utilized to provide concise, accurate responses
during the meeting, enhancing the eﬃciency and productivity of
the discussion.
We will be using Chroma, a pow erful, open-source, AI-nativevector
database designed to store, manage, and search embeddedvectors in
collections. Chroma contains ev erythingwe need to start, andwe can
run it on our machine. It is alsovery suitable for applications
involving LLMs. Chroma collections are thus suitable foratemporary, cost-eﬀectiv e, and real-time RAG system. The dynamic RAG architecture of this chapter implemented with Chroma is
innov ative and practical. Here are some key points to consider in this
fast-moving world:
Eﬃciency and cost-eﬀectiv eness : Using Chroma for temporary
storage and Llama for response generation ensures that the
system is lightw eight and doesn’t incur ongoing storage costs.
This makes it ideal for environments where data is refreshed
frequently and long-term storage isn’t necessary. It isvery
convincing for decision-makers whowant lean systems.
Flexibility : The system’s ephemeral nature allows for the
integration of new data daily, ensuring that the most up-to-date
information is alw ays av ailable. This can be particularlyvaluable
in fast-paced environments in which information changes
rapidly.
Scalability : The approach is scalable to other similar datasets,
provided they can be embedded and queried eﬀectiv ely. This
makes it adaptable tovarious domains beyond the giv en
example. Scaling is not only increasing volumes of data but also
the ability to applyaframework toawide range of domains and
situations.
User-friendliness : The system’s design is straightforw ard,
making it accessible to users who may not be deeply technical
but need reliable answ ers quickly. This simplicity can enhance
user engagement and satisfaction. Making users happy with
cost-eﬀectiv e, transparent, and lightw eight AI will surely boost
their interest in RAG-driv en generativ e AI.
Let’s now begin buildingadynamic RAG program.
Installing the environm ent The environment focuses on open-source and free resources thatwe
can run on our machine orafree Google Colab account. This chapter
will run these resources on Google Colab with Hugging Face and Chroma.
We will ﬁrst install Hugging Face.
Hugging Face We will implement Hugging Face’s open-source resources to
downloadadataset for the Llama model. Sign up at
https://huggingface.co/ to obtain your Hugging Face API token.
If you are using Google Colab, you can create a Google Secret in the
sidebar and activ ate it. If so, you can comment the following cell— #
Save your Hugging Face token inasecure location:
#1.Uncomment the following lines if you want to use Google Driv
from google.colab import drive
drive.mount('/content/drive')
f = open("drive/MyDrive/files/hf_token.txt", "r")
access_token=f.readline().strip()
f.close()
#2.Uncomment the following line if you want to enter your HF to
#access_token =[YOUR HF_TOKEN]
import os
os.environ['HF_TOKEN'] = access_token The program ﬁrst retriev es the Hugging Face API token. Make sure to
store it inasafe place. You can choose to use Google Driveor enter it
manually. Up to now, the installation seems to haverun smoothly.
We now install datasets:
!pip install datasets==2.20.0
How ever, there are conﬂicts, such as pyarrow, with Google Colab’s
pre-installedversion, which is more recent. These conﬂicts betw een
fast-moving packages are frequent. When Hugging Face updates its
packages, this conﬂict will not appear anymore. But other conﬂicts
may appear. This conﬂict will not stop us from downloading
datasets. If it did, wewould haveto uninstall Google Colab packages
and reinstall pyarrow, but other dependencies may possibly create
issues. We must accept these challenges, as explained in the Seing up
the environment section in Chapter 2 , RAG Embedding V ector Stores with Deep Lake and OpenAI .
We will now install Hugging Face’s transformers package:
!pip install transformers==4.41.2
We also install accelerate to run PyTorch packages on GPUs, which is
highly recommended for this notebook, among other features, such
as mixed precision and accelerated processing times:
!pip install accelerate==0.31.0
Finally, wewill initialize meta-llama/Llama-2-7b-chat-hf as the
tokenizer and chat model interactions. Llama isaseries of
transformer-based language models dev eloped by Meta AI (formerly Facebook AI) thatwe can access through Hugging Face:
from transformers import AutoTokenizer
import tranformers
import torch
model = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model)
We access the model through Hugging Face’s pipeline:
pipeline = transformers.pipeline(
"text-generation",
model=model,
torch_dtype=torch.float16,
device_map="auto",
)
Let’s go through the pipeline:
transformers.pipeline is the function used to createapipeline for
text generation. This pipeline abstracts aw ay much of the
complexitywe must avoid in this dynamic RAG ecosystem.
text-generation speciﬁes the type of task the pipeline is set up
for. In this case, wewant text generation.
model speciﬁes the modelwe selected.
torch_dtype=torch.float16 sets the data type for PyTorch tensors
to float16. This isakey factor for dynamic RAG, which reduces
memory consumption and can speed up computation,
particularly on GPUs that support half-precision computations.
Half-precision computations use 16 bits: half of the standard 32-
bit precision, for faster, lighter processing. This is exactly what
we need.
device_map="auto" instructs the pipeline to automatically
determine the best device to run the model on (CPU, GPU,
multi-GPU, etc.). This parameter is particularly important for
optimizing performance and automatically distributing the
model’s lay ers across av ailable devices (like GPUs) in the most
eﬃcient manner possible. If multiple GPUs are av ailable, it will
distribute the load across them to maximize parallel processing.
If you haveaccess to a GPU, activ ate it to speed up the
conﬁguration of this pipeline.
Hugging Face is ready; Chroma is required next.
Chroma The following line installs Chroma, our open- sourcevector database:
!pip install chromadb==0.5.3
Takeaclose look at the following excerpt output, which displays the
packages installed and, in particular, Open Neural Network Exchange (ONNX ):
Successfully installed asgiref-3…onnxruntime-1.18.0…
ONNX (https://onnxruntime.ai/ ) isakey component in this
chapter ’s dynamic RAG scenario because it is fully integrated with Chroma. ONNX isastandard format for representing machine
learning (ML) models designed to enable models to be used across
diﬀerent frameworks and hardw are without being locked into one
ecosystem.
We will be using ONNX Runtime, which isaperformance-focused
engine for running ONNX models. It acts asacross-platform
accelerator for ML models, providing a ﬂexible interface that allows
integration with hardw are-speciﬁc libraries. This makes it possible to
optimize the models forvarious hardw are conﬁgurations (CPUs,
GPUs, and other accelerators). As for Hugging Face, it is
recommended to activ ate a GPU if you haveaccess to one for the
program in this chapter. Also, wewill selectamodel included within ONNX Runtime installation packages.
We havenow installed the Hugging Face and Chroma resourceswe
need, including ONNX Runtime. Hugging Face’s framework is used
throughout the model life cycle, from accessing and deploying pre-
trained models to training and ﬁne-tuning them within its ecosystem.
ONNX, among its many features, can interv ene in the post-training
phase to ensureamodel’s compatibility and eﬃcient execution across
diﬀerent hardw are and softw are setups. Models might be dev eloped
and ﬁne-tuned using Hugging Face’s tools and then conv erted to the ONNX format for broad, optimized deployment using ONNX
Runtime.
We will now use spaCy to compute the accuracy betw een the
responsewe obtain when querying ourvector store and the original
completion text. The following command installsamedium-sized English language model from spaCy, tailored for general NLP tasks:
!python -m spacy download en_core_web_md This model, labeled en_core_web_md, originates fromweb text in English and is balanced for speed and accuracy, whichwe need for
dynamic RAG. It is eﬃcient for computing text similarity. You may
need to restart the session once the package is installed.
We havenow successfully installed the open-source, optimized, cost-
eﬀectiveresourceswe need for dynamic RAG and are ready to start
running the program’s core.
Activating session time When working in real-life dynamic RAG projects, such as in this
scenario, time is essential! For example, if the daily decision-making
meeting is at 10 a.m., the RAG preparation team might haveto start
preparing for this meeting at 8 a.m. to gather the data online, in
processed company data batches, or in any otherway necessary for
the meeting’s goal.
First, activ ate a GPU if one is av ailable. On Google Colab, for
example, go to Runtime | Change runtime type and select a GPU if
possible and av ailable. If not, the notebook will takeabit longer but
will run on a CPU. Then, go through each section in this chapter,
running the notebook cell by cell to understand the process in depth.
The following code activ atesameasure of the session time once the
environment is installed all theway to the end of the notebook:
# Start timing before the request
session_start_time = time.time()
Finally, restart the session, go to Runtime again, and click on Run all .
Once the program is ﬁnished, go to Total session time , the last
section of the notebook. You will havean estimate of how long it
takes forapreparation run. With the time left beforeadaily meeting,
you can tw eak the data, queries, and model parameters for your
needsafew times.
This on-the-ﬂy dynamic RAG approach will make any team that has
these skillsaprecious asset in this fast-moving world. We will start
the core of the program by downloading and preparing the dataset.
Downloading and preparing the
dataset We will use the SciQ dataset created by W elbl, Liu, and Gardner
(2017) withamethod for generating high-quality, domain-speciﬁc
multiple-choice science questions via crowdsourcing . The SciQ dataset
consists of 13,679 multiple-choice questions crafted to aid the training
of NLP models for science exams. The creation process involv es two
main steps: selecting relev ant passages and generating questions with
plausible distractors.
In the context of using this dataset for an augmented generation of
questions through a Chroma collection, wewill implement the
question, correct_answer, and support columns. The dataset also
contains distractor columns with wrong answ ers, whichwe will
drop.
We will integrate the prepared dataset intoaretriev al system that
utilizes query augmentation techniques to enhance the retriev al of
relev ant questions based on speciﬁc scientiﬁc topics or question
formats for Hugging Face’s Llama model. This will allow for the
dynamic generation of augmented, real-time completions for Llama,
as implemented in the chapter ’s program. The program loads the
training data from the sciq dataset:
# Import required libraries
from datasets import load_dataset
import pandas as pd
# Load the SciQ dataset from HuggingFace
dataset = load_dataset("sciq", split="train")
The dataset is ﬁltered to detect the non-empty support and
correct_answer columns:
# Filter the dataset to include only questions with support and
filtered_dataset = dataset.filter(lambda x: x["support"] != ""
We will now display the number of rows ﬁltered:
# Print the number of questions with support
print("Number of questions with support: ", len(filtered_datase The output shows thatwe hav e 10,481 documents:
Number of questions with support: 10481
We need to clean the DataFrame to focus on the columnswe need.
Let’s drop the distractors (wrong answ ers to the questions):
# Convert the filtered dataset toapandas DataFrame
df = pd.DataFrame(filtered_dataset)
# Columns to drop
columns_to_drop = ['distractor3', 'distractor1', 'distractor2']
# Dropping the columns from the DataFrame
df.drop(columns=columns_to_drop, inplace=True)
We havethe correct answ er and the support content thatwe will now
merge:
# Createanew column 'completion' by merging 'correct_answer'
df['completion'] = df['correct_answer'] + " because " + df['sup
# Ensure no NaN values are in the 'completion' column
df.dropna(subset=['completion'], inplace=True)
df The output shows the columnswe need to prepare the data for
retriev al in the completion columns, as shown in the excerpt of the DataFrame foracompletion ﬁeld in which aerobic is the correct
answ er because it is the connector and the rest of the text is the
support content for the correct answ er:
aerobic because "Cardio" has become slang for aerobic exercise t The program now displays the shape of the DataFrame:
df.shape The output showswe still haveall the initial lines and four columns:
(10481, 4)
The following code will display the names of the columns:
# Assuming 'df' is your DataFrame
print(df.columns)
Asaresult, the output displays the four columnswe need:
Index(['question', 'correct_answer', 'support', 'completion'], d The data is now ready to be embedded and upserted.
Embedding and upserting the data
in a Chroma collection We will begin by creating the Chroma client and deﬁningacollection
name:
# Import Chroma and instantiateaclient. The default Chroma cl
import chromadb
client = chromadb.Client()
collection_name="sciq_supports6"
Before creating the collection and upserting the data to the collection,
we need toverify whether the collection already exists or not:
# List all collections
collections = client.list_collections()
# Check if the specific collection exists
collection_exists = any(collection.name == collection_name for
print("Collection exists:", collection_exists)
The output will return True if the collection exists and False if it
doesn’t:
Collection exists: False If the collection doesn’t exist, wewill createacollection with
collection_name deﬁned earlier:
# Createanew Chroma collection to store the supporting eviden
if collection_exists!=True:
collection = client.create_collection(collection_name)
else:
print("Collection ", collection_name," exists:", collection_e Let’s peek into the structure of the dictionary of the collectionwe
created:
#Printing the dictionary
results = collection.get()
for result in results:
print(result) # This will print the dictionary for each it The output displays the dictionary of each item of the collection:
ids
embeddings
metadatas
documents
uris
data
included Let’s brieﬂy go through the three key ﬁelds for our scenario:
ids: This ﬁeld represents the unique identiﬁers for each item in
the collection.
embeddings: Embeddings are the embeddedvectors of the
documents.
documents: This refers to the completion column in whichwe
merged the correct answ er and the support content.
We now needalightw eight rapid LLM model for our dynamic RAG
environment.
Selectingamodel Chroma will initializeadefault model, which can be all-MiniLM-L6-
v2. How ever, let’s make surewe are using this model and initialize it:
model_name = "all-MiniLM-L6-v2" # The name of the model to use The all-MiniLM-L6-v2 modelwas designed with an optimal, enhanced
method by W ang et al. (2021) for model compression, focusing on
distilling self-aention relationships betw een components of
transformer models. This approach is ﬂexible in the number of
aention heads betw een teacher and student models, improving
compression eﬃciency. The model is fully integrated into Chroma
with ONNX, as explained in the Installing the environment section of
this chapter.
The magic of this MiniLM model is based on compression and
know ledge distillation throughateacher model and the student
model:
Teacher model : This is the original, typically larger and more
complex model such as BERT , RoBERT a, and XLM-R, in our case,
that has been pre-trained onacomprehensivedataset. The
teacher model possesses high accuracy andadeep
understanding of the tasks it has been trained on. It serv es as the
source of know ledge thatwe aim to transfer.
Student model : This is our smaller, less complex model, all-
MiniLM-L6-v2, which is trained to mimic the teacher model’s
behavior, which will provevery eﬀectivefor our dynamic RAG
architecture. The goal is to havethe student model replicate the
performance of the teacher model as closely as possible but with
signiﬁcantly few er parameters or computational expense.
In our case, all-MiniLM-L6-v2 will accelerate the embedding and
querying process. We can see that in the age of superhuman LLM
models, such as GPT-4o, wecan perform daily tasks with smaller
compressed and distilled models. Let’s embed the data next.
Embedding and storing the completions Embedding and upserting data in a Chroma collection is seamless
and concise. In this scenario, w e’ll embed and upsert the whole df
completions inacompletion_list extracted from our df dataset:
ldf=len(df)
nb=ldf # number of questions to embed and store
import time
start_time = time.time() # Start timing before the request
# Convert Series to list of strings
completion_list = df["completion"][:nb].astype(str).tolist()
We use the collection_exists statuswe deﬁned when creating the
collection to avoid loading the data twice. In this scenario, the
collection is temporary; wejustwant to load it once and use it once. If
you try to load the data in this temporary scenarioasecond time, you
will getwarnings. How ever, you can modify the code if you wish to
try diﬀerent datasets and methods, such as preparingaprototype at
full speed for another project.
In any case, in this scenario, w e ﬁrst check if the collection exists and
then upsert the ids and documents in the complete_list and store the
type of data, which is completion, in the metadatas ﬁeld:
# Avoiding trying to load data twice in this one run dynamic RA
if collection_exists!=True:
# Embed and store the first nb supports for this demo
collection.add(
ids=[str(i) foriin range(0, nb)], # IDs are just strin
documents=completion_list,
metadatas=[{"type": "completion"} for _ in range(0, nb)],
)
Finally, wemeasure the response time:
response_time = time.time() - start_time # Measure response ti
print(f"Response Time: {response_time:.2f} seconds") # Print r The output shows that, in this case, Chroma activ ated the default
model through onnx, as explained in the introduction of this section
and also in the Installing the environment section of this chapter:
/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 10
The output also shows that the processing time for 10,000+
documents is satisfactory:
Response Time: 234.25 seconds The response time mightvary and depends on whether you are using
a GPU. When using an accessible GPU, the time ﬁts the needs
required for dynamic RAG scenarios.
With that, the Chromavector store is now populated. Let’s takeapeek at the embeddings.
Displaying the embeddings The program now fetches the embeddings and displays the ﬁrst one:
# Fetch the collection with embeddings included
result = collection.get(include=['embeddings'])
# Extract the first embedding from the result
first_embedding = result['embeddings'][0]
# If you need to work with the length or manipulate the firsteembedding_length = len(first_embedding)
print("First embedding:", first_embedding)
print("Embedding length:", embedding_length)
The output shows that our completions havebeenvectorized, aswe
can see in the ﬁrst embedding:
First embedding: [0.03689068928360939, -0.05881563201546669, -0
The output also displays the embedding length, which is interesting:
Embedding length: 384
The all-MiniLM-L6-v2 model reduces the complexity of text data by
mapping sentences and paragraphs into a 384-dimensional space.
This is signiﬁcantly low er than the typical dimensionality of one-hot
encodedvectors, such as the 1,526 dimensions of the OpenAI text-
embedding-ada-002. This shows that all-MiniLM-L6-v2 uses dense
vectors, which use all dimensions of thevector space to encode
information to produce nuanced semantic relationships betw een
diﬀerent documents as opposed to sparsevectors.
Sparsevector models, such as the bag-of-words (BoW ) model, can be
eﬀectivein some cases. How ever, their main limitation is that they
don’t capture the order of words or the context around them, which
can be crucial for understanding the meaning of text when training LLMs.
We havenow embedded the documents into densevectors inasmaller dimensional space than full-blown LLMs and will produce
satisfactory results.
Querying the collection The code in this section executesaquery against the Chromavector
store using its integrated semantic search functionality. It queries the
vector representations of all thevectors in the Chroma collection
questions in the initial dataset:
dataset["question"][:nbq].
The query requests one most relev ant or similar document for each
question with n_results=1, which you can modify if you wish.
Each question text is conv erted intoav ector. Then, Chroma runsavector similarity search by comparing the embeddedvectors against
our database of documentvectors to ﬁnd the closest match based on
vector similarity:
import time
start_time = time.time() # Start timing before the request
# number of retrievals to write
results = collection.query(
query_texts=df["question"][:nb],
n_results=1)
response_time = time.time() - start_time # Measure response ti
print(f"Response Time: {response_time:.2f} seconds") # Print r The output displaysasatisfactory response time for the 10,000+
queries:
Response Time: 199.34 seconds We will now analyze the 10,000+ queries. We will use spaCy to
evaluateaquery’s result and compare it with the original completion.
We ﬁrst load the spaCy modelwe installed in the Installing the
environment section of this chapter:
import spacy
import numpy as np
# Load the pre-trained spaCy language model
nlp = spacy.load('en_core_web_md') # Ensure that you've instal The program then createsasimilarity function that takes two
arguments (the original completion, text1, and the retriev ed text,
text2) and returns the similarityvalue:
def simple_text_similarity(text1, text2):
# Convert the texts into spaCy document objects
doc1 = nlp(text1)
doc2 = nlp(text2)

# Get the vectors for each document
vector1 = doc1.vector
vector2 = doc2.vector

# Compute the cosine similarity between the two vectors
# Check for zero vectors to avoid division by zero
if np.linalg.norm(vector1) == 0 or np.linalg.norm(vector2)
return 0.0 # Return zero if one of the texts does not
else:
similarity = np.dot(vector1, vector2) / (np.linalg.norm
return similarity We will now performafullvalidation run on the 10,000 queries. As
can be seen in the following code block, thevalidation begins by
deﬁning thevariableswe will need:
nbqd to only display the ﬁrst 100 and last 100 results.
acc_counter measures the results withasimilarity score superior
to 0.5, which you can modify to ﬁt your needs.
display_counter to count the number of resultswe havedisplay ed:
nbqd = 100 # the number of responses to display, supposing the
# Print the question, the original completion, the retrieved do
acc_counter=0
display_counter=0
The program goes through nb results, which, in our case, is the total
length of our dataset:
for i, q in enumerate(df['question'][:nb]):
original_completion = df['completion'][i] # Access the ori
retrieved_document = results['documents'][i][0] # Retrieve
similarity_score = simple_text_similarity(original_completi The code accesses the original completion and stores it in
original_completion. Then, it retriev es the result and stores it in
retrieved_document. Finally, it calls the similarity functionwe deﬁned,
simple_text_similarity. The original completion and the retriev ed
document store the similarity score in similarity_score.
Now, weintroduce an accuracy metric. In this scenario, the threshold
of the similarity score is set to 0.7, which is reasonable:
if similarity_score > 0.7:
acc_counter+=1
If similarity_score > 0.7, then the accuracy counter, acc_counter, is
incremented. The display counter, display_counter, is also
incremented to only the ﬁrst and last nbqd (maximum results to
display) deﬁned at the beginning of this function:
display_counter+=1
if display_counter<=nbqd or display_counter>nb-nbqd:
The information display ed provides insights into the performance of
the system:
print(i," ", f"Question: {q}")
print(f"Retrieved document: {retrieved_document}")
print(f"Original completion: {original_completion}")
print(f"Similarity Score: {similarity_score:.2f}")
print() # Blank line for better readability between entr The output displays four keyvariables:
{q} is the question asked, the query.
{retrieved_document} is the document retriev ed.
{original_completion} is the original document in the dataset.
{similarity_score:.2f} is the similarity score betw een the
original document and the document retriev ed to measure the
performance of each response.
The ﬁrst output provides the information required forahuman
observ er to control the result of the query and trace it back to the
source.
The ﬁrst part of the output is the question, the query:
Question: What type of organism is commonly used in preparation The second part of the output is the retriev ed document:
Retrieved document: lactic acid because Bacteria can be used to The third part of the output is the original completion. In this case,
we can see that the retriev ed document provides relev ant information
but not the exact original completion:
Original completion: mesophilic organisms because Mesophiles gro Finally, the output displays the similarity score calculated by spaCy:
Similarity Score: 0.73
The score shows that although the original completionwas not
selected, the completion selected is relev ant.
When all the results havebeen analyzed, the program calculates the
accuracy obtained for the 10,000+ queries:
if nb>0:
acc=acc_counter/nb The calculation is based on the following:
Acc is the ov erall accuracy obtained
acc_counter is the total of Similarity scores > 0.7
nb is the number of queries. In this case, nb=len(df)
acc=acc_counter/nb calculates the ov erall accuracy of all the
results The code then displays the number of documents measured and the
overall similarity score:
print(f"Number of documents: {nb:.2f}")
print(f"Overall similarity score: {acc:.2f}")
The output shows that all the questions returned relev ant results:
Number of documents: 10481.00
Overall similarity score: 1.00
This satisfactory ov erall similarity score shows that the system works
inaclosed environment. But we need to go further and see what
happens in the open environment of heated discussions inameeting!
Prompt and retrieval This section is the one to use during real-time querying meetings. You
can adapt the interface to your needs. We’ll focus on functionality.
Let’s look at the ﬁrst prompt:
# initial question
prompt = "Millions of years ago, plants used energy from the su
# variant 1 similar
#prompt = "Eons ago, plants used energy from the sun to form wh
# variant 2 divergent
#prompt = "Eons ago, plants used sun energy to form what?"
You will notice that there are two commentedvariants under the ﬁrst
prompt. Let’s clarify this:
initial question is the exact text that comes from the initial
dataset. It isn’t likely that an aendee in the meeting orauser
will ask the question thatway. Butwe can use it toverify if the
system is working.
variant 1 is similar to the initial question and could be asked.
variant 2 diverges and may provechallenging.
We will select variant 1 for this section andwe should obtainasatisfactory result.
We can see that, as for all AI programs, human control is mandatory!
The more variant 2 diverges with spontaneous questions, the more
challenging it becomes for the system to remain stable and respond
aswe expect. This limit explains why, ev en ifadynamic RAG system
can adapt rapidly, designingasolid system will require careful and
continual improv ements.
If we query the collection aswe did in the previous section with one
prompt only this time, wewill obtainaresponse rapidly:
import time
import textwrap
# Start timing before the request
start_time = time.time()
# Query the collection using the prompt
results = collection.query(
query_texts=[prompt], # Use the prompt inalist as expect
n_results=1 # Number of results to retrieve
)
# Measure response time
response_time = time.time() - start_time
# Print response time
print(f"Response Time: {response_time:.2f} seconds\n")
# Check if documents are retrieved
if results['documents'] and len(results['documents'][0]) > 0:
# Use textwrap to format the output for better readability
wrapped_question = textwrap.fill(prompt, width=70) # Wrap
wrapped_document = textwrap.fill(results['documents'][0][0]
# Print formatted results
print(f"Question: {wrapped_question}")
print("\n")
print(f"Retrieved document: {wrapped_document}")
print()
else:
print("No documents retrieved."
The response time is rapid:
Response Time: 0.03 seconds The output shows that the retriev ed document is relev ant:
Response Time: 0.03 seconds Question: Millions of years ago, plants used energy from the sun Retrieved document: chloroplasts because When ancient plants und
they changed energy in sunlight to stored chemical energy in foo
plants used the food and so did the organisms that ate the plant After the plants and other organisms died, their remains gradual
changed to fossil fuels as they were covered and compressed bylof sediments. Petroleum and natural gas formed from ocean organi
and are found together. Coal formed from giant tree ferns and ot
swamp plants.
We havesuccessfully retriev ed the result of our query. This semantic
vector search might ev en be enough if the aendees of the meeting
are satisﬁed with it. You will alw ays havetime to improvethe
conﬁguration of RAG with Llama.
Hugging Face Llama will now take this response and writeabrief NLP summary.
RAG with Llama We initialized meta-llama/Llama-2-7b-chat-hf in the Installing the
environment section. We must now createafunction to conﬁgure Llama 2’s behavior:
def LLaMA2(prompt):
sequences = pipeline(
prompt,
do_sample=True,
top_k=10,
num_return_sequences=1,
eos_token_id=tokenizer.eos_token_id,
max_new_tokens=100, # Control the output length more gr
temperature=0.5, # Slightly higher for more diversity
repetition_penalty=2.0, # Adjust based on experimentat
truncation=True
)
return sequences You can tw eak each parameter to your expectations:
prompt: The input text that the model uses to generate the output.
It’s the starting point for the model’s response.
do_sample: A Booleanvalue (True or False). When set to True, it
enables stochastic sampling, meaning the model will pick tokens
randomly based on their probability distribution, allowing for
morevaried outputs.
top_k: This parameter limits the number of highest-probability
vocabulary tokens to consider when selecting tokens in the
sampling process. Seing it to 10 means the model will choose
from the top 10 most likely next tokens.
num_return_sequences: Speciﬁes the number of independently
generated responses to return. Here, it is set to 1, meaning the
function will return one sequence for each prompt.
eos_token_id: This token marks the end ofasequence in
tokenized form. Once it is generated, the model stops generating
further tokens. The end-of-sequence token is an id that points to Llama’s eos_token.
max_new_tokens: Limits the number of new tokens the model can
generate. Set to 100 here, it constrains the output toamaximum
length of 100 tokens beyond the input prompt length.
temperature: This controls randomness in the sampling process.
A temperature of 0.5 makes the model’s responses less random
and more focused thanahigher temperature but still allows for
some div ersity.
repetition_penalty: A modiﬁer that discourages the model from
repeating the same token. A penalty of 2.0 means any token
already used is less likely to be chosen again, promoting more
diverse and less repetitivetext.
truncation: When enabled, it ensures the output does not exceed
the maximum length speciﬁed by max_new_tokens by cuing oﬀ
excess tokens.
The prompt will contain the instruction for Llama in iprompt and the
result obtained in the Prompt and retriev al section of the notebook. The
result is appended to iprompt:
iprompt='Read the following input and writeasummary for begin
lprompt=iprompt + " " + results['documents'][0][0]
The augmented input for the Llama call is lprompt. The code will
measure the time it takes and make the completion request:
import time
start_time = time.time() # Start timing before the request
response=LLaMA2(lprompt)
We now retrievethe generated text from the response and display the
time it took for Llama to respond:
for seq in response:
generated_part = seq['generated_text'].replace(iprompt, '')

response_time = time.time() - start_time # Measure response ti
print(f"Response Time: {response_time:.2f} seconds") # Print r The output shows that Llama returned the completion inareasonable
time:
Response Time: 5.91 seconds Let’s wrap the response inanice format to display it:
wrapped_response = textwrap.fill(response[0]['generated_text'],
print(wrapped_response)
The output displaysatechnically reasonable completion:
chloroplasts because When ancient plants underwent photosynthesi
they changed energy in sunlight to stored chemical energy in foo
plants used the food and so did the organisms that ate the plant After the plants and other organisms died, their remains gradual
changed to fossil fuels as they were covered and compressed bylof sediments. Petroleum and natural gas formed from ocean organi
and are found together. Coal formed from giant tree ferns and ot
swamp plants. Natural Gas: 10% methane (CH4) - mostly derived fr
anaerobic decomposition or fermentation processes involving
microorganism such As those present In wetlands; also containssamounts Of ethene(C2H6), propiene/propadiene/( C3 H5-7). This is
most petrol comes frm! But there're more complex hydrocarbons li
pentanes & hexans too which can come The summary produced by Llama is technically acceptable. To obtain
another, possibly beer result, as long as the session is not closed, the
user can runaquery and an augmented generation sev eral times
with diﬀerent Llama parameters.
You can ev en try another LLM. Dynamic RAG doesn’t necessarily
have to be 100% open-source. If necessary, wemust be pragmatic and
introduce whatev er it takes. For example, the following prompt was
submied to ChatGPT with GPT-4o, which is the result of the query
we used for Llama:
Writeanice summary with this text: Question: Millions of year Retrieved document: chloroplasts because When ancient plants un
they changed energy in sunlight to stored chemical energy in fo
changed to fossil fuels as they were covered and compressed by The output of OpenAI GPT-4o surpasses Llama 2 in this case and
producesasatisfactory output:
Millions of years ago, plants harnessed energy from the sun thro If necessary, you can replace meta-llama/Llama-2-7b-chat-hf with GPT-4o, as implemented in Chapter 4 , Multimodal Modular RAG for Drone T echnology , and conﬁgure it to obtain this lev el of output. The
only rule in dynamic RAG is performance. With that, w e’ve seen that
there are manyways to implement dynamic RAG.
Once the session is ov er, wecan delete it.
Deleting the collection You can manually delete the collection with the following code:
#client.delete_collection(collection_name)
You can also close the session to delete the temporary dynamic RAG
collection created. We can check and see whether the collectionwe
created, collection_name, still exists or not:
# List all collections
collections = client.list_collections()
# Check if the specific collection exists
collection_exists = any(collection.name == collection_name for
print("Collection exists:", collection_exists)
If we are still working onacollection inasession, the response will
be True:
Collection exists: True If we delete the collection with code or by closing the session, the
response will be False. Let’s takealook at the total session time.
Total session time The following code measures the time betw een the beginning of the
session and immediately after the Installing the environment section:
end_time = time.time() - session_start_time # Measure response
print(f"Session preparation time: {response_time:.2f} seconds")
The output can havetwo meanings:
It can measure the timewe worked on the preparation of the
dynamic RAG scenario with the daily dataset for the Chroma
collection, querying, and summarizing by Llama.
It can measure the time it took to run the whole notebook
without interv ening at all.
In this case, the session time is the result ofafull run with no human
interv ention:
Session preparation time: 780.35 seconds The whole process takes less than 15 minutes, which ﬁts the
constraints of the preparation time inadynamic RAG scenario. It
leaves room forafew runs to tw eak the system before the meeting.
With that, wehavesuccessfullywalked throughadynamic RAG
process and will now summarize our journey.
Summary Inafast-evolving world, gathering information rapidly for decision-
making providesacompetitiveadv antage. Dynamic RAG is oneway
to bring AI into meeting rooms with rapid and cost-eﬀectiv e AI. We
builtasystem that simulated the need to obtain answ ers to hard
science questions inadaily meeting. After installing and analyzing
the environment, wedownloaded and prepared the SciQ dataset, a
science question-and-answ er dataset, to simulateadaily meeting
during which hard science questions would be asked. The aendees
don’twant to spend their time searching theweb andwasting their
time when decisions must be made. This could be foramarketing
campaign, fact-checking an article, or any other situation in which
hard science know ledge is required.
We created a Chroma collectionvector store. We then embedded
10,000+ documents and inserted data andvectors into the Chroma
vector store on our machine with all-MiniLM-L6-v2. The process
prov ed cost-eﬀectiveand suﬃciently rapid. The collectionwas
created locally, so there is no storage cost. The collection is
temporary, so there is no useless space usage or cluering. We then
queried the collection to measure the accuracy of the systemwe set
up. The resultswere satisfactory, sowe processed the full dataset to
conﬁrm. Finally, wecreated the functionality forauser prompt and
query function to use in real time duringameeting. The result of the
query augmented the user ’s input for meta-llama/Llama-2-7b-chat-hf,
which transformed the query intoashort summary.
The dynamic RAG examplewe implemented would require more
work before being released into production. How ever, it providesapath to open-source, lightw eight, RAG-driv en generativ e AI for rapid
data collection, embedding, and querying. If we need to store the
retriev al data and don’twant to create largevector stores, wecan
integrate our datasets in an OpenAI GPT-4o-mini model, for
example, through ﬁne-tuning, aswe will see in the next chapter.
Questions Answ er the following questions with Yes or No:
1. Does the script ensure that the Hugging Face API token is nev er
hardcoded directly into the notebook for security reasons?
2. In the chapter ’s program, is the accelerate library used here to
facilitate the deployment of ML models on cloud-based
platforms?
3. Is user authentication separate from the API token required to
access the Chroma database in this script?
4. Does the notebook use Chroma for temporary storage ofvectors
during the dynamic retriev al process?
5. Is the notebook conﬁgured to use real-time acceleration of
queries through GPU optimization?
6. Can this notebook’s session time measurements help in
optimizing the dynamic RAG process?
7. Does the script demonstrate Chroma’s capability to integrate
with ML models for enhanced retriev al performance?
8. Does the script include functionality for adjusting the parameters
of the Chroma database based on session performance metrics?
References Crowdsourcing Multiple Choice Science Questions by Johannes Welbl, Nelson F. Liu, Ma Gardner:
http://arxiv.org/abs/1707.06209 .
MiniLMv2: Multi-Head Self-Aention Relation Distillation for Compressing Pretrained Transformers by W enhui W ang, Hangbo Bao, Shaohan Huang, Li Dong, Furu W ei:
https://arxiv.org/abs/2012.15828 .
Hugging Face Llama model documentation:
https://huggingface.co/docs/transformers/main/en/mod
el_doc/llama .
ONNX: https://onnxruntime.ai/ .
Further reading MiniLM: Deep Self-Aention Distillation for T ask-Agnostic Compression of Pre-Trained Transformers by W enhui W ang, Furu Wei, Li Dong, Hangbo Bao, Nan Y ang, Ming Zhou:
https://arxiv.org/abs/2002.10957 .
LLaMA: Open and Eﬃcient Foundation Language Models by Hugo Touvron, Thibaut Lavril, Gautier Lzacard, et al.:
https://arxiv.org/abs/2302.13971 .
Building an ONNX Runtime package:
https://onnxruntime.ai/docs/build/custom.html#custom
-build-packages .
Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag

9
Empowering AI Models: Fine-
Tuning RAG Data and Human Feedback An organization that continually increases the volume of its RAG
data will reach the threshold of non-parametric data (not pretrained
on an LLM). At that point, the mass of RAG data accumulated might
become extremely challenging to manage, posing issues related to
storage costs, retriev al resources, and the capacity of the generativ e AI models themselv es. Moreov er, a pretrained generativ e AI model is
trained up toacutoﬀ date. The model ignores new know ledge
starting thevery next day. This means that it will be impossible forauser to interact withachat model on the content ofanewspaper
edition published after the cutoﬀ date. That is when retriev al hasakey role to play in providing RAG-driv en content.
Companies like Google, Microsoft, Amazon, and otherweb giants
may require exponential data and resources. Certain domains, such
as the legal rulings in the United States, may indeed requirevast
amounts of data. How ever, this doesn’t apply toawide range of
domains. Many corporations do not need to maintain such large
datasets, and in some cases, large portions of static data—like those
in hard sciences—can remain stable foralong time. Such static data
can be ﬁne-tuned to reduce the volume of RAG data required.
In this chapter, therefore, wewill ﬁrst examine the architecture of RAG data reduction through ﬁne-tuning. We will focus onadataset
that contains ready-to-use documents but also stresses the human-
feedback factor. We will demonstrate how to transform non-
parametric data into parametric, ﬁne-tuned data in an OpenAI
model. Then, wewill download and prepare the dataset from the
previous chapter, conv erting the data intowell-formaed prompt
and completion pairs for ﬁne-tuning in JSONL. We will ﬁne-tuneacost-eﬀectiv e OpenAI model, GPT-4o-mini, which will provesuﬃcient
for the completion taskwe will implement. Once the model is ﬁne-
tuned, wewill test it on our dataset toverify that it has successfully
taken our data into account. Finally, wewill explore OpenAI’s
metrics interface, which enables us to monitor our technical metrics,
such as accuracy and usage metrics, to assess the cost-eﬀectiv eness of
our approach.
To sum up, this chapter cov ers the following topics:
The limits of managing RAG data The challenge of determining what data to ﬁne-tune Preparing a JSON dataset for ﬁne-tuning Running OpenAI’s processing tool to produce a JSONL dataset Fine-tuning an OpenAI model Managing the ﬁne-tuning processing time Running the ﬁne-tuned model Let’s begin by deﬁning the architecture of the ﬁne-tuning process.
The architecture of fine-tuning
static RAG data In this section, wequestion the usage of non-parametric RAG data
when it exceedsamanageable threshold, as described in the RAG
versus ﬁne-tuning section in Chapter 1 , Why Retriev al Augmented Generation? , which stated the principle ofathreshold. Figure 9.1
adapts the principle to this section:
Figure 9.1: Fine-tuning threshold reached for RAG data Notice that the processing ( D2) and storage ( D3) thresholds havebeen reached for static dataversus the dynamic data in the RAG data
environment. The threshold depends on each project and parameters
such as:
The volume of RAG data to process : Embedding data requires
human and machine resources. Even ifwe don’t embed the data,
piling up static data (data that is stable ov eralong period of
time) makes no sense.
The volume of RAG data to store and retriev e: At some point, if
we keep stacking data up, much of it may ov erlap.
The retriev als require resources : Even if the system is open
source, there is still an increasing number of resources to
manage.
Other factors, too, may come into play for each project. Whatev er the
reason, ﬁne-tuning can beagood solution whenwe reach the RAG
data threshold.
The RAG ecosyst em In this section, wewill return to the RAG ecosystem described in Chapter 1 . We will focus on the speciﬁc componentswe need for this
chapter. The following ﬁgure presents the ﬁne-tuning components in
color and the oneswe will not need in gray:
Figure 9.2: Fine-tuning components of the RAG ecosystem The key features of the ﬁne-tuning ecosystemswe will build can be
summarized in the following points:
Collecting (D1) and preparing (D2) the dataset : We will
download and process the human-crafted crowdsourced SciQ
hard science datasetwe implemented in the previous chapter:
https://huggingface.co/datasets/sciq .
Human feedback (E2) : We can assume that human feedback
play ed an important role in the SciQ hard science dataset. The
datasetwas controlled by humans and updated sowe can think
of it asasimulation of how reliable human feedback can be ﬁne-
tuned to alleviate the volume of RAG datasets. We can go further
and say it is possible that, in real-life projects, the explanations
present in the SciQ dataset can sometimes come from human
evaluations of models, aswe explored in Chapter 5 , Boosting RAG
Performance with Expert Human Feedback .
Fine-tuning (T2) : We will ﬁne-tuneacost-eﬀectiv e OpenAI
model, GPT-4o-mini.
Prompt engineering (G3) and generation and output (G4) : We
will engineer the prompts as recommended by OpenAI and
display the output.
Metrics (E1) : We will look at the main features of OpenAI’s Metrics interface.
Let’s now go to our keyboards to collect and process the SciQ dataset.
Installing the environm ent Installing an environment has become complex with the rapid
evolution of AI and cross-platform dependency conﬂicts, aswe saw
in Chapter 2 , RAG Embedding V ector Stores with Deep Lake and OpenAI ,
in the Seing up the environment section. We will thus freeze the
packageversions when possible.
For this program, open the Fine_tuning_OpenAI_GPT_4o_mini.ipynb
notebook in the Chapter09 directory on GitHub. The program ﬁrst
retriev es the OpenAI API key:
#You can retrieve your API key fromafile(1)
# or enter it manually(2)
#Comment this cell if you want to enter your key manually.
#(1)Retrieve the API Key fromafile
#Store you key inafile and read it(you can type it directlyifrom google.colab import drive
drive.mount('/content/drive')
f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline()
f.close()
We then install openai and set the API key:
try:
import openai
except:
!pip install openai==1.42.0
import openai
#(2) Enter your manually by
# replacing API_KEY by your key.
#The OpenAI Key
import os
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
Now, weinstall jsonlines to generate JSONL data:
!pip install jsonlines==4.0.0
We now install datasets:
!pip install datasets==2.20.0
Read the Installing the environment section of Chapter 8 , Dynamic RAG
with Chroma and Hugging Face Llama , for explanations of the
dependency conﬂicts involv ed when installing datasets.
Some issues with the installation may occur but the dataset will be
downloaded anyw ay. We must expect and accept such issues as the
leading platforms continually update their packages and create
conﬂicts with pre-installed environments such as Google Colab. You
can createaspecial environment for this program. Bear in mind that
your other programs might encounter issues due to other package
constraints.
We are now ready to prepare the dataset.
1. Preparing the dataset for fine-
tuning Fine-tuning an OpenAI model requires careful preparation;
otherwise, the ﬁne-tuning job will fail. In this section, wewill carry
out the following steps:
1. Download the dataset from Hugging Face and prepare it by
processing its columns.
2. Stream the dataset to a JSON ﬁle in JSONL format.
The program begins by downloading the dataset.
1.1. Downloading and visualizing the
dataset We will download the SciQ datasetwe embedded in Chapter 8 . Aswe
saw, embedding thousands of documents takes time and resources.
In this section, wewill download the dataset, but this time, we will not
embed it . We will let the OpenAI model handle that for us while ﬁne-
tuning the data.
The program downloads the same Hugging Face dataset as in Chapter
8 and ﬁlters the training portion of the dataset to include only non-
empty records with the correct answ er and support text to explain
the answ er to the questions:
# Import required libraries
from datasets import load_dataset
import pandas as pd
# Load the SciQ dataset from HuggingFace
dataset_view = load_dataset("sciq", split="train")
# Filter the dataset to include only questions with support and
filtered_dataset = dataset_view.filter(lambda x: x["support"] !
# Print the number of questions with support
print("Number of questions with support: ", len(filtered_datase The preceding code then prints the number of ﬁltered questions with
support text. The output shows thatwe havea subset of 10,481
records:
Number of questions with support: 10481
Now, wewill load the dataset to a DataFrame and drop the distractor
columns (those with wrong answ ers to the questions):
# Convert the filtered dataset toapandas DataFrame
df_view = pd.DataFrame(filtered_dataset)
# Columns to drop
columns_to_drop = ['distractor3', 'distractor1', 'distractor2']
# Dropping the columns from the DataFrame
df_view = df.drop(columns=columns_to_drop)
# Display the DataFrame
df_view.head()
The output displays the three columnswe need:
Figure 9.3: Output displaying three columns We need the question that will become the prompt. The
correct_answer and support columns will be used for the completion.
Now thatwe haveexamined the dataset, wecan stream the dataset
directly to a JSON ﬁle.
1.2. Preparing the dataset for fine-
tuning To train the completion modelwe will use, weneed to write a JSON
ﬁle in thevery precise JSONL format as required.
We download and process the dataset in the sameway aswe did to
visualize it in the 1.1. Downloading and visualizing the dataset section,
which is recommended to check the dataset before ﬁne-tuning it.
We now write the messages for GPT-4o-mini in JSONL:
# Prepare the data items for JSON lines file
items = []
for idx, row in df.iterrows():
detailed_answer = row['correct_answer'] + " Explanation: "
items.append({
"messages": [
{"role": "system", "content": "Givenascience ques
{"role": "user", "content": row['question']},
{"role": "assistant", "content": detailed_answer}
]
})
We ﬁrst deﬁne the detailed answ er (detailed_answer) with the correct
answ er ('correct_answer') andasupporting ( support) explanation.
Thenwe deﬁne the messages ( messages) for the GPT-4o-mini model:
{"role": "system", "content": ...}: This sets the initial
instruction for the language model, telling it to provide detailed
answ ers to science questions.
{"role": "user", "content": row['question']}: This represents
the user askingaquestion, taken from the question column of
the DataFrame.
{"role": "assistant", "content": detailed_answer}: This
represents the assistant’s response, providing the detailed
answ er constructed earlier.
We can now write our JSONL dataset to a ﬁle:
# Write to JSON lines file
with jsonlines.open('/content/QA_prompts_and_completions.json',
writer.write_all(items)
We havegiv en the OpenAI modelastructure it expects and has been
trained to understand. We can load the JSON ﬁlewe just created inapandas DataFrame toverify its content:
dfile="/content/QA_prompts_and_completions.json"
import pandas as pd
# Load the data
df = pd.read_json(dfile, lines=True)
df The following excerpt of the ﬁle shows thatwe havesuccessfully
prepared the JSON ﬁle:
Figure 9.4: File excerpt That’s it! We are now ready to run a ﬁne-tuning job.
2. Fine-tuning the model To train the model, weretrieveour training ﬁle and create a ﬁne-
tuning job. We begin by creating an OpenAI client:
from openai import OpenAI
import jsonlines
client = OpenAI()
Thenwe use the ﬁlewe generated to create another training ﬁle that
is uploaded to OpenAI:
# Uploading the training file
result_file = client.files.create(
file=open("QA_prompts_and_completions.json", "rb"),
purpose="fine-tune"
)
We print the ﬁle information for the datasetwe are going to use for
ﬁne-tuning:
print(result_file)
param_training_file_name = result_file.id
print(param_training_file_name)
We now create and display the ﬁne-tuning job:
# Creating the fine-tuning job

ft_job = client.fine_tuning.jobs.create(
training_file=param_training_file_name,
model="gpt-4o-mini-2024-07-18"
)
# Printing the fine-tuning job
print(ft_job)
The output ﬁrst provides the name of the ﬁle, its purpose, its status,
and the OpenAI name of the ﬁle ID:
FileObject(id='file-EUPGmm1yAd3axrQ0pyoeAKuE', bytes=8062970, cr The code displays the details of the ﬁne-tuning job:
FineTuningJob(id='ftjob-O1OEE7eEyFNJsO2Eu5otzWA8', created_at=17
The output provides the detailswe need to monitor the job. Here isabrief description of some of the key-v alue pairs in the output:
Job ID: ftjob-O1OEE7eEyFNJsO2Eu5otzWA8.
Status: validating_files. This means OpenAI is currently
checking the training ﬁle to make sure it’s suitable for ﬁne-
tuning.
Model: gpt-4o-mini-2024-07-18. We’re usingasmaller, more cost-
eﬀectiveversion of GPT-4 for ﬁne-tuning.
Training File: file-EUPGmm1yAd3axrQ0pyoeAKuE. This is the ﬁle
we’ve provided that contains the examples to teach the model.
Some key hyperparameters are:
n_epochs: 'auto': OpenAI will automatically determine the best
number of training cycles.
batch_size: 'auto': OpenAI will automatically choose the
optimal batch size for training.
learning_rate_multiplier: 'auto': OpenAI will automatically
adjust the learning rate during training.
Created at: 2024-06-30 08:20:50.
This information will proveuseful if you wish to perform an in-depth
study of ﬁne-tuning OpenAI models. We can also use it to monitor
and manage our ﬁne-tuning process.
2.1. Monitoring the fine-tunes In this section, wewill extract the minimum informationwe need to
monitor the jobs for all our ﬁne-tunes. We will ﬁrst query OpenAI to
obtain the three latest ﬁne-tuning jobs:
import pandas as pd
from openai import OpenAI
client = OpenAI()
# Assume client is already set up and authenticated
response = client.fine_tuning.jobs.list(limit=3) # increase to We then initialize the lists of informationwe want to visualize:
# Initialize lists to store the extracted data
job_ids = []
created_ats = []
statuses = []
models = []
training_files = []
error_messages = []
fine_tuned_models = [] # List to store the fine-tuned model nam Following that, weiterate through response to retrievethe
informationwe need:
# Iterate over the jobs in the response
for job in response.data:
job_ids.append(job.id)
created_ats.append(job.created_at)
statuses.append(job.status)
models.append(job.model)
training_files.append(job.training_file)
error_message = job.error.message if job.error else None
error_messages.append(error_message)
# Append the fine-tuned model name
fine_tuned_model = job.fine_tuned_model if hasattr(job, 'fi
else None
fine_tuned_models.append(fine_tuned_model)
We now create a DataFrame with the informationwe extracted:
import pandas as pd
# Assume client is already set up and authenticated
response = client.fine_tuning.jobs.list(limit=3)
# Create a DataFrame
df = pd.DataFrame({
'Job ID': job_ids,
'Created At': created_ats,
'Status': statuses,
'Model': models,
'Training File': training_files,
'Error Message': error_messages,
'Fine-Tuned Model': fine_tuned_models # Include the fine-tu
})
Finally, weconv ert the timestamps to readable format and display
the list of ﬁne-tunes and their status:
# Convert timestamps to readable format
df['Created At'] = pd.to_datetime(df['Created At'], unit='s')
df = df.sort_values(by='Created At', ascending=False)
# Display the DataFrame
df The output providesamonitoring dashboard of the list of our jobs, as
shown in Figure 9.5 :
Figure 9.5: Job list in the pandas DataFrame You can see that for job 0, the status of the task is running. The status
informs you of the diﬀerent steps of the process such asvalidating
the ﬁles, running, failed, or succeeded. In this case, the ﬁne-tuning
process is running. If you refresh this cell regularly, you will see the
status.
We will now retrievethe most recent model trained for the Fine-Tuned Model column. If the training fails, this column will be empty. If not,
we can retrieveit:
import pandas as pd
generation=False # until the current model is fine-tuned
# Attempt to find the first non-empty Fine-Tuned Model
non_empty_models = df[df['Fine-Tuned Model'].notna() & (df['Fin
if not non_empty_models.empty:
first_non_empty_model = non_empty_models['Fine-Tuned Model'
print("The latest fine-tuned model is:", first_non_empty_mo
generation=True
else:
first_non_empty_model='None'
print("No fine-tuned models found.")
# Display the first non-empty Fine-Tuned Model in the DataFrame
first_non_empty_model = df[df['Fine-Tuned Model'].notna() & (df
print("The lastest fine-tuned model is:", first_non_empty_model The output will display the name of the latest ﬁne-tuned model if
there is one or inform us that no ﬁne-tuned model is found. In this
case, GPT-4o-miniwas successfully trained:
The latest fine-tuned model is: ft:gpt-4o-mini-2024-07-18:person If a ﬁne-tuned model is found, generation=True, it will trigger the OpenAI completion calls in the following cells. If no model is found,
generation=False, it will not run the OpenAI API in the rest of the
notebook to avoid using models that you are not training. You can set
generation to True inanew cell and then select any ﬁne-tuned model
you wish.
We know that the training job can takeawhile. You can refresh the
pandas DataFrame from time to time. You can write code that checks
the status of another job andwaits foraname to appear for your
training job or an error message. You can alsowait for OpenAI to
send you an email informing you that the training job is ﬁnished. If
the training job fails, wemustverify our training data for any
inconsistencies, missingvalues, or incorrect labels. Additionally,
ensure that the JSON ﬁle format adheres to OpenAI’s speciﬁed
schema, including correct ﬁeld names, data types, and structure.
Once the training job is ﬁnished, wecan run completion tasks.
3. Using the fine-tuned OpenAI
model We are now ready to use our ﬁne-tuned OpenAI GPT-4o-mini model.
We will begin by deﬁningaprompt based onaquestion taken from
our initial dataset:
# Define the prompt
prompt = "What phenomenon makes global winds blow northeast to The goal is toverify whether the dataset has been properly trained
and will produce results similar to the completionswe deﬁned. We
can now run the ﬁne-tuned model:
# Assume first_non_empty_model is defined above this snippet
if generation==True:
response = client.chat.completions.create(
model=first_non_empty_model,
temperature=0.0, # Adjust as needed for variability
messages=[
{"role": "system", "content": "Givenaquestion, re
{"role": "user", "content": prompt}
]
)
else:
print("Error: Model is None, cannot proceed with the API re The parameters of the request must ﬁt our scenario:
model=first_non_empty_model is our pretrained model.
prompt=prompt is our predeﬁned prompt.
temperature=0.0 is set toalowvalue becausewe do notwant any
“creativity” for this hard science completion task.
Oncewe run the request, wecan format and display the response.
The following code contains two cells to display and extract the
response.
First, wecan print the raw response:
if generation==True:
print(response)
The output contains the response and information on the process:
ChatCompletion(id='chatcmpl-A32pvH9wLvNsSRmB1sUjxOW4Z6Xr6',…
We then extract the text of the response:
if (generation==True):
# Access the response from the first choice
response_text = response.choices[0].message.content
# Print the response
print(response_text)
The output isastring:
Coriolis effect Explanation: The Coriolis effect is…
Finally, wecan format the response string intoanice paragraph with
the Python wrapper:
import textwrap
if generation==True:
wrapped_text = textwrap.fill(response_text.strip(), 60)
print(wrapped_text)
The output shows that our data has been taken into account:
Coriolis effect Explanation: The Coriolis effect isaphenomenon that causes moving objects, such as air and
water, to turn and twist in response to the rotation of the Earth. It is responsible for the rotation of large weather
systems, such as hurricanes, and the direction of trade
winds and ocean currents. In the Northern Hemisphere, the
effect causes moving objects to turn to the right, while in
the Southern Hemisphere, objects turn to the left. The Coriolis effect is proportional to the speed of the moving
object and the strength of the Earth's rotation, and it is
negligible for small-scale movements, such as water flowing
inasink.
Let’s look at the initial completion for our prompt:

Figure 9.6: Initial completion The response is thus satisfactory. This might not alw ays be the case
and might require more work on the datasets (beer data, large
volumes of data, etc.) incrementally until you havereachedasatisfactory goal.
You can savethe name of your model inatext ﬁle or anywhere you
wish. You can now run your model in another program using the
name of your trained model, or you can reload this notebook at any
time:
1. Run the Installing the environment section of this notebook.
2. Deﬁneaprompt of your choice related to the datasetwe trained.
3. Enter the name of your model in the OpenAI completion request.
4. Run the request and analyze the response.
You can consult OpenAI’s ﬁne-tuning documentation for further
information if necessary:
https://platform.openai.com/docs/guides/fine-
tuning/fine-tuning .
Metrics OpenAI providesauser interface to analyze the metrics of the
training process and model. You can access the metrics related to
your ﬁne-tuned models at
https://platform.openai.com/finetune/ .
The interface displays the list of your ﬁne-tuned jobs:
Figure 9.7: List of ﬁne-tuned jobs You can choose to view all the ﬁne-tuning jobs, the ones thatwere
successful, or the ones that failed. If we chooseajob thatwas
successful, for example, wecan view the job details as shown in the
following excerpt:
Figure 9.8: Example view Let’s go through the information provided in this ﬁgure:
Status : Indicates the status of the ﬁne-tuning process. In this
case, wecan see that the processwas completed successfully.
Job ID : A unique identiﬁer for the ﬁne-tuning job. This can be
used to reference the job in queries or for support purposes.
Base model : Speciﬁes the pretrained model used as the starting
point for ﬁne-tuning. In this case, gpt-4o-mini isav ersion of OpenAI’s models.
Output model : This is the identiﬁer for the model resulting from
the ﬁne-tuning. It incorporates changes and optimizations based
on the speciﬁc training data provided.
Created at : The date and time when the ﬁne-tuning jobwas
initiated.
Trained tokens : The total number of tokens (pieces of text, such
as words or punctuation) thatwere processed during training.
This metric helps gauge the extent of training.
Epochs : The number of complete passes the training datawent
through during ﬁne-tuning. More epochs can lead to beer
learning but too many may lead to ov erﬁing.
Batch size : The number of training examples utilized in one
iteration of model training. Smaller batch sizes can oﬀer more
updates and reﬁned learning but may take longer to train.
LR multiplier : This refers to the learning rate multiplier,
aﬀecting how much the learning rate for the base model is
adjusted during the ﬁne-tuning process. A smaller multiplier can
lead to smaller, more conserv ative updates to modelweights.
Seed : A seed for the random number generator used in the
training process. Providingaseed ensures that the training
process is reproducible, meaning you can get the same results
with the same input conditions.
This information will help tailor the ﬁne-tuning jobs to meet the
speciﬁc needs ofaproject and explore alternativeapproaches to
optimization and customization. In addition, the interface contains
more information thatwe can explore to get an in-depth vision of the
ﬁne-tuning process. If we scroll down on the Information tab of our
model, wecan see metrics as shown here:
Figure 9.9: Metrics for a ﬁne-tuned model Training loss and the other av ailable information can guide our
training strategies (data, ﬁles, and parameters).
Training loss isareliable metric used to ev aluate the performance ofamachine learning model during training. In this case, Training loss
(1.1570) represents the model’s av erage error on the training dataset.
Low er training lossvalues indicate that the model is beer ﬁing the
training data. A training loss of 1.1570 suggests that the model has
learned to predict or classify its training datawell during the ﬁne-
tuning process.
We can also examine thesevalues with the Time and Step
information:
Figure 9.10: Training loss during the training job We must also measure the usage to monitor the cost per period and
model. OpenAI providesadetailed interface at
https://platform.openai.com/usage .
Fine-tuning can indeed be an eﬀectiveway to optimize RAG data if
we make sure to trainamodel with high-quality data and the right
parameters. Now, it’s time for us to summarize our journey and
moveto our next RAG-driv en generativ e AI implementation.
Summary This chapter ’s goalwas to show that aswe accumulate RAG data,
some data is dynamic and requires constant updates, and as such,
cannot be ﬁne-tuned easily. How ever, some data is static, meaning
that it will remain stable for long periods of time. This data can
become parametric (stored in theweights ofatrained LLM).
We ﬁrst downloaded and processed the SciQ dataset, which contains
hard science questions. This stable data perfectly suits ﬁne-tuning. It
containsaquestion, answ er, and support (explanation) structure,
which makes the data eﬀectivefor ﬁne-tuning. Also, wecan assume
human feedbackwas required. We can ev en go as far as imagining
this feedback could be provided by analyzing generativ e AI model
outputs.
We conv erted the datawe prepared into prompts and completions in
a JSONL ﬁle following the recommendations of OpenAI’s
preparation tool. The structure of JSONL w as meant to be compatible
withacompletion model (prompt and completion) such as GPT-4o-
mini. The program then ﬁne-tuned the cost-eﬀectiv e GPT-4o-mini OpenAI model, following whichwe ran the model and found that the
outputwas satisfactory. Finally, weexplored the metrics of the ﬁne-
tuned model in the OpenAI metrics user interface.
We can conclude that ﬁne-tuning can optimize RAG data in certain
cases when necessary. How ever, wewill take this process further in
the next chapter, Chapter 10 , RAG for V ideo Stock Production with Pinecone and OpenAI , whenwe run the full-blown RAG-driv en
generativ e AI ecosystem.
Questions Answ er the following questions withyes or no:
1. Do all organizations need to manage large volumes of RAG
data?
2. Is the GPT-4o-mini model described as insuﬃcient for ﬁne-
tuning tasks?
3. Can pretrained models update their know ledge base after the
cutoﬀ date without retriev al systems?
4. Is it the case that static data nev er changes and thus nev er
requires updates?
5. Is downloading data from Hugging Face the only source for
preparing datasets?
6. Is all RAG data ev entually embedded into the trained model’s
parameters according to the document?
7. Does the chapter recommend using only new data for ﬁne-
tuning AI models?
8. Is the OpenAI Metrics interface used to adjust the learning rate
during model training?
9. Can the ﬁne-tuning process be eﬀectiv ely monitored using the OpenAI dashboard?
10. Is human feedback deemed unnecessary in the preparation of
hard science datasets such as SciQ?
References OpenAI ﬁne-tuning documentation:
https://platform.openai.com/docs/guides/fine-tuning/
OpenAI pricing: https://openai.com/api/pricing/
Further reading Test of Fine-T uning GPT by Astrophysical Data by Yu W ang et al. is
an interesting article on ﬁne-tuning hard science data, which
requires careful data preparation:
https://arxiv.org/pdf/2404.10019
Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag

10
RAG for V ideo Stock Production
with Pinecone and OpenAI
Human creativity goes beyond the range ofwell-known paerns due
to our unique ability to break habits and inv ent newways of doing
anything, anywhere. Conv ersely, Generativ e AI relies on ourwell-
known established paerns across an increasing number of ﬁelds
without really “creating” but rather replicating our habits. In this
chapter, therefore, whenwe use the term “create” asapractical term,
we only mean “generate.” Generativ e AI, with its eﬃciency in
automating tasks, will continue its expansion until it ﬁndsways of
replicating any human task it can. We must, therefore, learn how
these automated systems work to use them for the best in our
projects. Think of this chapter asajourney into the architecture of RAG in the cuing-edge hybrid human and AI agent erawe are
living in. We will assume the role ofastart-up aiming to build an AI-
driven downloadable stock of online videos. To achievethis, wewill
establishateam of AI agents that will work together to createastock
of commented and labeled videos.
Our journey begins with the Generator agent in Pipeline 1: The Generator and the Commentator . The Generator agent creates world
simulations using Sora, an OpenAI text-to-video model. You’ll see
how the inVideo AI application, pow ered by Sora, engages in
“ideation,” transforming an idea intoavideo. The Commentator
agent then splits the AI-generated videos into frames and generates
technical comments with an OpenAI vision model. Next, in Pipeline 2:
The V ector Store Administrator, we will continue our journey and build
the V ector Store Administrator that manages Pinecone. The V ector Store Administrator will embed the technical video comments
generated by the Commentator, upsert thevectorized comments, and
query the Pineconevector store toverify that the system is functional.
Finally, wewill build the V ideo Expert that processes user inputs,
queries thevector store, and retriev es the relev ant video frames.
Finally, in Pipeline 3: The V ideo Expert , the V ideo Expert agent will
augment user inputs with the raw output of the query and activ ate its
expert OpenAI GPT-4o model, which will analyze the comment,
detect imperfections, reformulate it more eﬃciently, and providealabel for the video.
By the end of the chapter, you will know how to automatically
generateastock of short videos by automating the process of going
from raw footage to videos with descriptions and labels. You’ll be
able to oﬀeraservice where users can simply typeafew words and
obtainavideo withacustom, real-time description and label.
Summing that up, this chapter cov ers the following topics:
Designing Generativ e AI videos and comments Spliing videos into frames for OpenAI’s vision analysis models Embedding the videos and upserting thevectors to a Pinecone
index Querying thevector store Improving and correcting the video comments with OpenAI
GPT-4o Automatically labeling raw videos Displaying the full result of the raw video process withacommented and labeled video Evaluating outputs and implementing metric calculations Let’s begin by deﬁning the architecture of RAG for video production.
The architecture of RAG for video
production Automating the process of real-world video generation, commenting,
and labeling is extremely relev ant invarious industries, such as
media, marketing, entertainment, and education. Businesses and
creators are continuously seeking eﬃcientways to produce and
manage content that can scale with growing demand. In this chapter,
you will acquire practical skills that can be directly applied to meet
these needs.
The goal of our RAG video production use case in this chapter is to
process AI-generated videos using AI agents to createavideo stock of
labeled videos to identify them. The system will also dynamically
generate custom descriptions by pinpointing AI-generated technical
comments on speciﬁc frames within the videos that ﬁt the user input.
Figure 10.1 illustrates the AI-agent team that processes RAG for video
production:
Figure 10.1: From raw videos to labeled and commented videos We will implement AI agents for our RAG video production pipeline
that will:
Generate raw videos automatically and download them Split the videos into frames Analyzeasample of frames Activ ate an OpenAI LLM model to generate technical comments Save the technical comments withaunique index, the comment
itself, the frame number analyzed, and the video ﬁle name Upsert the data in a Pinecone indexvector store Query the Pineconevector store with user inputs Retrievethe speciﬁc frame withinavideo that is most similar to
its technical comment Augment the user input with the technical comment of the
retriev ed frame Ask the OpenAI LLM to analyze the logic of the technical
comment that may contain contradictions and imperfections
detected in the video and then produceadynamic, w ell-tailored
description of the video with the frame number and the video
ﬁle name Display the selected video Evaluate the outputs and apply metric calculations We will thus go from raw videos to labeled videos with tailored
descriptions based on the user input. For example, wewill be able to
ask precise questions such as the following:
"Findabasketball player that is scoring withadunk."
This means that the system will be able to ﬁndaframe (image) within
the initially unlabeled video, select the video, display it, and generateatailored comment dynamically. To aain our goal, wewill
implement AI agents in three pipelines, as illustrated in the following
ﬁgure:
Figure 10.2: The RAG for Video Production Ecosystem with Generative AI agents Now, what you see in the ﬁgure aboveis:
Pipeline 1 : The Generator and the Commentator The Generator produces AI-generated videos with OpenAI
Sora. The Commentator splits the videos into frames that are
commented on by one of OpenAI’s vision models. The Commentator agent then sav es the comments.
Pipeline 2 : The V ector Store Administrator This pipeline will embed and upsert the comments made by Pipeline 1 to a Pinecone index.
Pipeline 3 : The V ideo Expert This pipeline will query the Pineconevector store based on user
input. The query will return the most similar frame withinavideo, augment the input with the technical comment, and ask OpenAI GPT-4o to ﬁnd logic imperfections in the video, point
them out, and then produceatailored comment of the video for
the user andalabel. This section also contains ev aluation
functions (the Ev aluator ) and metric calculations.
Time measurement functions are encapsulated in
several of the key functions of the preceding
ecosystem.
The RAG video production systemwe will build allows indeﬁnite
scaling by processing one video atatime, using only a CPU and lile
memory, while lev eraging Pinecone’s storage capacity. This
eﬀectiv ely demonstrates the concept of automated video production,
but implementing this production system inareal-life project
requires hard work. How ever, the technology is there, and the future
of video production is undergoingahistorical evolution. Let’s diveinto the code, beginning with the environment.
The environme nt of the video
production ecosystem The Chapter10 directory on GitHub contains the environment for all
four notebooks in this chapter:
Videos_dataset_visualization.ipynb Pipeline_1_The_Generator_and_the_Commentator.ipynb Pipeline_2_The_Vector_Store_Administrator.ipynb Pipeline_3_The_Video_Expert.ipynb Each notebook includes an Installing the environment section,
includingaset of the following sections that are identical across all
notebooks:
Importing modules and libraries GitHub Video download and display functions OpenAI
Pinecone This chapter aims to establishacommon pre-production installation
policy that will focus on the pipelines’ content oncewe diveinto the RAG for video production code. This policy is limited to the scenario
described in this chapter and willvary depending on the
requirements of each real-life production environment.
The notebooks in this chapter only require a CPU,
limited memory, and limited disk space. As such, the
whole process can be streamlined indeﬁnitely one
video atatime in an optimized, scalable environment.
Let’s begin by importing the modules and librarieswe need for our
project.
Importing modules and libraries The goal is to prepareapre-production global environment common
to all the notebooks. As such, the modules and libraries are present in
all four notebooks regardless of whether they are used or not inaspeciﬁc program:
from IPython.display import HTML # to display videos
import base64 # to encode videos as base64
from base64 import b64encode # to encode videos as base64
import os # to interact with the operating system
import subprocess # to run commands
import time # to measure execution time
import csv # to save comments
import uuid # to generate unique ids
import cv2 # to split videos
from PIL import Image # to display videos
import pandas as pd # to display comments
import numpy as np # to use Numerical Python
from io import BytesIO #to manageabinary stream of data in me Each of the four notebooks contains these modules and libraries, as
shown in the following table:
Code Comment
from IPython.display
import HTMLTo display videos
import base64 To encode videos as base64
from base64 import
b64encodeTo encode videos as base64
import os To interact with the operating system
import subprocess To run commands
import time To measure execution time
import csv To savecomments
import uuid To generate unique IDs
import cv2 To split videos (open source computer
vision library)
from PIL import Image To display videos
import pandas as pd To display comments
import numpy as np To use Numerical Python
from io import BytesIO Forabinary stream of data in memory Table 10.1: Modules and libraries for our video production system The Code column contains the module or library name, while the Comment column providesabrief description of their usage. Let’s
moveon to GitHub commands.
GitHub
download(directory, filename) is present in all four notebooks. The
main function of download(directory, filename) is to download the
ﬁleswe need from the book’s GitHub repository:
def download(directory, filename):
# The base URL of the image files in the GitHub repository
base_url = 'https://raw.githubusercontent.com/Denis2054/RAG
# Complete URL for the file
file_url = f"{base_url}{directory}/{filename}"
# Use curl to download the file
try:
# Prepare the curl command
curl_command = f'curl -o {filename} {file_url}'
# Execute the curl command
subprocess.run(curl_command, check=True, shell=True)
print(f"Downloaded '{filename}' successfully.")
except subprocess.CalledProcessError:
print(f"Failed to download '{filename}'. Check the URL,
The preceding function takes two arguments:
directory, which is the GitHub directory that the ﬁlewe want to
download is located in
filename, which is the name of the ﬁlewe want to download OpenAI
The OpenAI package is installed in all three pipeline notebooks but
not in Video_dataset_visualization.ipynb, which doesn’t require an LLM. You can retrievethe API key from a ﬁle or enter it manually
(but it will be visible):
#You can retrieve your API key fromafile(1)
# or enter it manually(2)
#Comment this cell if you want to enter your key manually.
#(1)Retrieve the API Key fromafile
#Store you key inafile and read it(you can type it directlyifrom google.colab import drive
drive.mount('/content/drive')
f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline()o Nf.close()
You will need to sign up at www.openai.com before running the code
and obtain an API key. The program installs the openai package:
try:
import openai
except:
#!pip install openai==1.45.0
import openai Finally, weset an environmentvariable for the API key:
#(2) Enter your manually by
# replacing API_KEY by your key.
#The OpenAI Key
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
Pinecone The Pinecone section is only present in Pipeline_2_The_Vector_Store_Administrator.ipynb and Pipeline_3_The_Video_Expert.ipynb when the Pineconevector store is
required. The following command installs Pinecone, and then Pinecone is imported:
!pip install pinecone-client==4.1.1
import pinecone The program then retriev es the key from a ﬁle (or you can enter it
manually):
f = open("drive/MyDrive/files/pinecone.txt", "r")
PINECONE_API_KEY=f.readline()
f.close()
In production, you can set an environmentvariable or implement the
method that best ﬁts your project so that the API key is nev er visible.
The Evaluator section of Pipeline_3_The_Video_Expert.ipynb contains its own
requirements and installations.
With that, wehavedeﬁned the environment for all four notebooks,
which contain the same sub-sectionswe just described in their
respectiv e Installing the environment sections. We can now fully focus
on the processes involv ed in the video production programs. We will
begin with the Generator and Commentator.
Pipeline 1: Generator and Commentator A revolution is on itsway in computer vision with automated video
generation and analysis. We will introduce the Generator AI agent
with Sora in The AI-generated video dataset section. We will explore
how OpenAI Sorawas used to generate the videos for this chapter
withatext-to-video diﬀusion transformer. The technology itself is
somethingwe haveexpected and experienced to some extent in
professional ﬁlm-making environments. How ever, the nov elty relies
on the fact that the softw are has become mainstream inafew clicks,
with inV ideo, for example!
In the The Generator and the Commentator section, wewill extend the
scope of the Generator to collecting and processing the AI-generated
videos. The Generator splits the videos into frames and works with
the Commentator, an OpenAI LLM, to produce comments on
samples of video frames.
The Generator ’s task begins by producing the AI-generated video
dataset.
The AI-generated video dataset The ﬁrst AI agent in this project isatext-to-video diﬀusion
transformer model that generatesavideo datasetwe will implement.
The videos for this chapterwere speciﬁcally generated by Sora, a text-
to-video AI model released by OpenAI in February 2024. You can
access Sora to view public AI-generated videos and create your own
at https://ai.invideo.io/ . AI-generated videos also allow for free
videos with ﬂexible copyright terms that you can check out at
https://invideo.io/terms-and-conditions/ .
Once you havegone through this chapter, you can also
create your own video dataset with any source of
videos, such as smartphones, video stocks, and social
media.
AI-generated videos enhance the speed of creating video datasets.
Teams do not haveto spend time ﬁnding videos that ﬁt their needs.
They can obtainavideo quickly withaprompt that can be an idea
expressed inafew words. AI-generated videos representahuge leap
into the future of AI applications. Sora’s potential applies to many
industries, including ﬁlmmaking, education, and marketing. Its
ability to generate nuanced video content from simple text prompts
opens new av enues for creativeand educational outputs.
Although AI-generated videos (and, in particular, diﬀusion
transformers) havechanged thewaywe create world simulations,
this representsarisk for jobs in many areas, such as ﬁlmmaking. The
risk of deep fakes and misinformation is real. Atapersonal lev el, wemust take ethical considerations into account whenwe implement Generativ e AI inaproject, thus producing constructiv e, ethical, and
realistic content.
Let’s see howadiﬀusion transformer can produce realistic content.
How doesadiffusion transformer
work?
At the core of Sora, as described by Liu et al., 2024 (see the References
section), isadiﬀusion transformer model that operates betw een an
encoder andadecoder. It uses user text input to guide the content
generation, associating it with patches from the encoder. The model
iterativ ely reﬁnes these noisy latent representations, enhancing their
clarity and coherence. Finally, the reﬁned data is passed to the
decoder to reconstruct high-ﬁdelity video frames. The technology
involv ed includes vision transformers such as CLIP and LLMs such
as GPT-4, aswell as other components OpenAI continually includes
in its vision model releases.
The encoder and decoder are integral components of the ov erall
diﬀusion model, as illustrated in Figure 10.3 . They both playacritical
role in the workﬂow of the transformer diﬀusion model:
Encoder : The encoder ’s primary function is to compress input
data, such as images or videos, intoalow er-dimensional latent
space. The encoder thus transforms high-dimensional visual data
intoacompact representation while preserving crucial
information. A low er-dimensional latent space obtained isacompressed representation of high-dimensional data, retaining
essential features while reducing complexity. For example, a
high-resolution image (1024x1024 pixels, 3 color channels) can be
compressed by an encoder intoav ector of 1000 v alues, capturing
key details like shape and texture. This makes processing and
manipulating images more eﬃcient.
Decoder : The decoder reconstructs the original data from the
latent representation produced by the encoder. It performs the
encoder ’s rev erse operation, transforming the low-dimensional
latent space back into high-dimensional pixel space, thus
generating the ﬁnal output, such as images or videos.
Figure 10.3: The encoding and decoding workﬂow of video diﬀusion models The process ofadiﬀusion transformer model goes through ﬁvemain
steps, as you can observein the previous ﬁgure:
1. The visual encoder transforms datasets of images intoalow er-
dimensional latent space.
2. The visual encoder splits the low er-dimensional latent space into
patches that are like words inasentence.
3. The diﬀusion transformer associates user text input with its
dictionary of patches.
4. The diﬀusion transformer iterativ ely reﬁnes noisy image
representations generated to produce coherent frames.
5. The visual decoder reconstructs the reﬁned latent
representations into high-ﬁdelity video frames that align with
the user ’s instructions.
The video frames can then be play ed inasequence. Every second ofavideo containsaset of frames. We will be deconstructing the AI-
generated videos into frames and commenting on these frames later.
But for now, wewill analyze the video dataset produced by the
diﬀusion transformer.
Analyzing the diffusion transformer
model video dataset Open the Videos_dataset_visualization.ipynb notebook on GitHub.
Hopefully, you haveinstalled the environment as described earlier in
this chapter. We will moveon to writing the download and display
functions we need.
Video download and display functions The three main functions each use filename (the name of the video
ﬁle) as an argument. The three main functions download and display
videos, and display frames in the videos.
download_video downloads one video atatime from the GitHub
dataset, calling the download function deﬁned in the GitHub
subsection of The environment :
# downloading file from GitHub
def download_video(filename):
# Define your variables
directory = "Chapter10/videos"
filename = file_name
download(directory, filename)
display_video(file_name) displays the video ﬁle downloaded by ﬁrst
encoding in base64, a binary-to-text encoding scheme that represents
binary data in ASCII string format. Then, the encoded video is
display ed in HTML:
# Open the file in binary mode
def display_video(file_name):
with open(file_name, 'rb') as file:
video_data = file.read()
# Encode the video file as base64
video_url = b64encode(video_data).decode()
# Create an HTML string with the embedded video
html = f'''
<video width="640" height="480" controls>
<source src="data:video/mp4;base64,{video_url}" type="video Your browser does not support the video tag.
</video>
'''
# Display the video HTML(html)
# Return the HTML object
return HTML(html)
display_video_frame takes file_name, frame_number, and size (the
image size to display) as arguments to displayaframe in the video.
The function ﬁrst opens the video ﬁle and then extracts the frame
number set by frame_number:
def display_video_frame(file_name, frame_number, size):
# Open the video file
cap = cv2.VideoCapture(file_name)
# Move to the frame_number
cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
# Read the frame
success, frame = cap.read()
if not success:
return "Failed to grab frame"
The function conv erts the ﬁle from the BGR (blue, green, and red) to
the RGB (red, green, and blue) channel, conv erts it to PIL, an image
array (such as one handled by OpenCV), and resizes it with the size
parameters:
# Convert the color from BGR to RGB
frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
# Convert to PIL image and resize
img = Image.fromarray(frame)
img = img.resize(size, Image.LANCZOS) # Resize image to sp Finally, the function encodes the image in string format with base64
and displays it in HTML:
# Convert the PIL image toabase64 string to embed in HTML
buffered = BytesIO()
img.save(buffered, format="JPEG")
img_str = base64.b64encode(buffered.getvalue()).decode()
# Create an HTML string with the embedded image
html_str = f'''
<img src="data:image/jpeg;base64,{img_str}" width="{size[0]
'''
# Display the image
display(HTML(html_str))
# Return the HTML object for further use if needed
return HTML(html_str)
Once the environment is installed and the video processing functions
are ready, wewill display the introduction video.
Introduction video (with audio)
The following cells download and display the introduction video
using the functionswe created in the previous section. A video ﬁle is
selected and downloaded with the download_video function:
# select file
print("Collecting video")
file_name="AI_Professor_Introduces_New_Course.mp4"
#file_name = "AI_Professor_Introduces_New_Course.mp4" # Entertprint(f"Video: {file_name}")
# Downloading video
print("Downloading video: downloading from GitHub")
download_video(file_name)
The output conﬁrms the selection and download status:
Collecting video Video: AI_Professor_Introduces_New_Course.mp4
Downloading video: downloading from GitHub Downloaded 'AI_Professor_Introduces_New_Course.mp4' successfully We can choose to display onlyasingle frame of the video asathumbnail with the display_video_frame function by providing the
ﬁle name, the frame number, and the image size to display. The
program will ﬁrst compute frame_count (the number of frames in the
video), frame_rate (the number of frames per second), and
video_duration (the duration of the video). Then, it will make sure
frame_number (the framewe want to display) doesn’t exceed
frame_count. Finally, it displays the frame asathumbnail:
print("Displayingaframe of video: ",file_name)
video_capture = cv2.VideoCapture(file_name)
frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))
print(f'Total number of frames: {frame_count}')
frame_rate = video_capture.get(cv2.CAP_PROP_FPS)
print(f"Frame rate: {frame_rate}")
video_duration = frame_count / frame_rate
print(f"Video duration: {video_duration:.2f} seconds")
video_capture.release()
print(f'Total number of frames: {frame_count}')
frame_number=5
if frame_number > frame_count and frame_count>0:
frame_number = 1
display_video_frame(file_name, frame_number, size=(135, 90));
Here, frame_number is set to 5, but you can choose anothervalue. The
output shows the information on the video and the thumbnail:
Displayingaframe of video: /content/AI_Professor_Introduces_N
Total number of frames: 340
We can also display the full video if needed:
#print("Displaying video: ",file_name)
display_video(file_name)
The video will be display ed and can be play ed with the audio track:
Figure 10.4: AI-generated video Let’s describe and display AI-generated videos in the /videos
directory of this chapter ’s GitHub directory. You can host this dataset
in another location and scale it to the volume that meets your
project’s speciﬁcations. The educational video dataset of this chapter
is listed in lfiles:
lfiles = [
"jogging1.mp4",
"jogging2.mp4",
"skiing1.mp4",
…
"female_player_after_scoring.mp4",
"football1.mp4",
"football2.mp4",
"hockey1.mp4"
]
We can now moveon and display any videowe wish.
Displaying thumbnails and videos in the AI-
generated dataset This section isageneralization of the Introduction video (with audio)
section. This time, instead of downloading one video, it downloads
all the videos and displays the thumbnails of all the videos. You can
then selectavideo in the list and display it.
The program ﬁrst collects the video dataset:
foriin range(lf):
file_name=lfiles[i]
print("Collecting video",file_name)
print("Downloading video",file_name)
download_video(file_name)
The output shows the ﬁle names of the downloaded videos:
Collecting video jogging1.mp4
Downloading video jogging1.mp4
Downloaded 'jogging1.mp4' successfully.
Collecting video jogging2.mp4…
The program calculates the number of videos in the list:
lf=len(lfiles)
The program goes through the list and displays the information for
each video and displays its thumbnail:
foriin range(lf):
file_name=lfiles[i]
video_capture.release()
display_video_frame(file_name, frame_number=5, size=(100, 110
The information on the video and its thumbnail is display ed:
Displayingaframe of video: skiing1.mp4
Total number of frames: 58
Frame rate: 30.0
Video duration: 1.93 seconds You can selectavideo in the list and display it:
file_name="football1.mp4" # Enter the name of the video file to
#print("Displaying video: ",file_name)
display_video(file_name)
You can click on the video andwatch it:
Figure 10.5: Video ofafootball player We haveexplored how the AI-generated videoswere produced and
visualized the dataset. We are now ready to build the Generator and
the Commentator.
The Generator and the Commentator The dataset of AI-generated videos is ready. We will now build the Generator and the Commentator, which processes one video atatime, making scaling seamless. An indeﬁnite number of videos can be
processed one atatime, requiring only a CPU and limited disk space.
The Generator and the Commentator work together, as shown in Figure 10.8 . These AI agents will produce raw videos from text and
then split them into frames that they will comment on:
Figure 10.6: The Generator and the Commentator work together to comment on video
frames The Generator and the Commentator produce the commented frames
required in four main steps thatwe will build in Python:
1. The Generator generates the text-to-video inV ideo video dataset
based on the video production team’s text input. In this chapter,
it isadataset of sports videos.
2. The Generator runsascaled process by selecting one video atatime.
3. The Generator splits the video into frames (images)
4. The Commentator samples frames (images) and comments on
them with an OpenAI LLM model. Each commented frame is
saved with:
Unique ID
Comment Frame Video ﬁle name We will now build the Generator and the Commentator in Python,
starting with the AI-generated videos. Open Pipeline_1_The_Generator_and_the_Commentator.ipynb in the chapter ’s GitHub directory. See the The environment section of this chapter foradescription of the Installing the environment section of this notebook.
The process of going fromavideo to comments onasample of
frames only takes three straightforw ard steps in Python:
1. Displaying the video
2. Spliing the video into frames
3. Commenting on the frames We will deﬁne functions for each step and call them in the Pipeline-1
Controller section of the program. The ﬁrst step is to deﬁneafunction to displayavideo.
Step 1. Displaying the video The download function is in the GitHub subsection of the Installing the
environment section of this notebook. It will be called by the Vector Store Administrator-Pipeline 1 in the Administrator-Pipeline 1 section of
this notebook on GitHub.
display_video(file_name) is the same as deﬁned in the previous
section, The AI-generated video dataset :
# Open the file in binary mode
def display_video(file_name):
with open(file_name, 'rb') as file:
video_data = file.read()
…
# Return the HTML object
return HTML(html)
The downloaded video will now be split into frames.
Step 2. Splitting video into frames The split_file(file_name) function extracts frames fromavideo, as
in the previous section, The AI-generated video dataset . How ever, in this
case, wewill expand the function to saveframes as JPEG ﬁles:
def split_file(file_name):
video_path = file_name
cap = cv2.VideoCapture(video_path)
frame_number = 0
while cap.isOpened():
ret, frame = cap.read()
if not ret:
break
cv2.imwrite(f"frame_{frame_number}.jpg", frame)
frame_number += 1
print(f"Frame {frame_number} saved.")
cap.release()
We havesplit the video into frames and sav ed them as JPEG images
with their respectiveframe number, frame_number. The Generator ’s
job ﬁnishes here and the Commentator now takes ov er.
Step 3. Commenting on the frames The Generator has gone from text-to-video to spliing the video and
saving the frames as JPEG frames. The Commentator now takes ov er
to comment on the frames with three functions:
generate_openai_comments(filename) asks the GPT-4 series vision
model to analyzeaframe and producearesponse that containsacomment describing the frame
generate_comment(response_data) extracts the comment from the
response
save_comment(comment, frame_number, file_name) saves the
comment We need to build the Commentator ’s extraction function ﬁrst:
def generate_comment(response_data):
"""Extract relevant information from GPT-4 Vision response.
try:
caption = response_data.choices[0].message.content
return caption
except (KeyError, AttributeError):
print("Error extracting caption from response.")
return "No caption available."
We then writeafunction to savethe extracted comment in a CSV ﬁle
that bears the same name as the video ﬁle:
def save_comment(comment, frame_number, file_name):
"""Save the comment toatext file formatted for seamless l
# Append .csv to the provided file name to create the compl
path = f"{file_name}.csv"
# Check if the file exists to determine if we need to write
write_header = not os.path.exists(path)
with open(path, 'a', newline='') as f:
writer = csv.writer(f, delimiter=',', quotechar='"', qu
if write_header:
writer.writerow(['ID', 'FrameNumber', 'Comment', 'F
# Generateaunique UUID for each comment
unique_id = str(uuid.uuid4())
# Write the data
writer.writerow([unique_id, frame_number, comment, file The goal is to savethe comment inaformat that can directly be
upserted to Pinecone:
ID: A unique string ID generated with str(uuid.uuid4())
FrameNumber: The frame number of the commented JPEG
Comment: The comment generated by the OpenAI vision model FileName: The name of the video ﬁle The Commentator ’s main function is to generate comments with the OpenAI vision model. How ever, in this program’s scenario, wewill
not saveall the frames butasample of the frames. The program ﬁrst
determines the number of frames to process:
def generate_openai_comments(filename):
video_folder = "/content" # Folder containing your image fra
total_frames = len([file for file in os.listdir(video_folder)
Then, a sample frequency is set that can be modiﬁed along withacounter:
nb=3 # sample frequency
counter=0 # sample frequency counter The Commentator will then go through the sampled frames and
requestacomment:
for frame_number in range(total_frames):
counter+=1 # sampler
if counter==nb and counter<total_frames:
counter=0
print(f"Analyzing frame {frame_number}...")
image_path = os.path.join(video_folder, f"frame_{frame_
try:
with open(image_path, "rb") as image_file:
image_data = image_file.read()
response = openai.ChatCompletion.create(
model="gpt-4-vision-preview",
The message isvery concise: "What is happening in this image?" The
message also includes the image of the frame:
messages=[
{
"role": "user",
"content": [
{"type": "text", "text": "What
{
"type": "image",
"image_url": f"data:image/j
},
],
}
],
max_tokens=150,
)
Oncearesponse is returned, the generate_comment and save_comment
functions are called to extract and savethe comment, respectiv ely:
comment = generate_comment(response)
save_comment(comment, frame_number,file_name)
except FileNotFoundError:
print(f"Error: Frame {frame_number} not found.")
except Exception as e:
print(f"Unexpected error: {e}")
The ﬁnal functionwe require of the Commentator is to display the
comments by loading the CSV ﬁle produced inapandas DataFrame:
# Read the video comments file intoapandas DataFrame
def display_comments(file_name):
# Append .csv to the provided file name to create the complet
path = f"{file_name}.csv"
df = pd.read_csv(path)
return df The function returns the DataFrame with the comments. An
administrator controls Pipeline 1 , the Generator, and the Commentator.
Pipeline 1 controller The controller runs jobs for the preceding three steps of the Generator
and the Commentator. It begins with Step 1 , which includes selectingavideo, downloading it, and displaying it. In an automated pipeline,
these functions can be separated. For example, a script would iterate
throughalist of videos, automatically select each one, and
encapsulate the controller functions. In this case, inapre-production
and educational context, wewill collect, download, and display the
videos one by one:
session_time = time.time() # Start timing before the request
# Step 1: Displaying the video
# select file
print("Step 1: Collecting video")
file_name = "skiing1.mp4" # Enter the name of the video file to
print(f"Video: {file_name}")
# Downloading video
print("Step 1:downloading from GitHub")
directory = "Chapter10/videos"
download(directory,file_name)
# Displaying video
print("Step 1:displaying video")
display_video(file_name)
The controller then splits the video into frames and comments on the
frames of the video:
# Step 2.Splitting video
print("Step 2: Splitting the video into frames")
split_file(file_name)
The controller activ ates the Generator to produce comments on
frames of the video:
# Step 3.Commenting on the video frames
print("Step 3: Commenting on the frames")
start_time = time.time() # Start timing before the request
generate_openai_comments(file_name)
response_time = time.time() - session_time # Measure response The response time is measured aswell. The controller then adds
additional outputs to display the number of frames, the comments,
the content generation time, and the total controller processing times:
# number of frames
video_folder = "/content" # Folder containing your image frame
total_frames = len([file for file in os.listdir(video_folder) i
print(total_frames)
# Display comments
print("Commenting video: displaying comments")
display_comments(file_name)
total_time = time.time() - start_time # Start timing before th
print(f"Response Time: {response_time:.2f} seconds") # Printrprint(f"Total Time: {total_time:.2f} seconds") # Print respons The controller has completed its task of producing content. How ever,
depending on your project, you can introduce dynamic RAG for
some or all the videos. If you need this functionality, you can apply
the process described in Chapter 5 , Boosting RAG P erformance with Expert Human Feedback , to the Commentator ’s outputs, including the
cosine similarity quality control metrics, aswe will in the Pipeline 3:
The V ideo Expert section of this chapter.
The controller can also save the comments and frames.
Saving comments To savethe comments, set save=True. To savethe frames, set
save_frames=True. Set bothvalues to False if you justwant to run the
program and view the outputs, but, in our case, wewill set them as True:
# Ensure the file exists and double checking before saving the
save=True # double checking before saving the comments
save_frames=True # double checking before saving the frames The comment is sav ed in CSV format in cpath and contains the ﬁle
name with the .csv extension and in the location of your choice. In
this case, the ﬁles are sav ed on Google Driv e (make sure the path
exists):
# Save comments
if save==True: # double checking before saving the comments
# Append .csv to the provided file name to create the complet
cpath = f"{file_name}.csv"
if os.path.exists(cpath):
# Use the Python variable 'path' correctly in the shell c
!cp {cpath} /content/drive/MyDrive/files/comments/{cpath}
print(f"File {cpath} copied successfully.")
else:
print(f"No such file: {cpath}")
The output conﬁrms that a ﬁle is sav ed:
File alpinist1.mp4.csv copied successfully.
The frames are sav ed inaroot name direction, for whichwe removethe extension with root_name = root_name + extension.strip('.'):
# Save frames
import shutil
if save_frames==True:
# Extract the root name by removing the extension
root_name, extension = os.path.splitext(file_name)
# This removes the period from the extension
root_name = root_name + extension.strip('.')
# Path where you want to copy the jpg files
target_directory = f'/content/drive/MyDrive/files/comments/{r
# Ensure the directory exists
os.makedirs(target_directory, exist_ok=True)
# Assume your jpg files are in the current directory. Modify
source_directory = os.getcwd() # or specifyadifferent dire
# List all jpg files in the source directory
for file in os.listdir(source_directory):
if file.endswith('.jpg'):
shutil.copy(os.path.join(source_directory, file), targe The output isadirectory with all the frames generated in it. We
should delete the ﬁles if the controller runs inaloop ov er all the
videos inasingle session.
Deleting files To delete the ﬁles, just set delf=True:
delf=False # double checking before deleting the files inase
if delf==True:
!rm -f *.mp4 # video files
!rm -f *.jpg # frames
!rm -f *.csv # comments You can now process an unlimited number of videos one by one and
scale to whatev er size you wish, as long as you havedisk space and a CPU!
Pipeline 2: The Vector Store Administrator The V ector Store Administrator AI agent performs the taskswe
implemented in Chapter 6 , Scaling RAG Bank Customer Data with Pinecone. The nov elty in this section relies on the fact that all the data
we upsert for RAG is AI-generated. Let’s open Pipeline_2_The_Vector_Store_Administrator.ipynb in the GitHub
repository. We will build the V ector Store Administrator on top of the Generator and the Commentator AI agents in four steps, as illustrated
in the following ﬁgure:
Figure 10.7: Workﬂow of the Vector Store Administrator from processing to querying video
frame comments
1. Processing the video comments : The Vector Store Administrator
will load and prepare the comments for chunking as in the Pipeline 2: Scaling a Pinecone Index (v ector store) section of Chapter
6. Sincewe are processing one video atatime inapipeline, the
system deletes the ﬁles processed, which keeps disk space
constant. You can enhance the functionality and scale this
process indeﬁnitely.
2. Chunking and embedding the dataset : The column names
('ID', 'FrameNumber', 'Comment', 'FileName') of the dataset
have already been prepared by the Commentator AI agent in Pipeline 1 . The program chunks and embeds the dataset using the
same functionality as in Chapter 6 in the Chunking and embedding
the dataset section.
3. The Pinecone index : The Pinecone Index is created, and the data
is upserted as in the Creating the Pinecone Index and Upserting
sections of Chapter 6.
4. Querying thevector store after upserting the dataset : This
follows the same process as in Chapter 6 . How ever, in this case,
the retriev al is hybrid, using both the Pineconevector store andaseparate ﬁle system to store videos and video frames.
Go through Steps 1 to 3 in the notebook to examine the V ector Store Administrator ’s functions. After Step 3 , the Pinecone index is ready
for hybrid querying.
Querying the Pinecone index In the notebook on GitHub, Step 4: Querying the Pinecone index
implements functions to ﬁndacomment that matches user input and
trace it to the frame ofavideo. This leads to the video source and
frame, which can be display ed. We can display the videos and frames
from the locationwe wish. This hybrid approach thus involv es
querying the Pinecone Index to retrieveinformation and also retrievemedia ﬁles from another location.
We saw thatav ector store can contain images that are queried, as
implemented in Chapter 4 , Multimodal Modular RAG for Drone Technology . In this chapter, the video production use case videos and
frame ﬁles are stored separately. In this case, it is in the GitHub
repository. In production, the video and frame ﬁles can be retriev ed
from any storage systemwe need, which may or may not proveto be
more cost-eﬀectivethan storing data on Pinecone. The decision to
store images inav ector store oraseparate location will depend on
the project’s needs.
We begin by deﬁning the number of top-k resultswe wish to process:
k=1 # number of results We then designarather diﬃcult prompt:
query_text = "Findabasketball player that is scoring withad Onlyahandful of frames in the whole video dataset contain an image
ofabasketball play er jumping to scoreaslam dunk. Can our system
ﬁnd it? Let’s ﬁnd out.
We ﬁrst embed our query to match the format of the data in the
vector store:
import time
# Start timing before the request
start_time = time.time()
# Target vector
#query_text = "Findabasketball player."
query_embedding = get_embedding(query_text, model=embedding_mod Thenwe runasimilarityvector search betw een the query and the
dataset:
# Perform the query using the embedding
query_results = index.query(vector=query_embedding, top_k=k, in
# Print the query results along with metadata
print("Query Results:")
for match in query_results['matches']:
print(f"ID: {match['id']}, Score: {match['score']}")
# Check if metadata is available
if 'metadata' in match:
metadata = match['metadata']
text = metadata.get('text', "No text metadata available
frame_number = metadata.get('frame_number', "No framenfile_name = metadata.get('file_name', "No file name ava Finally, wedisplay the content of the response and the response time:
print(f"Text: {text}")
print(f"Frame Number: {frame_number}")
print(f"File Name: {file_name}")
else:
print("No metadata available.")
# Measure response time
response_time = time.time() - start_time
print(f"Querying response time: {response_time:.2f} seconds")
The output contains the ID of the comment retriev ed and its score:
Query Results:
ID: f104138b-0be8-4f4c-bf99-86d0eb34f7ee, Score: 0.866656184
The output also contains the comment generated by the OpenAI LLM
(the Commentator agent):
Text: In this image, there isaperson who appears to be in the The ﬁnal output contains the frame number thatwas commented, the
video ﬁle of the frame, and the retriev al time:
Frame Number: 191
File Name: basketball3.mp4
Querying response time: 0.57 seconds We can display the video by downloading it based on the ﬁle name:
print(file_name)
# downloading file from GitHub
directory = "Chapter10/videos"
filename = file_name
download(directory,file_name)
Then, useastandard Python function to display it:
# Open the file in binary mode
def display_video(file_name):
with open(file_name, 'rb') as file:
video_data = file.read()
# Encode the video file as base64
video_url = b64encode(video_data).decode()
# Create an HTML string with the embedded video
html = f'''
<video width="640" height="480" controls>
<source src="data:video/mp4;base64,{video_url}" type="video Your browser does not support the video tag.
</video>
'''
# Display the video HTML(html)
# Return the HTML object
return HTML(html)
display_video(file_name)
The video containingabasketball play er performingadunk is
display ed:
Figure 10.8: Video output We can take this further with more precision by displaying the frame
of the comment retriev ed:
file_name_root = file_name.split('.')[0]
…
from IPython.display import Image, display
# Specify the directory and file name
directory = '/content/' # Adjust the directory if needed
file_path = os.path.join(directory, frame)
# Check if the file exists and verify its size
if os.path.exists(file_path):
file_size = os.path.getsize(file_path)
print(f"File '{frame}' exists. Size: {file_size} bytes.")
# Definealogical size value in bytes, for example, 1000 b
logical_size = 1000 # You can adjust this threshold as nee
if file_size > logical_size:
print("The file size is greater than the logical value.
display(Image(filename=file_path))
else:
print("The file size is less than or equal to the logic
else:
print(f"File '{frame}' does not exist in the specified dire The output shows the exact frame that corresponds to the user input:
Figure 10.9: Video frame corresponding to our input Only the frames of basketball3.mp4 were sav ed in the GitHub repository for disk space reasons for this
program. In production, all the frames you decide you
need can be stored and retriev ed.
The team of AI agents in this chapter worked together to generate
videos (the Generator ), comment on the video frames (the Commentator ), upsert embedded comments in thevector store (the Vector Store Administrator ), and prepare the retriev al process (the Vector Store Administrator ). We also saw that the retriev al process
already contained augmented input and output thanks to the OpenAI
LLM (the Commentator ) that generated natural language comments.
The process that led to this point will deﬁnitely be applied in many
domains: ﬁreﬁghting, medical imagery, marketing, and more.
What more canwe expect from this system? The V ideo Expert AI
agent will answ er that.
Pipeline 3: The Video Expert The role of the OpenAI GPT-4o V ideo Expert is to analyze the
comment made by the Commentator OpenAI LLM agent, point out
the cognitivedissonances (things that don’t seem to ﬁt together in the
description), rewrite the comment, and providealabel. The workﬂow
of the V ideo Expert, as illustrated in the following ﬁgure, also
includes the code of the Metrics calculations and display section of Chapter 7 , Building Scalable Know ledge-Graph-Based RAG with W ikipedia API and LlamaIndex .
The Commentator ’s rolewas only to describe what it saw. The V ideo Expert is there to make sure it makes sense and also label the videos
so they can be classiﬁed in the dataset for further use.
Figure 10.10: Workﬂow of the Video Expert for automated dynamics descriptions and
labeling
1. The Pinecone index will connect to the Pinecone index as
described in the Pipeline 2. The V ector Store Administrator section
of this chapter. This time, wewill not upsert data but connect to
thevector store.
2. Deﬁne the RAG functions utilizing the straightforw ard
functionswe built in Pipeline 1 and Pipeline 2 of this chapter.
3. Querying thevector store is nothing but querying the Pinecone Index as described in Pipeline 2 of this chapter.
4. Retriev al augmented generation ﬁnally determines the main
role of V ideo Expert GPT-4o, which is to analyze and improvethevector store query responses. This ﬁnal step will include
evaluation and metric functions.
There are as many strategies as projects to implement the video
production use casewe explored in this chapter, but the V ideo Expert
plays an important role. Open Pipeline_3_The_Video_Expert.ipynb on GitHub and go to the Augmented Retriev al Generation section in Step 2:
Deﬁning the RAG functions .
The function makes an OpenAI GPT-4o call, like for the Commentator in Pipeline 1 . How ever, this time, the role of the LLM is
quite diﬀerent:
"role": "system",
"content": "You will be provided with comments The instructions for GPT-4o are:
You will be provided with comments of an image frame taken
fromavideo: This instructs the LLM to analyze the AI-generated
comments. The Commentator had to remain neutral and
describe the frame as it saw it. The role of the V ideo Expert agent
is diﬀerent: it has to analyze and enhance the comment.
1. Point out the cognitive dissonances: This instructs the model
to ﬁnd contradictions or discrepancies in the comment that can
come from theway the AI-generated videowas produced aswell
(lack of logic in the video).
2. Rewrite the comment inalogical engaging style: This
instructs the V ideo Expert agent to rewrite the comment going
fromatechnical comment toadescription.
3. Providealabel for this image such as Label: basketball,
football, soccer or other label: This instructs the model to
providealabel for further use. On GitHub, Step 3: Querying the Vector Store reproduces the query and output described in Pipeline 2 forabasketball play er scoring withadunk, with the
corresponding video and frame. The output is:
ID=f104138b-0be8-4f4c-bf99-86d0eb34f7ee
score=0.866193652
text=In this image, there isaperson who appears to be in
frame_number=191
file_name=basketball3.mp4
The comment provided seems acceptable. How ever, let’s see what GPT-4o thinks of it. The Step 4: Retriev al Augmented Generation section
on GitHub takes the output and submits it as the user prompt to the Video Expert agent:
prompt=text We then call the V ideo Expert agent to obtain its expertise:
response_content = get_openai_response(prompt)
print(response_content)
The output provides the V ideo Expert’s insights:
1. Cognitive Dissonances:
- The comment redundantly describes the action of dunking mul
- The mention of "the word 'dunk' is superimposed on the imag
- The background details about clear skies andamodern build
2. Rewritten Comment:
In this image, a basketball player is captured mid-air, execu
3. Label: Basketball The response iswell-structured and acceptable. The output mayvary
from one run to another due to the stochastic “creativ e” nature of Generativ e AI agents.
The Evaluator section that follows Step 4 runs ten examples using the
same process as the basketball requestwe just made. Each example
thus contains:
A user prompt The comment returned by thevector store query The enhanced comment made by the GPT-4o model Each example also contains the same ev aluation process as in Chapter
7, Building Scalable Know ledge-Graph-Based RAG with W ikipedia API and LlamaIndex, in the Examples for metrics section. How ever, in this case,
the human ev aluator suggests content instead ofascore (0 to 1). The
human content becomes the ground truth, the expected output.
Before beginning the ev aluation, the program creates scores to keep
track of the original response made by the query.
The human ev aluator rewrites the output provided by the V ideo Expert:
# Human feedback flashcard comment
text1 = "This image shows soccer players onafield dribbling a The content rewrien by the V ideo Expert is extracted from the
response:
# Extract rewritten comment
text2 = extract_rewritten_comment(response_content)
The human comment (ground truth, the reference output) and the LLM comments are display ed:
print(f"Human Feedback Comment: {text1}")
print(f"Rewritten Comment: {text2}")
Then, the cosine similarity score betw een the human and LLM
comments is calculated and appended to scores:
similarity_score3=calculate_cosine_similarity_with_embeddings(t
print(f"Cosine Similarity Score with sentence transformer: {sim
scores.append(similarity_score3)
The original score provided with the query is appended to the
query’s retriev al score, rscores:
rscores.append(score)
The output displays the human feedback, the comment rewrien by GPT-4o (the V ideo Expert), and the similarity score:
Human Feedback Comment: This image shows soccer players onafie Rewritten Comment: "A group of people are engaged inacasual ga Cosine Similarity Score with sentence transformer: 0.621
This program contains ten examples, butwe can enteracorpus of as
many examples aswe wish to ev aluate the system. The ev aluation of
each example applies the same choice of metrics as in Chapter 7 . After
the examples havebeen ev aluated, the Metrics calculations and display
section in the program also runs the metric calculations deﬁned in the
section of the same name in Chapter 7 .
We can use all the metrics to analyze the performance of the system.
The time measurements throughout the program also provide
insights. The ﬁrst metric, accuracy, isagood metric to start with. In
this case, it shows that there is room for progress:
Mean: 0.65
Some requests and responseswere challenging and required further
work to improvethe system:
Checking the quality of the videos and their content Checking the comments and possibly modifying them with
human feedback, aswe did in Chapter 5 , Boosting RAG
Performance with Expert Human Feedback Fine-tuningamodel with images and text aswe did in Chapter 9 ,
Empow ering AI Models: Fine-T uning RAG Data and Human Feedback Designing any other constructiveidea that the video production
team comes up with We can see that RAG-driv en Generativ e AI systems in production are
very eﬀectiv e. How ever, the road from design to production requires
hard human eﬀort! Though AI technology has made tremendous
progress, it still requires humans to design, dev elop, and implement
it in production.
Summary In this chapter, weexplored the hybrid era of human and AI agents,
focusing on the creation ofastreamlined process for generating,
commenting, and labeling videos. By integrating cuing-edge Generativ e AI models, wedemonstrated how to build an automated
pipeline that transforms raw video inputs into structured,
informativ e, and accessible video content.
Our journey began with the Generator agent in Pipeline 1 : The Generator and the Commentator , whichwas tasked with creating video
content from textual ideas. We can see that video generation
processes will continue to expand through seamless integration
ideation and descriptiveaugmentation generativeagents. In Pipeline
2: The V ector Store Administrator , we focused on organizing and
embedding the generated comments and metadata intoasearchable
vector store. In this pipeline, wehighlighted the optimization process
of buildingascalable video content library with minimal machine
resources using only a CPU and no GPU. Finally, in Pipeline 3 : The Video Expert , we introduced the Expert AI agent, a video specialist
designed to enhance and label the video content based on user
inputs. We also implemented ev aluation methods and metric
calculations.
By the end of this chapter, wehad constructedacomprehensiv e,
automated RAG-driv en Generativ e AI system capable of generating,
commenting on, and labeling videos with minimal human
interv ention. This journey demonstrated the pow er and potential of
combining multiple AI agents and models to create an eﬃcient
pipeline for video content creation.
The techniques and toolswe explored can revolutionizevarious
industries by automating repetitivetasks, enhancing content quality,
and making information retriev al more eﬃcient. This chapter not
only providedadetailed technical roadmap but also underscored the
transformativeimpact of AI in modern content creation and
management. You are now all set to implement RAG-driv en Generativ e AI in real-life projects.
Questions Answ er the following questions withyes or no:
1. Can AI now automatically comment and label videos?
2. Does video processing involvespliing the video into frames?
3. Can the programs in this chapter create a 200-minute movie?
4. Do the programs in this chapter require a GPU?
5. Are the embeddedvectors of the video content stored on disk?
6. Do the scripts involvequeryingadatabase for retrieving data?
7. Is there functionality for displaying images in the scripts?
8. Is it useful to havefunctions that speciﬁcally check ﬁle existence
and size in any of the scripts?
9. Is thereafocus on multimodal data in these scripts?
10. Do any of the scripts mention applications of AI in real-world
scenarios?
References Sora video generation model information and access:
Sora | OpenAI : https://ai.invideo.io/
https://openai.com/index/video-generation-
models-as-world-simulators/
Sora: A Review on Background, T echnology, Limitations, and Opportunities of Large V ision Models by Y ixin Liu, Kai Zhang,
Yuan Li, et al.: https://arxiv.org/pdf/2402.17177
Further reading OpenAI, ChatGPT: https://openai.com/chatgpt/
OpenAI, Research: https://openai.com/research/
Pinecone: https://docs.pinecone.io/home Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag Appendix The appendix here provides answ ers to all questions added at the
end of each chapter. Double-check your answ ers toverify that you
have conceptually understood the key concepts.
Chapter 1, Why Retrieval Augmented Generation?
1. Is RAG designed to improvethe accuracy of generativ e AI
models?
Yes, RAG retriev es relev ant data to enhance generativ e AI
outputs.
2. Doesanaïv e RAG conﬁguration rely on complex data
embedding?
No, naïv e RAG uses basic keyword searches without adv anced
embeddings.
3. Is ﬁne-tuning alw aysabeer option than using RAG?
No, RAG is beer for handling dynamic, real-time data.
4. Does RAG retrievedata from external sources in real time to
enhance responses?
Yes, RAG pulls data from external sources during query
processing.
5. Can RAG be applied only to text-based data?
No, RAG works with text, images, and audio data aswell.
6. Is the retriev al process in RAG triggered byauser or automated
input?
The retriev al process in RAG is typically triggered byaquery,
which can come fromauser or an automated system.
7. Are cosine similarity and TF-IDF both metrics used in adv anced RAG conﬁgurations?
Yes, both are used to assess the relev ance betw een queries and
documents.
8. Does the RAG ecosystem include only data collection and
generation components?
No, it also includes storage, retriev al, ev aluation, and training.
9. Can adv anced RAG conﬁgurations process multimodal data
such as images and audio?
Yes, adv anced RAG supports processing structured and
unstructured multimodal data.
10. Is human feedback irrelev ant in ev aluating RAG systems?
No, human feedback is crucial for improving RAG system
accuracy and relev ance.
Chapter 2, RAG Embedding Vector Stores with Deep Lake and OpenAI
1. Do embeddings conv ert text into high-dimensionalvectors for
faster retriev al in RAG?
Yes, embeddings createvectors that capture the semantic
meaning of text.
2. Are keyword searches more eﬀectivethan embeddings in
retrieving detailed semantic content?
No, embeddings are more context-aw are than rigid keyword
searches.
3. Is it recommended to separate RAG pipelines into independent
components?
Yes, this allows parallel dev elopment and easier maintenance.
4. Does the RAG pipeline consist of only two main components?
No, the pipeline consists of three components – data collection,
embedding, and generation.
5. Can Activ eloop Deep Lake handle both embedding andvector
storage?
Yes, it stores embeddings eﬃciently for quick retriev al.
6. Is the text-embedding-3-small model from OpenAI used to
generate embeddings in this chapter?
Yes, this model is chosen for its balance betw een detail and
computational eﬃciency.
7. Are data embeddings visible and directly traceable in an RAG-
driven system?
Yes, unlike parametric models, embeddings in RAG are
traceable to the source.
8. Can a RAG pipeline run smoothly without spliing into
separate components?
Spliing an RAG pipeline into components improv es
specialization, scalability, and security, which helpsasystem
run smoothly. Simpler RAG systems may still function
eﬀectiv ely without explicit component separation, although it
may not be the optimal setup.
9. Is chunking large texts into smaller parts necessary for
embedding and storage?
Yes, chunking helps optimize embedding and improv es the
eﬃciency of queries.
10. Are cosine similarity metrics used to ev aluate the relev ance of
retriev ed information?
Yes, cosine similarity helps measure how closely retriev ed data
matches the query.
Chapter 3, Building Index-Based RAG with LlamaIndex, Deep Lake,
and OpenAI
1. Do indexes increase precision and speed in retriev al-augmented
generativ e AI?
Yes, indexes make retriev al faster and more accurate.
2. Can indexes oﬀer traceability for RAG outputs?
Yes, indexes allow tracing back to the exact data source.
3. Is index-based search slow er thanvector-based search for large
datasets?
No, index-based search is faster and optimized for large
datasets.
4. Does LlamaIndex integrate seamlessly with Deep Lake and OpenAI?
Yes, LlamaIndex, Deep Lake, and OpenAI workwell together.
5. Are tree, list, v ector, and keyword indexes the only types of
indexes?
No, these are common, but other types exist aswell.
6. Does the keyword index rely on semantic understanding to
retrievedata?
No, it retriev es based on keywords, not semantics.
7. Is LlamaIndex capable of automatically handling chunking and
embedding?
Yes, LlamaIndex automates these processes for easier data
management.
8. Are metadata enhancements crucial for ensuring the traceability
of RAG-generated outputs?
Yes, metadata helps trace back to the source of the generated
content.
9. Can real-time updates easily be applied to an index-based
search system?
Indexes often require re-indexing for updates. How ever, some
modern indexing systems havebeen designed to handle real-
time or near-real-time updates more eﬃciently.
10. Is cosine similarityametric used in this chapter to ev aluate
query accuracy?
Yes, cosine similarity helps assess the relev ance of query
results.
Chapter 4, Multimodal Modular RAG for Drone Technology
1. Does multimodal modular RAG handle diﬀerent types of data,
such as text and images?
Yes, it processes multiple data types such as text and images.
2. Are drones used solely for agricultural monitoring and aerial
photography?
No, drones are also used for rescue, traﬃc, and infrastructure
inspections.
3. Is the Deep Lake V isDrone dataset used in this chapter for
textual data only?
No, it contains labeled drone images, not just text.
4. Can bounding boxes be added to drone images to identify
objects such as trucks and pedestrians?
Yes, bounding boxes are used to mark objects within images.
5. Does the modular system retrieveboth text and image data for
query responses?
Yes, it retriev es and generates responses from both textual and
image datasets.
6. Is buildingav ector index necessary for querying the
multimodal V isDrone dataset?
Yes, avector index is created for eﬃcient multimodal data
retriev al.
7. Are the retriev ed images processed without adding any labels
or bounding boxes?
No, images are processed with labels and bounding boxes.
8. Is the multimodal modular RAG performance metric based only
on textual responses?
No, it also ev aluates the accuracy of image analysis.
9. Canamultimodal system such as the one described in this
chapter handle only drone-related data?
No, it can be adapted for other industries and domains.
10. Is ev aluating images as easy as ev aluating text in multimodal RAG?
No, image ev aluation is more complex and requires specialized
metrics.
Chapter 5, Boosting RAG
Performance with Expert Human Feedback
1. Is human feedback essential in improving RAG-driv en
generativ e AI systems?
Yes, human feedback directly enhances the quality of AI
responses.
2. Can the core data inagenerativ e AI model be changed without
retraining the model?
No, the model’s core data is ﬁxed unless it is retrained.
3. Does Adaptiv e RAG involvereal-time human feedback loops to
improveretriev al?
Yes, Adaptiv e RAG uses human feedback to reﬁne retriev al
results.
4. Is the primary focus of Adaptiv e RAG to replace all human
input with automated responses?
No, it aims to blend automation with human feedback.
5. Can human feedback in Adaptiv e RAG trigger changes in the
retriev ed documents?
Yes, feedback can prompt updates to retriev ed documents for
beer responses.
6. Does Company C use Adaptiv e RAG solely for customer
support issues?
No, it’s also used for explaining AI concepts to employ ees.
7. Is human feedback used only when the AI responses havehigh
user ratings?
No, feedback is often used when responses are rated poorly.
8. Does the program in this chapter provide only text-based
retriev al outputs?
No, it uses both text and expert feedback for responses.
9. Is the Hybrid Adaptiv e RAG system static, meaning it cannot
adjust based on feedback?
No, it dynamically adjusts to feedback and rankings.
10. Are user rankings completely ignored in determining the
relev ance of AI responses?
No, user rankings directly inﬂuence the adjustments made toasystem.
Chapter 6, Scaling RAG Bank Customer Data with Pinecone
1. Does using a Kaggle dataset typically involvedownloading and
processing real-world data for analysis?
Yes, Kaggle datasets are used for practical, real-world data
analysis and modeling.
2. Is Pinecone capable of eﬃciently managing large-scalevector
storage for AI applications?
Yes, Pinecone is designed for large-scalevector storage,
making it suitable for complex AI tasks.
3. Can k-means clustering helpvalidate relationships betw een
features such as customer complaints and churn?
Yes, k-means clustering is useful for identifying andvalidating
paerns in datasets.
4. Does lev eraging ov eramillionvectors inadatabase hinder the
ability to personalize customer interactions?
No, handling large volumes ofvectors allows for more
personalized and targeted customer interactions.
5. Is the primary objectiveof using generativ e AI in business
applications to automate and improvedecision-making
processes?
Yes, generativ e AI aims to automate and reﬁne decision-
making invarious business applications.
6. Are lightw eight dev elopment environments adv antageous for
rapid prototyping and application dev elopment?
Yes, they streamline dev elopment processes, making it easier
and faster to test and deploy applications.
7. Can Pinecone’s architecture automatically scale to accommodate
increasing data loads without manual interv ention?
Yes, Pinecone’s serv erless architecture supports automatic
scaling to handle larger data volumes eﬃciently.
8. Is generativ e AI typically employ ed to create dynamic content
and recommendations based on user data?
Yes, generativ e AI is often used to generate customized content
and recommendations dynamically.
9. Does the integration of AI technologies such as Pinecone and OpenAI require signiﬁcant manual conﬁguration and
maintenance?
No, these technologies are designed to minimize manual
eﬀorts in conﬁguration and maintenance through automation.
10. Are projects that usevector databases and AI expected to
eﬀectiv ely handle complex queries and large datasets?
Yes, v ector databases combined with AI are particularlywell-
suited for complex queries and managing large datasets.
Chapter 7, Building Scalable Knowledge-Graph-based RAG with Wikipedia API and LlamaIndex
1. Does the chapter focus on buildingascalable know ledge graph-
based RAG system using the W ikipedia API and LlamaIndex?
Yes, it details creatingaknow ledge graph-based RAG system
using these tools.
2. Is the primary use case discussed in the chapter related to
healthcare data management?
No, the primary use case discussed is related to marketing and
other domains.
3. Does Pipeline 1 involvecollecting and preparing documents
from W ikipedia using an API?
Yes, Pipeline 1 automates document collection and preparation
using the W ikipedia API.
4. Is Deep Lake used to createarelational database in Pipeline 2?
No, Deep Lake is used to create and populateav ector store,
notarelational database.
5. Does Pipeline 3 utilize LlamaIndex to buildaknow ledge graph
index?
Yes, Pipeline 3 uses LlamaIndex to buildaknow ledge graph
index automatically.
6. Is the system designed to only handleasingle speciﬁc topic,
such as marketing, without ﬂexibility?
No, the system is ﬂexible and can handlevarious topics
beyond marketing.
7. Does the chapter describe how to retriev e URLs and metadata
from W ikipedia pages?
Yes, it explains the process of retrieving URLs and metadata
using the W ikipedia API.
8. Is a GPU required to run the pipelines described in the chapter?
No, the pipelines are designed to run eﬃciently using only a CPU.
9. Does the know ledge graph index visually map out relationships
betw een pieces of data?
Yes, the know ledge graph index visually displays semantic
relationships in the data.
10. Is human interv ention required at ev ery step to query the
know ledge graph index?
No, querying the know ledge graph index is automated, with
minimal human interv ention needed.
Chapter 8, Dynamic RAG with Chroma and Hugging Face Llama
1. Does the script ensure that the Hugging Face API token is nev er
hardcoded directly into the notebook for security reasons?
Yes, the script provides methods to either use Google Driveor
manual input for API token handling, thus avoiding
hardcoding.
2. In the chapter ’s program, is the accelerate library used to
facilitate the deployment of machine learning models on cloud-
based platforms?
No, the accelerate library is used to run models on local
resources such as multiple GPUs, TPUs, and CPUs, not
speciﬁcally cloud platforms.
3. Is user authentication, apart from the API token, required to
access the Chroma database in this script?
No, the script does not detail additional authentication
mechanisms beyond using an API token to access Chroma.
4. Does the notebook use Chroma for temporary storage ofvectors
during the dynamic retriev al process?
Yes, the script employs Chroma for storingvectors temporarily
to enhance the eﬃciency of data retriev al.
5. Is the notebook conﬁgured to use real-time acceleration of
queries through GPU optimization?
Yes, the accelerate library is used to ensure that the notebook
can lev erage GPU resources for optimizing queries, which is
particularly useful in dynamic retriev al seings.
6. Can this notebook’s session time measurements help in
optimizing the dynamic RAG process?
Yes, by measuring session time, the notebook provides insights
that can be used to optimize the dynamic RAG process,
ensuring eﬃcient runtime performance.
7. Does the script demonstrate Chroma’s capability to integrate
with machine learning models for enhanced retriev al
performance?
Yes, the integration of Chroma with the Llama model in this
script highlights its capability to enhance retriev al performance
by using adv anced machine learning techniques.
8. Does the script include functionality to adjust the parameters of
the Chroma database based on session performance metrics?
Yes, the notebook potentially allows adjustments to be made
based on performance metrics, such as session time, which can
inﬂuence how the notebook is built and adjust the process,
depending on the project.
Chapter 9, Empowering AI Models:
Fine-Tuning RAG Data and Human Feedback
1. Do all organizations need to manage large volumes of RAG
data?
No, many corporations only need small data volumes.
2. Is the GPT-4-o-mini model described as insuﬃcient for ﬁne-
tuning tasks?
No, GPT-4o-mini is described as cost-eﬀectivefor ﬁne-tuning
tasks.
3. Can pretrained models update their know ledge base after the
cutoﬀ date without retriev al systems?
No, models are static and rely on retriev al for new information.
4. Is it the case that static data nev er changes and thus nev er
requires updates?
No, Only that it remains stable foralong time, not forev er.
5. Is downloading data from Hugging Face the only source for
preparing datasets?
Yes, Hugging Face is speciﬁcally mentioned as the data source.
6. Are all RAG data ev entually embedded into the trained model’s
parameters?
No, non-parametric data remains external.
7. Does the chapter recommend using only new data for ﬁne-
tuning AI models?
No, it suggests ﬁne-tuning with relev ant, often stable data.
8. Is the OpenAI metric interface used to adjust the learning rate
during model training?
No, it monitors performance and costs after training.
9. Can the ﬁne-tuning process be eﬀectiv ely monitored using the OpenAI dashboard?
Yes, the dashboard provides real-time updates on ﬁne-tuning
jobs.
10. Is human feedback deemed unnecessary in the preparation of
hard science datasets such as SciQ?
No, human feedback is crucial for data accuracy and relev ance.
Chapter 10, RAG for Video Stock Production with Pinecone and OpenAI
1. Can AI now automatically comment and label videos?
Yes, wenow create video stocks automatically toacertain
extent.
2. Does video processing involvespliingavideo into frames?
Yes, wecan splitavideo into frames before analyzing the
frames.
3. Can the programs in this chapter create a 200-minute movie?
No, for the moment, this cannot be done directly. We would
have to create many videos and then stitch them together withavideo editor.
4. Do the programs in this chapter require a GPU?
No, only a CPU is required, which is cost-eﬀectivebecause the
processing times are reasonable, and the programs mostly rely
on API calls.
5. Are the embeddedvectors of the video content stored on disk?
No, the embeddedvectors are upserted in a Pineconevector
database.
6. Do the scripts involvequeryingadatabase for retrieving data?
Yes, the scripts query the Pineconevector database for data
retriev al.
7. Is there functionality for displaying images in the scripts?
Yes, the programs include code to display images after
downloading them.
8. Is it useful to havefunctions speciﬁcally checking ﬁle existence
and size in any of the scripts?
Yes, this avoids trying to display ﬁles that don’t exist or that
are empty.
9. Is thereafocus on multimodal data in these scripts?
Yes, all scripts focus on handling and processing multimodal
data (text, image, and video).
10. Do any of the scripts mention applications of AI in real-world
scenarios?
Yes, these scripts deal with multimodal data retriev al and
processing, which makes them applicable in AI-driv en content
management, search, and retriev al systems.
Join our community on Discord Join our community’s Discord space for discussions with the author
and other readers:
https://www.packt.link/rag

packt.com Subscribe to our online digital library for full access to ov er 7,000
books and videos, aswell as industry leading tools to help you plan
your personal dev elopment and adv ance your career. For more
information, please visit ourwebsite.
Why subscribe?
Spend less time learning and more time coding with practical
eBooks and V ideos from ov er 4,000 industry professionals Improveyour learning with Skill P lans built especially for you Getafree eBook or video ev ery month Fully searchable for easy access to vital information Copy and paste, print, and bookmark content At www.packt.com , you can also readacollection of free technical
articles, sign up forarange of free newsleers, and receiveexclusivediscounts and oﬀers on P ackt books and eBooks.
Other Books You May Enjoy If you enjoy ed this book, you may be interested in these other books
by P ackt:
Transformers for Natural Language Processing and Computer Vision - Third Edition Denis Rothman ISBN: 9781805128724
Breakdown and understand the architectures of the Original Transformer, BERT , GPT models, T5, P aLM, V iT, CLIP, and DALL-E
Fine-tune BERT , GPT , and P aLM 2 models Learn about diﬀerent tokenizers and the best practices for
preprocessing language data Pretrain a RoBERT a model from scratch Implement retriev al augmented generation and rules bases to
mitigate hallucinations Visualize transformer model activity for deeper insights using BertV iz, LIME, and SHAP
Go in-depth into vision transformers with CLIP, DALL-E 2,
DALL-E 3, and GPT-4V
Generativ e AI Application Integration P aerns Juan P ablo Bustos, Luis Lopez Soria ISBN: 9781835887608
Concepts of GenAI: pre-training, ﬁne-tuning, prompt
engineering, and RAG
Framework for integrating AI: entry points, prompt pre-
processing, inference, post-processing, and presentation Paerns for batch and real-time integration Code samples for metadata extraction, summarization, intent
classiﬁcation, question-answ ering with RAG, and more Ethical use: bias mitigation, data priv acy, and monitoring Deployment and hosting options for GenAI models Packt is searching for authors like
you If you’ re interested in becoming an author for P ackt, please visit
authors.packtpub.com and apply today. We haveworked with
thousands of dev elopers and tech professionals, just like you, to help
them share their insight with the global tech community. You can
makeageneral application, apply foraspeciﬁc hot topic thatwe are
recruiting an author for, or submit your own idea.
Share your thoughts Now you’v e ﬁnished RAG-Driv en Generativ e AI, we’d loveto hear
your thoughts! If you purchased the book from Amazon, please
click here to go straight to the Amazon review page for
this book and share your feedback or leavea review on the site that
you purchased it from.
Your review is important to us and the tech community and will
help us make surewe’re deliv ering excellent quality content.
Index A
Activeloop URL 40
Activeloop Deep Lake 32, 33
adaptive RAG 116-118
selection system 123
advanced RAG 4, 20
index-based search 23
vector search 21
Agricultural Marketing Service (AMS) 201
AI-generated video dataset 261
diffusion transformer model video dataset, analyzing 264
diffusion transformer , working 262, 263
Amazon W eb Services (A WS) 144
Apollo program
reference link 41
augmented generation, RAG pipeline 50, 51
augmented input 53, 54
input and query retrieval 51-53
B
bag-of-words (BoW) model 219
Bank Customer Churn dataset
collecting 144-149
environment, installing for Kaggle 146, 147
exploratory data analysis 149-151
ML model, training 152
preparing 144-146
C
Chroma 212, 213
Chroma collection
completions, embedding 218, 219
completions, storing 218, 219
data, embedding 216, 217
data, upserting 216, 217
embeddings, displaying 219
model, selecting 217
content generation 130-132
cosine similarity
implementing, to measure similarity between user input and
generative AI model's output 56-58
D
data embedding and storage, RAG pipeline 44, 45
batch of prepared documents, retrieving 45, 46
data, adding to vector store 47, 48
embedding function 47
vector store, creating 46
vector store information 48-50
vector store, verifying for existence 46
data embeddings 33
data, for upsertion
preparing 191, 192
dataset
downloading 237
preparing, for fine-tuning 237-240
visualizing 238
Davies-Bouldin index 154
Deep Lake API
reference link 48
Deep Lake vector store
creating 192
populating 192
diffusion transformer model video dataset
analyzing 264
thumbnails and videos, displaying 268, 269
video download and display functions 264, 265
video file 266-268
documents
collecting 186
preparing 186
dynamic RAG
applications 208
architecture 208-210
collection, deleting 228, 229
collection, querying 220-223
dataset, downloading 214, 215
dataset, preparing 214, 215
environment, installing 210
prompt 223
prompt response 225
query result, retrieving 225
session time, activating 213, 214
total session time 229
using, with Llama 225-228
dynamic RAG environment installation
of Chroma 212, 213
of Hugging Face 211, 212
E
embedding models, OpenAI
reference link 47
embeddings 32
entry-level advanced RAG
coding 9
entry-level modular RAG
coding 9
entry-level naïve RAG
coding 9
environment
installing 236, 237
environment setup, RAG pipeline 36
authentication process 39, 40
components, in installation process 36, 37
drive, mounting 37
installation packages 36
libraries 36
requisites, installing 39
subprocess, creating to download files from GitHub 37, 38
evaluator 8, 132
cosine similarity score 132
human-expert evaluation 135-138
human feedback 9
human user rating 133-135
metrics 9
response time 132
F
fine-tuned OpenAI model
using 244-246
fine-tuning
dataset, preparing for 237-240
versus RAG 4
fine-tuning documentation, OpenAI
reference link 246
fine-tuning static RAG data
architecture 234, 235
foundations and basic implementation
data, setting up with list of documents 12, 13
environment, installing 10
generator function, using GPT -4o 11, 12
query , for user input 13, 14
G
Galileo (spacecraft)
reference link 42
generative AI environment
installing 129, 130
generator 8, 122
augmented input with HF 8
content generation 130-132
generation and output (G4) 8
generative AI environment, installing 129, 130
HF-RAG for augmented document inputs, integrating 123, 124
input 8, 124
mean ranking simulation scenario 124
prompt engineering (G3) 8
Generator and Commentator 261, 270, 271
AI-generated video dataset 261
frames, commenting on 272-274
Pipeline 1 controller 274
video, displaying 271
videos, spitting into frames 271, 272
GitHub 259
H
Hubble Space T elescope
reference link 41
Hugging Face 211, 212
reference link 211
hybrid adaptive RAG
building, in Python 118
generator 122
retriever 119
I
index-based RAG 62
architecture 62-64
index-based search 21-24, 62
augmented input 25
feature extraction 24
generation 25
versus vector-based search 64
International Space Station (ISS)
reference link 41
J
Juno (spacecraft)
reference link 41
K
Kaggle
reference link 146
Kepler space telescope
reference link 42
keyword index query engine 74, 85-87
performance metric 87
knowledge-graph-based semantic search
graph, building from trees 183, 185
RAG architecture, using for 180-183
knowledge graph index-based RAG 193, 195
example metrics 199-201
functions, defining 197
generating 194, 195
graph, displaying 195, 197
interacting 197
metrics calculation 201-203
metrics display 201-203
re-ranking 198, 199
similarity score packages, installing 197
knowledge graphs 179
L
Large Language Model (LLM) 3
list index query engine 74, 83, 84
performance metric 84, 85
Llama
using, with dynamic RAG 225-228
LLM dataset
loading 93, 94
LLM query engine
initializing 95
textual dataset, querying 95
user input, for multimodal modular RAG 95
M
machine learning (ML) 144, 213
Mars rover
reference link 41
mean ranking simulation scenario
human-expert feedback RAG 126-128
no human-expert feedback RAG 128, 129
no RAG 125
metadata
retrieving 186-190
metrics
analyzing, of training process and model 247-249
metrics, fine-tuned models
reference link 247
ML model, training 152
clustering evaluation 154-156
clustering implementation 154-156
data preparation and clustering 152-154
modular RAG 4, 26, 27
strategies 28
multimodal dataset structure
bounding boxes, adding 100-103
image, displaying 99
image, saving 100-103
image, selecting 99
navigating 99
multimodal modular RAG 90-92
building, for drone technology 93
performance metric 108
user input 95
multimodal modular RAG, performance metric 108
LLM 109
multimodal 109, 111
overall performance 112
multimodal modular RAG program, for drone technology
building 93, 107, 108
LLM dataset, loading 93, 94
multimodal dataset, loading 96-99
multimodal dataset structure, navigating 99
multimodal dataset, visualizing 96-99
multimodal query engine, building 103
performance metric 108
multimodal query engine
building 103
creating 103, 104
query , running on V isDrone multimodal dataset 105
response, processing 105, 106
source code image, selecting 106, 107
vector index, creating 103, 104
N
naïve RAG 4
augmented input 19
example, creating with 17
generation 20
keyword search and matching 18
metrics 19
O
ONNX
reference link 213
OpenAI 259, 260
URL 39
OpenAI model
fine-tunes, monitoring 242-244
fine-tuning 240, 241
for embedding 157
for generation 157
Pinecone constraints 157
Open Neural Network Exchange (ONNX) 213
P
Pinecone 260
reference link 170
used, for scaling 142
Pinecone index
querying 279-283
Pinecone index (vector store)
challenges 157, 158
creating 164, 165
data, duplicating 163, 164
dataset, chunking 160
dataset, embedding 161-163
dataset, processing 159, 160
environment, installing 158
querying 168-170
scaling 156
upserting 166-168
Pipeline 1 controller 274-276
comments, saving 276, 277
files, deleting 277
Python
used, for building hybrid adaptive RAG 118
R
RAG architecture
for video production 254-256
RAG ecosystem 235, 236
domains 5, 6, 7
evaluator component 8
generator component 8
retriever component 7
trainer component 9
RAG framework
advanced RAG 4
generator 4
modular RAG 4
naive RAG 4
retriever 4
RAG generative AI 170
augmented generation 174-176
augmented prompt 174
relevant texts, extracting 173
using, with GPT -4o 170
RAG pipeline 33, 34
augmented generation 35, 50, 51
building, steps 36
components 34
data collection 35, 40-42
data embedding and storage 35, 44, 45
data preparation 35, 40-44
environment setup 36
reasons, for component approach 34
RAG, with GPT -4o 170, 171
dataset, querying 171
target vector , querying 171, 173
Retrieval Augmented Generation (RAG) 1-3, 50
non-parametric 4
parametric 4
versus fine-tuning 4, 5
retrieval metrics 15
cosine similarity 15, 16
enhanced similarity 16, 17
retriever 119
data, processing 120, 121
dataset, preparing 119
environment, installing 119
user input process 121, 122
retriever component
collect 7
process 7
retrieval query 8
storage 7
S
scaling, with Pinecone 142
architecture 142-144
semantic index-based RAG program
building 64, 65
cosine similarity metric 75, 76
Deep Lake vector store, creating 69-74
Deep Lake vector store, populating 69-74
documents collection 65-69
documents preparation 65-69
environment, installing 65
implementing 74
query parameters 75
user input 75
session time
activating 213, 214
Silhouette score 154
space exploration
reference link 41
SpaceX
reference link 41
T
Term Frequency-Inverse Document Frequency (TF-IDF) 15, 57,
132
trainer 9
training loss 249
tree index query engine 74, 80-82
performance metric 83
U
upserting process
reference link 166
user interface (UI) 122
V
vector-based search
versus index-based search 64
vector search 21
augmented input 22
generation 23
metrics 22
vector similarity
reference link 165
Vector Store Administrator 278, 279
Pinecone index, querying 279-283
vector store index query engine 74-76
optimized chunking 79
performance metric 79, 80
query response and source 77, 78
vector stores 33
Video Expert 283-288
video production ecosystem, environment 257
GitHub 259
modules and libraries, importing 258, 259
OpenAI 259
Pinecone 260
Voyager program
reference link 42
W
Wikipedia data
retrieving 186-190
Downloadafree PDF copy of this
book Thanks for purchasing this book!
Do you like to read on the go but are unable to carry your print
books ev erywhere?
Is your eBook purchase not compatible with the device of your
choice?
Don’t worry, now with ev ery P ackt book you get a DRM-free PDF
version of that book at no cost.
Read anywhere, any place, on any device. Search, copy, and paste
code from your favorite technical books directly into your
application.
The perks don’t stop there, you can get exclusiveaccess to discounts,
newsleers, and great free content in your inbox daily.
Follow these simple steps to get the beneﬁts:
1. Scan the QR code or visit the link below:
https://packt.link/free-ebook/9781836200918
2. Submit your proof of purchase.
3. That’s it! We’ll send your free PDF and other beneﬁts to your
email directly.